<doc id="649" url="?curid=649" title="Arraignment">
Arraignment

Arraignment is a formal reading of a criminal charging document in the presence of the defendant, to inform them of the criminal charges against them. In response to arraignment, in some jurisdictions, the accused is expected to enter a plea; in other jurisdictions, no plea is required. Acceptable pleas vary among jurisdictions, but they generally include "guilty", "not guilty", and the peremptory pleas (pleas in bar) setting out reasons why a trial cannot proceed. Pleas of "nolo contendere" ('no contest') and the Alford plea are allowed in some circumstances.
By country.
Australia.
In the Australian legal system, arraignment is the first stage in a criminal trial. The indictment is read to the defendant, who is asked to plead guilty or not guilty. Arraignment procedures vary somewhat among jurisdictions. In New South Wales, the arraignment takes place before the judge only. In South Australian practice, the jury hears the arraignment. In Queensland the indictment is read to the defendant by the judge's associate prior to the empanelling of the jury.
Canada.
In British Columbia, arraignment takes place in one of the first few court appearances by the defendant or their lawyer. The defendant is asked whether they plead guilty or not guilty to each charge.
France.
In France, the general rule is that one cannot remain in police custody for more than 24 hours from the time of their arrest. However, police custody can last another 24 hours in specific circumstances, especially if the offence is punishable by at least one year's imprisonment, or if the investigation is deemed to require the extra time, and can last up to 96 hours in certain cases involving terrorism, drug trafficking, or organised crime. The police need to have the consent of the prosecutor, the "procureur". In the vast majority of cases, the prosecutor will consent. 
Germany.
In Germany, if one has been arrested and taken into custody by the police, one must be brought before a judge as soon as possible and at the latest on the day after the arrest.
New Zealand.
Under New Zealand law, at the first appearance of the accused, they are read the charges and asked for a plea. The available pleas are: guilty, not guilty, and no plea. The response of "no plea" allows the defendant to get legal advice on the plea, which must be made on the second appearance.
South Africa.
In South Africa, arraignment is defined as the calling upon the accused to appear, the informing of the accused of the crime charged against them, the demanding of the accused whether they plead guilty or not guilty, and the entering of their plea.
United Kingdom.
In England, Wales, and Northern Ireland, arraignment is the first of 11 stages in a criminal trial, and involves the clerk of the court reading out the indictment.
In England and Wales, the police cannot legally detain anyone for more than 24 hours without charging them, unless an officer with the rank of superintendent (or above) authorises detention for a further 12 hours (i.e., 36 hours total), or a judge (who will be a magistrate) authorises detention by the police before charge for up to a maximum of 96 hours; for terrorism-related offences a person can be held by the police for up to 28 days before charge. If they are not released after being charged, they should be brought before a court as soon as practicable.
In Scotland, the police cannot detain anyone for more than 12 hours without charging them unless an officer of the rank of superintendent (or above) authorises detention for a further 12 hours (i.e., up to 24 hours in total); for terrorism-related offences a person can be held by the police for up to 14 days before charge. If they are not released after being charged, they should be brought before a court as soon as practicable.
United States.
The Sixth Amendment to the United States Constitution grants criminal defendants the right to be notified of the charges against them. Under the United States' Federal Rules of Criminal Procedure, arraignment shall consist of an open reading of the indictment (and delivery of a copy) to the defendant, and a call for them to plead.
In federal courts, arraignment takes place in two stages. The first is called the "initial arraignment" and must take place within 48 hours of an individual's arrest, or within 72 hours if the individual was arrested on the weekend and not able to go before a judge until Monday. During this stage, the defendant is informed of the pending legal charges and is informed of his or her right to retain counsel. The presiding judge also decides at what amount, if any, to set bail. During the second stage, the post-indictment arraignment, the defendant is allowed to enter a plea.
In New York, a person arrested without a warrant and kept in custody must be brought before a local criminal court for arraignment "without unnecessary delay". A delay of more than 24 hours is rebuttably presumed to be unnecessary.
In California, arraignments must be conducted without unnecessary delay and, in any event, within 48 hours of arrest, excluding weekends and holidays.
Form of the arraignment.
The wording of the arraignment varies from jurisdiction to jurisdiction. However, it generally conforms with the following principles:
Video arraignment.
Video arraignment is the act of conducting the arraignment process using some form of videoconferencing technology. Use of video arraignment system allows the court to conduct the requisite arraignment process without the need to transport the defendant to the courtroom by using an audio-visual link between the location where the defendant is being held and the courtroom.
Use of the video arraignment process addresses the problems associated with having to transport defendants. The transportation of defendants requires time, puts additional demands on the public safety organizations to provide for the safety of the public, court personnel and for the security of the population held in detention. It also addresses the rising costs of transportation.
Guilty and not-guilty pleas.
If the defendant pleads guilty, an evidentiary hearing usually follows. The court is not required to accept a guilty plea. During the hearing, the judge assesses the offense, the mitigating factors, and the defendant's character, and passes sentence.
If the defendant pleads not guilty, a date is set for a preliminary hearing or a trial.
In the past, a defendant who refused to plead (or "stood mute") was subject to "peine forte et dure" (Law French for "strong and hard punishment"). Today, in common law jurisdictions, the court enters a plea of not guilty for a defendant who refuses to enter a plea. The rationale for this is the defendant's right to silence.
Pre-trial release.
This is also often the stage at which arguments for or against pre-trial release and bail may be made, depending on the alleged crime and jurisdiction.

</doc>
<doc id="651" url="?curid=651" title="America the Beautiful">
America the Beautiful

"America the Beautiful" is a patriotic American song. Its lyrics were written by Katharine Lee Bates and its music was composed by church organist and choirmaster Samuel A. Ward at Grace Episcopal Church in Newark, New Jersey. The two never met.
Bates wrote the words as a poem, originally titled "Pikes Peak". It was first published in the Fourth of July 1895 edition of the church periodical, "The Congregationalist". At that time, the poem was titled "America".
Ward had initially composed the song's melody in 1882 to accompany lyrics to "Materna", basis of the hymn, "O Mother dear, Jerusalem", though the hymn was not first published until 1892. The combination of Ward's melody and Bates's poem was first entitled "America the Beautiful" in 1910. The song is one of the most popular of the many American patriotic songs.
History.
In 1893, at the age of 33, Bates, an English professor at Wellesley College, had taken a train trip to Colorado Springs, Colorado, to teach at Colorado College. Several of the sights on her trip inspired her, and they found their way into her poem, including the World's Columbian Exposition in Chicago, the "White City" with its promise of the future contained within its gleaming white buildings; the wheat fields of North America's heartland Kansas, through which her train was riding on July 16; and the majestic view of the Great Plains from high atop Pikes Peak.
On the pinnacle of that mountain, the words of the poem started to come to her, and she wrote them down upon returning to her hotel room at the original Antlers Hotel. The poem was initially published two years later in "The Congregationalist" to commemorate the Fourth of July. It quickly caught the public's fancy. An amended version was published in 1904.
The first known melody written for the song was sent in by Silas Pratt when the poem was published in "The Congregationalist". By 1900, at least 75 different melodies had been written. A hymn tune composed in 1882 by Samuel A. Ward, the organist and choir director at Grace Church, Newark, was generally considered the best music as early as 1910 and is still the popular tune today. Just as Bates had been inspired to write her poem, Ward, too, was inspired. The tune came to him while he was on a ferryboat trip from Coney Island back to his home in New York City after a leisurely summer day and he immediately wrote it down. He composed the tune for the old hymn "O Mother Dear, Jerusalem", retitling the work "Materna". Ward's music combined with Bates's poem were first published together in 1910 and titled "America the Beautiful".
Ward died in 1903, not knowing the national stature his music would attain. The song's popularity was well established by the time of Bates's death in 1929. It is included in songbooks in many religious congregations in the United States.
At various times in the more than one hundred years that have elapsed since the song was written, particularly during the John F. Kennedy administration, there have been efforts to give "America the Beautiful" legal status either as a national hymn or as a national anthem equal to, or in place of, "The Star-Spangled Banner", but so far this has not succeeded. Proponents prefer "America the Beautiful" for various reasons, saying it is easier to sing, more melodic, and more adaptable to new orchestrations while still remaining as easily recognizable as "The Star-Spangled Banner". Some prefer "America the Beautiful" over "The Star-Spangled Banner" due to the latter's war-oriented imagery, while others object to the implicit support of slavery and racism in its third verse; others prefer "The Star-Spangled Banner" "because" of its war themes. While that national dichotomy has stymied any effort at changing the tradition of the national anthem, "America the Beautiful" continues to be held in high esteem by a large number of Americans, and was even being considered before 1931 as a candidate to become the national anthem of the United States.
Notable performances.
Elvis Presley performed it many times in concerts starting in 1976.
Bing Crosby included the song in a medley on his album "101 Gang Songs" (1961).
Frank Sinatra recorded the song with Nelson Riddle during the sessions for "The Concert Sinatra" in February 1963, for a projected 45 single release. The 45 was not commercially issued however, but the song was later added as a bonus track to the enhanced 2012 CD release of "The Concert Sinatra".
In 1976, while the United States celebrated its bicentennial, a soulful version popularized by Ray Charles peaked at number 98 on the US R&amp;B chart. His version was traditionally played on New Year's Eve in Times Square following the ball drop.
Three different renditions of the song have entered the Hot Country Songs charts. The first was by Charlie Rich, which went to number 22 in 1976. A second, by Mickey Newbury, peaked at number 82 in 1980. Aretha Franklin performed a rendition before an undisputed audience of 93,173 to open WrestleMania III, a performance meta-critic RJ City called "a lovely version". An all-star version of "America the Beautiful" performed by country singers Trace Adkins, Sherrié Austin, Billy Dean, Vince Gill, Carolyn Dawn Johnson, Toby Keith, Brenda Lee, Lonestar, Lyle Lovett, Lila McCann, Lorrie Morgan, Jamie O'Neal, The Oak Ridge Boys, Collin Raye, Kenny Rogers, Keith Urban and Phil Vassar reached number 58 in July 2001. The song re-entered the chart following the September 11 attacks.
Barbra Streisand released an official music video footage during Norman Lear's Special in 1982.
During her rise to stardom, R&amp;B singer Mariah Carey sang the song at the 1990 NBA Finals.
Whitney Houston also recorded the song, covering Ray Charles' soulful rearranged version as the b-side to her 1991 rendition of "The Star Spangled Banner."
The song has been performed as part of the Indianapolis 500 pre-race ceremonies since 1991.
The US singer/songwriter Martin Sexton recorded a gospel-tinged version on his LP "Black Sheep," released in 1996.
Popularity of the song increased greatly in the decades following 9/11; at some sporting events it was sung in addition to the traditional singing of the national anthem. During the first taping of the "Late Show with David Letterman" following the attacks, CBS newsman Dan Rather cried briefly as he quoted the fourth verse.
The hymn has been featured in the pregame for a number of Super Bowls, the championship game for each NFL season. It is sung along with the "Star-Spangled Banner" and, more recently, the hymn "Lift Every Voice and Sing," commonly referred to as the "Black national anthem". For Super Bowl XLVIII, The Coca-Cola Company aired a multilingual version of the song, sung in several different languages. The commercial received some criticism on social media sites, such as Twitter and Facebook, and from some conservatives, such as Glenn Beck. Despite the controversies, Coca-Cola later reused the Super Bowl ad during Super Bowl LI, the opening ceremonies of the 2014 Winter Olympics and 2016 Summer Olympics and for patriotic holidays. Notable performers at the Super Bowl include Ray Charles, Alicia Keys, John Legend, Faith Hill, Mary J. Blige with Marc Anthony, Blake Shelton with Miranda Lambert, Queen Latifah, Leslie Odom Jr., and Babyface. Post Malone performed the song for the most recent game, Super Bowl LVIII, in 2024.
The song, performed by 5 Alarm Music, is featured heavily in a dystopian action horror franchise The Purge in both trailers and films.
In 2016, American five-piece girl group Fifth Harmony performed a rendition to honor the United States women's national soccer team on defeating Japan 5–2 in the Final to win the 2015 FIFA Women's World Cup last July at BC Place in Vancouver, British Columbia, Canada before an undisputed AT&amp;T Stadium audience of 101,763 to open WrestleMania 32 in Dallas, Texas.
In 2017, Jackie Evancho released "Together We Stand", a disc containing three patriotic songs including "America the Beautiful." The song charted at No. 4 on "Billboard's" Classical Digital Song sales chart.
An abbreviated cover with the 1911 lyrics was performed by Greg Jong for the soundtrack of the 2020 video game "Wasteland 3" and is played during the final hostile encounters in the Denver section.
In 2021, Jennifer Lopez performed the song at the inauguration of Joe Biden, as the second half of a medley with "This Land Is Your Land" by Woody Guthrie.
In 2023, Cécile McLorin Salvant performed the song at the US Open woman's final. In her rendition, Salvant notably skipped ahead to the lyrics of the second half of the second verse while singing the first verse (replacing "God shed His grace on thee..." with "God mend thine every flaw...", etc.). Jazz Critic Nate Chinen wrote the following day of the performance, "What does it mean for a singer such as Salvant to inhabit a platform like the US Open, and implore God to mend America’s every flaw? What does it mean, in the Year of Our Lord 2023, for a singer like Salvant to urge the nation to confirm thy soul in self-control, and find liberty in law? I’m not going to spell it out, but it means a lot."
In 2024 Christopher Macchio sang the song to close the 2024 Republican Party's national convention in Milwaukee, Wisconsin.
Idioms.
"From sea to shining sea", originally used in the charters of some of the English colonies in North America, is an American idiom meaning "from the Atlantic Ocean to the Pacific Ocean" (or vice versa). Other songs that have used this phrase include the American patriotic song "God Bless the U.S.A." and Schoolhouse Rock's "Elbow Room". The phrase and the song are also the namesake of the Shining Sea Bikeway, a bike path in Bates's hometown of Falmouth, Massachusetts. The phrase is similar to the Latin phrase "" ("From sea to sea"), which is the official motto of Canada.
"Purple mountain majesties" refers to the shade of Pikes Peak in Colorado Springs, Colorado, which inspired Bates to write the poem. The idiom inspired the Colorado Rockies to have purple as one of its team colors.
In 2003, Tori Amos appropriated the phrase "for amber waves of grain" to create a personification for her song "Amber Waves". Amos imagines Amber Waves as an exotic dancer, like the character of the same name portrayed by Julianne Moore in "Boogie Nights".
Books.
Lynn Sherr's 2001 book "America the Beautiful: The Stirring True Story Behind Our Nation's Favorite Song" discusses the origins of the song and the backgrounds of its authors in depth. The book points out that the poem has the same meter as that of "Auld Lang Syne"; the songs can be sung interchangeably. Additionally, Sherr discusses the evolution of the lyrics, for instance, changes to the original third verse written by Bates.
Melinda M. Ponder, in her 2017 biography "Katharine Lee Bates: From Sea to Shining Sea", draws heavily on Bates's diaries and letters to trace the history of the poem and its place in American culture.

</doc>
<doc id="653" url="?curid=653" title="Assistive technology">
Assistive technology

Assistive technology (AT) is a term for assistive, adaptive, and rehabilitative devices for people with disabilities and the elderly. Disabled people often have difficulty performing activities of daily living (ADLs) independently, or even with assistance. ADLs are self-care activities that include toileting, mobility (ambulation), eating, bathing, dressing, grooming, and personal device care. Assistive technology can ameliorate the effects of disabilities that limit the ability to perform ADLs. Assistive technology promotes greater independence by enabling people to perform tasks they were formerly unable to accomplish, or had great difficulty accomplishing, by providing enhancements to, or changing methods of interacting with, the technology needed to accomplish such tasks. For example, wheelchairs provide independent mobility for those who cannot walk, while assistive eating devices can enable people who cannot feed themselves to do so. Due to assistive technology, disabled people have an opportunity of a more positive and easygoing lifestyle, with an increase in "social participation", "security and control", and a greater chance to "reduce institutional costs without significantly increasing household expenses." In schools, assistive technology can be critical in allowing students with disabilities to access the general education curriculum. Students who experience challenges writing or keyboarding, for example, can use voice recognition software instead. Assistive technologies assist people who are recovering from strokes and people who have sustained injuries that affect their daily tasks.
Adaptive technology.
Adaptive technology and assistive technology are different. "Assistive technology" is something that is used to help disabled people, while "adaptive technology" covers items that are specifically designed for disabled people and would seldom be used by a non-disabled person. In other words, assistive technology is any object or system that helps people with disabilities, while adaptive technology is specifically designed for disabled people. Consequently, adaptive technology is a subset of assistive technology. Adaptive technology often refers specifically to electronic and information technology access.
Occupational therapy and assisitve technology.
Occupational Therapy (OT) utilizes everyday occupations as a therapeutic tool for enhancing or enabling participation in healthy occupations to promote health and well-being (AOTA, 2020). Occupations include activities of daily living (ADLs), instrumental activities of daily living (IADLs), health management, rest and sleep, education, work, play, leisure, and social participation (AOTA, 2020).  “As occupational therapy professionals, we are uniquely trained to advocate for client-centered care that reduces barriers to participation in meaningful occupations and promotes overall well-being" (Clark, Iqbal &amp; Myers, 2022). 
OT practitioners (OTP) utilize assistive technologies (AT) to modify environments and promote access and fit to facilitate independence. For example, voice activated smart home technology allows an individual to control devices such as light switches, thermostat, oven, blinds, and music from their location.  OTP evaluate client’s strengths and abilities and connects with desired tasks.  OTP help empower the client to match specific goals to AT tools. The theoretical approaches or frameworks OTPs frequently use to guide a client’s AT choices may include: 1) The HAAT model by Cook, Polgar &amp; Encarnaçāo (2015) 2) The interdependence - Human Activity Assistive Technology Model (I-HAAT) by Lee, et al. (2020); 3) The SETT Framework by Zabala (2005); or 4) The Unified Theory of Acceptance and Use of Technology (UTAUT 2) by Venkatesh, Thong &amp; Xu (2012). Also, OTPs may seek advanced training through the Rehabilitation Engineering and Assistive Technology Society of North America (RESNA) organization to receive their Assistive Technology Professional (ATP) Certification and/or Seating and Mobility Specialist (SMS) Certification. Additional trainings and certifications may specialize in a focus area such as the Certified Assistive Technology Instructional Specialist for Individuals with Visual Impairments (CATIS™) (ACVREP, 2024). 
Mobility impairments.
Wheelchairs.
Wheelchairs are devices that can be manually propelled or electrically propelled, and that include a seating system and are designed to be a substitute for the normal mobility that most people have. Wheelchairs and other mobility devices allow people to perform mobility-related activities of daily living which include feeding, toileting, dressing, grooming, and bathing. The devices come in a number of variations where they can be propelled either by hand or by motors where the occupant uses electrical controls to manage motors and seating control actuators through a joystick, sip-and-puff control, head switches or other input devices. Often there are handles behind the seat for someone else to do the pushing or input devices for caregivers. Wheelchairs are used by people for whom walking is difficult or impossible due to illness, injury, or disability. People with both sitting and walking disability often need to use a wheelchair or walker.
Newer advancements in wheelchair design enable wheelchairs to climb stairs, go off-road or propel using segway technology or additional add-ons like handbikes or power assists.
Transfer devices.
Patient transfer devices generally allow patients with impaired mobility to be moved by caregivers between beds, wheelchairs, commodes, toilets, chairs, stretchers, shower benches, automobiles, swimming pools, and other patient support systems (i.e., radiology, surgical, or examining tables).
The most common devices are transfer benches, stretcher or convertible chairs (for lateral, supine transfer), sit-to-stand lifts (for moving patients from one seated position to another i.e., from wheelchairs to commodes), air bearing inflatable mattresses (for supine transfer i.e., transfer from a gurney to an operating room table), gait belts (or transfer belt) and a slider board (or transfer board), usually used for transfer from a bed to a wheelchair or from a bed to an operating table. Highly dependent patients who cannot assist their caregiver in moving them often require a patient lift (a floor or ceiling-suspended sling lift) which though invented in 1955 and in common use since the early 1960s is still considered the state-of-the-art transfer device by OSHA and the American Nursing Association.
Walkers.
A walker or walking frame or Rollator is a tool for disabled people who need additional support to maintain balance or stability while walking. It consists of a frame that is about waist high, approximately twelve inches deep and slightly wider than the user. Walkers are also available in other sizes, such as for children, or for heavy people. Modern walkers are height-adjustable. The front two legs of the walker may or may not have wheels attached depending on the strength and abilities of the person using it. It is also common to see caster wheels or glides on the back legs of a walker with wheels on the front.
Treadmills.
Bodyweight-supported treadmill training (BWSTT) is used to enhance walking ability of people with neurological injury. These machines are therapist-assisted devices that are used in the clinical setting, but is limited by the personnel and labor requirements placed on physical therapists. The BWSTT device, and many others like it, assist physical therapists by providing task-specific practice of walking in people following neurological injury.
Prosthesis.
A prosthesis, prosthetic, or prosthetic limb is a device that replaces a missing body part. It is part of the field of biomechatronics, the science of using mechanical devices with human muscular, musculoskeletal, and nervous systems to assist or enhance motor control lost by trauma, disease, or defect. Prostheses are typically used to replace parts lost by injury (traumatic) or missing from birth (congenital) or to supplement defective body parts. Inside the body, artificial heart valves are in common use with artificial hearts and lungs seeing less common use but under active technology development. Other medical devices and aids that can be considered prosthetics include hearing aids, artificial eyes, palatal obturator, gastric bands, and dentures.
Prostheses are specifically not orthoses, although given certain circumstances a prosthesis might end up performing some or all of the same functionary benefits as an orthosis. Prostheses are technically the complete finished item. For instance, a C-Leg knee alone is not a prosthesis, but only a prosthetic component. The complete prosthesis would consist of the attachment system  to the residual limb – usually a "socket", and all the attachment hardware components all the way down to and including the terminal device. Despite the technical difference, the terms are often used interchangeably.
The terms "prosthetic" and "orthotic" are adjectives used to describe devices such as a prosthetic knee. The terms "prosthetics" and "orthotics" are used to describe the respective allied health fields.
An Occupational Therapist's role in prosthetics include therapy, training and evaluations. Prosthetic training includes orientation to prosthetics components and terminology, donning and doffing, wearing schedule, and how to care for residual limb and the prosthesis.
Exoskeletons.
A powered exoskeleton is a wearable mobile machine that is powered by a system of electric motors, pneumatics, levers, hydraulics, or a combination of technologies that allow for limb movement with increased strength and endurance. Its design aims to provide back support, sense the user's motion, and send a signal to motors which manage the gears. The exoskeleton supports the shoulder, waist and thigh, and assists movement for lifting and holding heavy items, while lowering back stress.
Adaptive seating and positioning.
People with balance and motor function challenges often need specialized equipment to sit or stand safely and securely. This equipment is frequently specialized for specific settings such as in a classroom or nursing home.  Positioning is often important in seating arrangements to ensure that user's body pressure is distributed equally without inhibiting movement in a desired way.
Positioning devices have been developed to aid in allowing people to stand and bear weight on their legs without risk of a fall.  These standers are generally grouped into two categories based on the position of the occupant.  Prone standers distribute the body weight to the front of the individual and usually have a tray in front of them.  This makes them good for users who are actively trying to carry out some task.  Supine standers distribute the body weight to the back and are good for cases where the user has more limited mobility or is recovering from injury.
For children.
Children with severe disabilities can develop learned helplessness, which makes them lose interest in their environment. Robotic arms are used to provide an alternative method to engage in joint play activities. These robotic arms allow children to manipulate real objects in the context of play activities.
Children with disabilities have challenges in accessing play and social interactions. Play is essential for the physical, emotional, and social well-being of all children. The use of assistive technology has been recommended to facilitate the communication, mobility, and independence of children with disabilities. Augmentative Alternative Communication (AAC) devices have been shown to facilitate the growth and development of language as well as increase rates of symbolic play in children with cognitive disabilities. AAC devices can be no-tech (sign language and body language), low-tech (picture boards, paper and pencils), or high-tech (tablets and speech generating devices). The choice of AAC device is very important and should be determined on a case-by-case basis by speech therapists and assistive technology professionals. The early introduction of powered mobility has been shown to positively impact the play and psychosocial skills of children who are unable to move independently. Powered cars, such as the Go Baby Go program, have emerged as a cost-effective means of facilitating the inclusion of children with mobility impairments in school. 
Visual impairments.
Many people with serious visual impairments live independently, using a wide range of tools and techniques. Examples of assistive technology for visually impairment include screen readers, screen magnifiers, Braille embossers, desktop video magnifiers, and voice recorders.
Screen readers.
Screen readers are used to help the visually impaired to easily access electronic information. These software programs run on a computer to convey the displayed information through voice (text-to-speech) or braille (refreshable braille displays) in combination with magnification for low vision users in some cases. There are a variety of platforms and applications available for a variety of costs with differing feature sets.
Some example of screen readers are Apple VoiceOver, Google TalkBack and Microsoft Narrator. 
Screen readers may rely on the assistance of text-to-speech tools. To use the text-to-speech tools, the documents must be in an electronic form, which is uploaded as the digital format. However, people usually will use the hard copy documents scanned into the computer, which cannot be recognized by the text-to-speech software. To solve this issue, people often use Optical Character Recognition technology accompanied with text-to-speech software.
Braille and braille technology.
Braille is a system of raised dots formed into units called braille cells. A full braille cell is made up of six dots, with two parallel rows of three dots, but other combinations and quantities of dots represent other letters, numbers, punctuation marks, or words. People can then use their fingers to read the code of raised dots. Assistive technology using braille is called braille technology.
Braille translator.
A braille translator is a computer program that can translate inkprint into braille or braille into inkprint. A braille translator can be an app on a computer or be built into a website, a smartphone, or a braille device.
Braille embosser.
A braille embosser is, simply put, a printer for braille. Instead of a standard printer adding ink onto a page, the braille embosser imprints the raised dots of braille onto a page. Some braille embossers combine both braille and ink so the documents can be read with either sight or touch.
Refreshable braille display.
A refreshable braille display or braille terminal is an electro-mechanical device for displaying braille characters, usually by means of round-tipped pins raised through holes in a flat surface. Computer users who cannot use a computer monitor use it to read a braille output version of the displayed text.
Desktop video magnifier.
Desktop video magnifiers are electronic devices that use a camera and a display screen to perform digital magnification of printed materials. They enlarge printed pages for those with low vision. A camera connects to a monitor that displays real-time images, and the user can control settings such as magnification, focus, contrast, underlining, highlighting, and other screen preferences. They come in a variety of sizes and styles; some are small and portable with handheld cameras, while others are much larger and mounted on a fixed stand.
Screen magnification software.
A screen magnifier is software that interfaces with a computer's graphical output to present enlarged screen content. It allows users to enlarge the texts and graphics on their computer screens for easier viewing. Similar to desktop video magnifiers, this technology assists people with low vision. After the user loads the software into their computer's memory, it serves as a kind of "computer magnifying glass". Wherever the computer cursor moves, it enlarges the area around it. This allows greater computer accessibility for a wide range of visual abilities.
Large-print and tactile keyboards.
A large-print keyboard has large letters printed on the keys. On the keyboard shown, the round buttons at the top control software which can magnify the screen (zoom in), change the background color of the screen, or make the mouse cursor on the screen larger. The "bump dots" on the keys, installed in this case by the organization using the keyboards, help the user find the right keys in a tactile way.
Navigation assistance.
Assistive technology for navigation has exploded on the IEEE Xplore database since 2000, with over 7,500 engineering articles written on assistive technologies and visual impairment in the past 25 years, and over 1,300 articles on solving the problem of navigation for people who are blind or visually impaired. As well, over 600 articles on augmented reality and visual impairment have appeared in the engineering literature since 2000. Most of these articles were published within the past five years, and the number of articles in this area is increasing every year. GPS, accelerometers, gyroscopes, and cameras can pinpoint the exact location of the user and provide information on what is in the immediate vicinity, and assistance in getting to a destination.
Wearable technology.
Wearable technology are smart electronic devices that can be worn on the body as an implant or an accessory. New technologies are exploring how the visually impaired can receive visual information through wearable devices.
Some wearable devices for visual impairment include: OrCam device, eSight and Brainport.
Personal emergency response systems.
Personal emergency response systems (PERS), or Telecare (UK term), are a particular sort of assistive technology that use electronic sensors connected to an alarm system to help caregivers manage risk and help vulnerable people stay independent at home longer. An example would be the systems being put in place for senior people such as fall detectors, thermometers (for hypothermia risk), flooding and unlit gas sensors (for people with mild dementia). Notably, these alerts can be customized to the particular person's risks. When the alert is triggered, a message is sent to a caregiver or contact center who can respond appropriately.
Accessibility software.
In human–computer interaction, computer accessibility (also known as accessible computing) refers to the accessibility of a computer system to all people, regardless of disability or severity of impairment, examples include web accessibility guidelines. Another approach is for the user to present a token to the computer terminal, such as a smart card, that has configuration information to adjust the computer speed, text size, etc. to their particular needs. This is useful where users want to access public computer based terminals in Libraries, ATM, Information kiosks etc. The concept is encompassed by the CEN EN 1332-4 Identification Card Systems – Man-Machine Interface. This development of this standard has been supported in Europe by SNAPI and has been successfully incorporated into the Lasseo specifications, but with limited success due to the lack of interest from public computer terminal suppliers.
Hearing impairments.
People in the d/Deaf and hard of hearing community have a more difficult time receiving auditory information as compared to hearing individuals. These individuals often rely on visual and tactile mediums for receiving and communicating information. The use of assistive technology and devices provides this community with various solutions to auditory communication needs by providing higher sound (for those who are hard of hearing), tactile feedback, visual cues and improved technology access. Individuals who are deaf or hard of hearing use a variety of assistive technologies that provide them with different access to information in numerous environments. Most devices either provide amplified sound or alternate ways to access information through vision and/or vibration. These technologies can be grouped into three general categories: Hearing Technology, alerting devices, and communication support.
Hearing aids.
A hearing aid or deaf aid is an electro-acoustic device which is designed to amplify sound for the wearer, usually with the aim of making speech more intelligible, and to correct impaired hearing as measured by audiometry. This type of assistive technology helps people with hearing loss participate more fully in their hearing communities by allowing them to hear more clearly. They amplify any and all sound waves through use of a microphone, amplifier, and speaker. There is a wide variety of hearing aids available, including digital, in-the-ear, in-the-canal, behind-the-ear, and on-the-body aids.
Assistive listening devices.
Assistive listening devices include FM, infrared, and loop assistive listening devices. This type of technology allows people with hearing difficulties to focus on a speaker or subject by getting rid of extra background noises and distractions, making places like auditoriums, classrooms, and meetings much easier to participate in. The assistive listening device usually uses a microphone to capture an audio source near to its origin and broadcast it wirelessly over an FM (Frequency Modulation) transmission, IR (Infra Red) transmission, IL (Induction Loop) transmission, or other transmission methods. The person who is listening may use an FM/IR/IL Receiver to tune into the signal and listen at his/her preferred volume.
Amplified telephone equipment.
This type of assistive technology allows users to amplify the volume and clarity of their phone calls so that they can easily partake in this medium of communication. There are also options to adjust the frequency and tone of a call to suit their individual hearing needs. Additionally, there is a wide variety of amplified telephones to choose from, with different degrees of amplification. For example, a phone with 26 to 40 decibel is generally sufficient for mild hearing loss, while a phone with 71 to 90 decibel is better for more severe hearing loss.
Augmentative and alternative communication.
Augmentative and alternative communication (AAC) is an umbrella term that encompasses methods of communication for those with impairments or restrictions on the production or comprehension of spoken or written language. AAC systems are extremely diverse and depend on the capabilities of the user. They may be as basic as pictures on a board that are used to request food, drink, or other care; or they can be advanced speech generating devices, based on speech synthesis, that are capable of storing hundreds of phrases and words.
Cognitive impairments.
Assistive Technology for Cognition (ATC) is the use of technology (usually high tech) to augment and assist cognitive processes such as attention, memory, self-regulation, navigation, emotion recognition and management, planning, and sequencing activity. Systematic reviews of the field have found that the number of ATC are growing rapidly, but have focused on memory and planning, that there is emerging evidence for efficacy, that a lot of scope exists to develop new ATC. Examples of ATC include: NeuroPage which prompts users about meetings, Wakamaru, which provides companionship and reminds users to take medicine and calls for help if something is wrong, and telephone Reassurance systems.
Memory aids.
Memory aids are any type of assistive technology that helps a user learn and remember certain information. Many memory aids are used for cognitive impairments such as reading, writing, or organizational difficulties. For example, a Smartpen records handwritten notes by creating both a digital copy and an audio recording of the text. Users simply tap certain parts of their notes, the pen saves it, and reads it back to them. From there, the user can also download their notes onto a computer for increased accessibility. Digital voice recorders are also used to record "in the moment" information for fast and easy recall at a later time.
A 2017 Cochrane Review highlighted the current lack of high-quality evidence to determine whether assistive technology effectively supports people with dementia to manage memory issues. Thus, it is not presently sure whether or not assistive technology is beneficial for memory problems.
Educational software.
Educational software is software that assists people with reading, learning, comprehension, and organizational difficulties. Any accommodation software such as text readers, notetakers, text enlargers, organization tools, word predictions, and talking word processors falls under the category of educational software.
Eating impairments.
Adaptive eating devices include items commonly used by the general population like spoons and forks and plates. However they become assistive technology when they are modified to accommodate the needs of people who have difficulty using standard cutlery due to a disabling condition. Common modifications include increasing the size of the utensil handle to make it easier to grasp. Plates and bowls may have a guard on the edge that stops food being pushed off of the dish when it is being scooped. More sophisticated equipment for eating includes manual and powered feeding devices. These devices support those who have little or no hand and arm function and enable them to eat independently.
In sports.
Assistive technology in sports is an area of technology design that is growing. Assistive technology is the array of new devices created to enable sports enthusiasts who have disabilities to play. Assistive technology may be used in adaptive sports, where an existing sport is modified to enable players with a disability to participate; or, assistive technology may be used to invent completely new sports with athletes with disabilities exclusively in mind.
An increasing number of people with disabilities are participating in sports, leading to the development of new assistive technology. Assistive technology devices can be simple, or "low-technology", or they may use highly advanced technology. "Low-tech" devices can include velcro gloves and adaptive bands and tubes. "High-tech" devices can include all-terrain wheelchairs and adaptive bicycles. Accordingly, assistive technology can be found in sports ranging from local community recreation to the elite Paralympic Games. More complex assistive technology devices have been developed over time, and as a result, sports for people with disabilities "have changed from being a clinical therapeutic tool to an increasingly competition-oriented activity".
In education.
In the United States there are two major pieces of legislation that govern the use of assistive technology within the school system. The first is Section 504 of the Rehabilitation Act of 1973 and the second being the Individuals with Disabilities Education Act (IDEA) which was first enacted in 1975 under the name The Education for All Handicapped Children Act. In 2004, during the reauthorization period for IDEA, the National Instructional Material Access Center (NIMAC) was created which provided a repository of accessible text including publisher's textbooks to students with a qualifying disability. Files provided are in XML format and used as a starting platform for braille readers, screen readers, and other digital text software. IDEA defines assistive technology as follows: "any item, piece of equipment, or product system, whether acquired commercially off the shelf, modified, or customized, that is used to increase, maintain, or improve functional capabilities of a child with a disability. (B) Exception.--The term does not include a medical device that is surgically implanted, or the replacement of such device."
Assistive technology listed is a student's IEP is not only recommended, it is required (Koch, 2017). These devices help students both with and without disabilities access the curriculum in a way they were previously unable to (Koch, 2017). Occupational therapists play an important role in educating students, parents and teachers about the assistive technology they may interact with.
Assistive technology in this area is broken down into low, mid, and high tech categories. Low tech encompasses equipment that is often low cost and does not include batteries or requires charging. Examples include adapted paper and pencil grips for writing or masks and color overlays for reading. Mid tech supports used in the school setting include the use of handheld spelling dictionaries and portable word processors used to keyboard writing. High tech supports involve the use of tablet devices and computers with accompanying software. Software supports for writing include the use of auditory feedback while keyboarding, word prediction for spelling, and speech to text. Supports for reading include the use of text to speech (TTS) software and font modification via access to digital text. Limited supports are available for math instruction and mostly consist of grid based software to allow younger students to keyboard equations and auditory feedback of more complex equations using MathML and Daisy.
Computer accessibility.
One of the largest problems that affect disabled people is discomfort with prostheses. An experiment performed in Massachusetts used 20 people with various sensors attached to their arms. The subjects tried different arm exercises, and the sensors recorded their movements. All of the data helped engineers develop new engineering concepts for prosthetics.
Assistive technology may attempt to improve the ergonomics of the devices themselves such as Dvorak and other alternative keyboard layouts, which offer more ergonomic layouts of the keys.
Assistive technology devices have been created to enable disabled people to use modern touch screen mobile computers such as the iPad, iPhone and iPod Touch. The Pererro is a plug and play adapter for iOS devices which uses the built in Apple VoiceOver feature in combination with a basic switch. This brings touch screen technology to those who were previously unable to use it. Apple, with the release of iOS 7 had introduced the ability to navigate apps using switch control. Switch access could be activated either through an external bluetooth connected switch, single touch of the screen, or use of right and left head turns using the device's camera. Additional accessibility features include the use of Assistive Touch which allows a user to access multi-touch gestures through pre-programmed onscreen buttons.
For users with physical disabilities a large variety of switches are available and customizable to the user's needs varying in size, shape, or amount of pressure required for activation. Switch access may be placed near any area of the body which has consistent and reliable mobility and less subject to fatigue. Common sites include the hands, head, and feet. Eye gaze and head mouse systems can also be used as an alternative mouse navigation. A user may use single or multiple switch sites and the process often involves a scanning through items on a screen and activating the switch once the desired object is highlighted.
Home automation.
The form of home automation called assistive domotics focuses on making it possible for elderly and disabled people to live independently. Home automation is becoming a viable option for the elderly and disabled who would prefer to stay in their own homes rather than move to a healthcare facility. This field uses much of the same technology and equipment as home automation for security, entertainment, and energy conservation but tailors it towards elderly and disabled users. For example, automated prompts and reminders use motion sensors and pre-recorded audio messages; an automated prompt in the kitchen may remind the resident to turn off the oven, and one by the front door may remind the resident to lock the door.
Assistive technology and innovation.
Innovation is happening in assistive technology either through improvements to existing devices or the creation of new products.
In the WIPO published 2021 report on Technology Trends, assistive products are grouped into either conventional or emerging technologies. Conventional assisting technology tracks innovation within well-established assistive products, whereas emerging assistive technology refers to more advanced products. These identified advanced assistive products are distinguished from the conventional ones by the use of one or more enabling technologies (for instance, artificial intelligence, Internet of things, advanced sensors, new material, Additive Manufacturing, advanced robotics, augmented and virtual reality) or by the inclusion of implantable products/components. Such emerging assistive products are either more sophisticated or more functional versions of conventional assistive products, or completely novel assistive devices.
For instance, in conventional self-care assistive technology, technologies involved typically include adaptive clothing, adaptive eating devices, incontinence products, assistive products for manicure, pedicure, hair and facial care, dental care, or assistive products for sexual activities. In comparison, emerging self-care assistive technologies include health and emotion monitoring, smart diapers, smart medication dispensing and management or feeding assistant robot. Although the distinction between conventional and emerging technologies is not always clear-cut, emerging assistive technology tends to be "smarter", using AI and being more connected and interactive, and including body-integrated solutions or components.
To a great extent this « conventional » versus « emerging » classification is based on the WHO's Priority Assistive Products List and the ISO 9999 standard for assistive products for persons with disabilities, the APL delineating the absolute minimum that countries should be offering to their citizens and ISO 9999 defining those products which are already well established in the market.
This "well-established status" is reflected in the patent filings between 2013 and 2017. Patent registrations for assistive technologies identified as conventional are nearly eight times larger than the ones for emerging assistive technologies. However, patent filings related to more recent emerging assistive technologies are growing almost three times as fast as those pertaining to conventional ones. Patent filings in both conventional and emerging assistive technology are highly concentrated on mobility, hearing and vision. Investment in emerging assistive technology also focuses on environment. In the conventional sector, mobility represent 54% of all patents fillings, and is an indication of increased interest in advanced mobility assistive product categories, such as advanced prosthetics, walking aids, wheelchairs, and exoskeletons.
In the past, the top patent offices for filing, and therefore perceived target markets, in assistive technology have been the U.S. and Japan. Patenting activity has, however, been declining in these two jurisdictions. At the same time, there has been a surge in patent filings in China and an increase in filings in the Republic of Korea. This pattern is observed for both conventional and emerging assistive technology, with China's annual filings surpassing those of the U.S. in 2008 for conventional and 2014 for emerging assistive technology. Patent filings related to conventional assistive technology have also declined in Europe, especially in Germany, France, the Netherlands and Norway.
Patenting activity indicates the amount of interest and the investment made in respect to an invention's applicability and its commercialization potential. There is typically a lag between filing a patent application and commercialization, with a product being classified in various stages of readiness levels, research concept, proof of concept, minimum viable product and finally commercial product. According to the 2021 WIPO report, the emerging technologies closest to a fully commercial product were for example:
The technology readiness level and the related patenting activity can also be explained through the following factors which contribute to a product's entry to market, such as the expected impact on a person's participation in different aspects of life, the ease of adoption (need for training, fitting, additional equipment for interoperability, and so on), the societal acceptance and potential ethical concerns, and the need for regulatory approval. This is mainly the case for assistive technology that qualifies as medical technology.
Among these aspects, acceptability and ethical considerations are particularly relevant to those technologies that are extremely invasive (such as cortical or auditory brainstem implants), or replace the human caregiver and human interaction, or collect and use data on cloud-based services or interconnected devices (e.g., companion robots, smart nursing and health-monitoring technologies), raising privacy issues and requiring connectivity, or raise safety concerns, such as autonomous wheelchairs.
Beyond the patent landscape, industrial designs have an added importance for the field of assistive technology. Assistive technology is often not adopted, or else abandoned entirely, because of issues to do with design (lack of appeal) or comfort (poor ergonomics). Design often plays a role after the patenting activity, as a product needs to be re-designed for mass production.
Impacts.
Overall, assistive technology aims to allow disabled people to "participate more fully in all aspects of life (home, school, and community)" and increases their opportunities for "education, social interactions, and potential for meaningful employment". It creates greater independence and control for disabled individuals. For example, in one study of 1,342 infants, toddlers and preschoolers, all with some kind of developmental, physical, sensory, or cognitive disability, the use of assistive technology created improvements in child development. These included improvements in "cognitive, social, communication, literacy, motor, adaptive, and increases in engagement in learning activities". Additionally, it has been found to lighten caregiver load. Both family and professional caregivers benefit from assistive technology. Through its use, the time that a family member or friend would need to care for a patient significantly decreases. However, studies show that care time for a professional caregiver increases when assistive technology is used. Nonetheless, their work load is significantly easier as the assistive technology frees them of having to perform certain tasks. There are several platforms that use machine learning to identify the appropriate assistive device to suggest to patients, making assistive devices more accessible.
History.
In 1988 the National institute on disability and rehabilitation research, NIDRR, awarded Gaulladet University a grant for the project "Robotic finger spelling hand for communication and access to text by deaf-blind persons". Researchers at the university developed and tested a robotic hand. Although it was never commercialized the concept is relevant for current and future research.
Since this grant, many others have been written. NIDRR funded research appears to be moving from the fabrication of robotic arms that can be used by disabled persons to perform daily activities, to developing robotics that assist with therapy in the hopes of achieving long-term performance gains. If there is success in development of robotics, these mass-marketed products could assist tomorrow's longer-living elderly individuals enough to postpone nursing home stays. "Jim Osborn, executive director of the Quality of Life Technology Center, told a 2007 gathering of long-term care providers that if such advances could delay all nursing home admissions by a month, societal savings could be $1 billion monthly". Shortage of both paid personal assistants and available family members makes artificial assistance a necessity.
rATA Tool by World Health Organization.
The rapid assistive technology assessment (rATA) is a tool developed by World Health Organization in order to undertake household surveys which can measure various parameters needed to access assistive technology and to make informed policies for governments around the world. 

</doc>
<doc id="654" url="?curid=654" title="Accessible computing">
Accessible computing


</doc>
<doc id="655" url="?curid=655" title="Abacus">
Abacus

An abacus (: abaci or abacuses), also called a counting frame, is a hand-operated calculating tool which was used from ancient times in the ancient Near East, Europe, China, and Russia, until the adoption of the Arabic numeral system. An abacus consists of a two-dimensional array of slidable beads (or similar objects). In their earliest designs, the beads could be loose on a flat surface or sliding in grooves. Later the beads were made to slide on rods and built into a frame, allowing faster manipulation.
Each rod typically represents one digit of a multi-digit number laid out using a positional numeral system such as base ten (though some cultures used different numerical bases). Roman and East Asian abacuses use a system resembling bi-quinary coded decimal, with a top deck (containing one or two beads) representing fives and a bottom deck (containing four or five beads) representing ones. Natural numbers are normally used, but some allow simple fractional components (e.g. , , and in Roman abacus), and a decimal point can be imagined for fixed-point arithmetic.
Any particular abacus design supports multiple methods to perform calculations, including addition, subtraction, multiplication, division, and square and cube roots. The beads are first arranged to represent a number, then are manipulated to perform a mathematical operation with another number, and their final position can be read as the result (or can be used as the starting number for subsequent operations).
In the ancient world, abacuses were a practical calculating tool. Although calculators and computers are commonly used today instead of abacuses, abacuses remain in everyday use in some countries. The abacus has an advantage of not requiring a writing implement and paper (needed for algorism) or an electric power source. Merchants, traders, and clerks in some parts of Eastern Europe, Russia, China, and Africa use abacuses. The abacus remains in common use as a scoring system in non-electronic table games. Others may use an abacus due to visual impairment that prevents the use of a calculator. The abacus is still used to teach the fundamentals of mathematics to children in most countries.
Etymology.
The word "abacus" dates to at least 1387 AD when a Middle English work borrowed the word from Latin that described a sandboard abacus. The Latin word is derived from ancient Greek () which means something without a base, and colloquially, any piece of rectangular material. Alternatively, without reference to ancient texts on etymology, it has been suggested that it means "a square tablet strewn with dust", or "drawing-board covered with dust (for the use of mathematics)" (the exact shape of the Latin perhaps reflects the genitive form of the Greek word, ("")). While the table strewn with dust definition is popular, some argue evidence is insufficient for that conclusion. Greek probably borrowed from a Northwest Semitic language like Phoenician, evidenced by a cognate with the Hebrew word "ʾābāq" (), or "dust" (in the post-Biblical sense "sand used as a writing surface").
Both "abacuses" and "abaci" are used as plurals. The user of an abacus is called an "abacist".
History.
Mesopotamia.
The Sumerian abacus appeared between 2700 and 2300 BC. It held a table of successive columns which delimited the successive orders of magnitude of their sexagesimal (base 60) number system.
Some scholars point to a character in Babylonian cuneiform that may have been derived from a representation of the abacus. It is the belief of Old Babylonian scholars, such as Ettore Carruccio, that Old Babylonians "seem to have used the abacus for the operations of addition and subtraction; however, this primitive device proved difficult to use for more complex calculations".
Egypt.
Greek historian Herodotus mentioned the abacus in Ancient Egypt. He wrote that the Egyptians manipulated the pebbles from right to left, opposite in direction to the Greek left-to-right method. Archaeologists have found ancient disks of various sizes that are thought to have been used as counters. However, wall depictions of this instrument are yet to be discovered.
Persia.
At around 600 BC, Persians first began to use the abacus, during the Achaemenid Empire. Under the Parthian, Sassanian, and Iranian empires, scholars concentrated on exchanging knowledge and inventions with the countries around them – India, China, and the Roman Empire – which is how the abacus may have been exported to other countries.
Greece.
The earliest archaeological evidence for the use of the Greek abacus dates to the 5th century BC. Demosthenes (384–322 BC) complained that the need to use pebbles for calculations was too difficult. A play by Alexis from the 4th century BC mentions an abacus and pebbles for accounting, and both Diogenes and Polybius use the abacus as a metaphor for human behavior, stating "that men that sometimes stood for more and sometimes for less" like the pebbles on an abacus. The Greek abacus was a table of wood or marble, pre-set with small counters in wood or metal for mathematical calculations. This Greek abacus was used in Achaemenid Persia, the Etruscan civilization, Ancient Rome, and the Western Christian world until the French Revolution.
A tablet found on the Greek island Salamis in 1846 AD (the Salamis Tablet) dates to 300 BC, making it the oldest counting board discovered so far. It is a slab of white marble in length, wide, and thick, on which are 5 groups of markings. In the tablet's center is a set of 5 parallel lines equally divided by a vertical line, capped with a semicircle at the intersection of the bottom-most horizontal line and the single vertical line. Below these lines is a wide space with a horizontal crack dividing it. Below this crack is another group of eleven parallel lines, again divided into two sections by a line perpendicular to them, but with the semicircle at the top of the intersection; the third, sixth and ninth of these lines are marked with a cross where they intersect with the vertical line. Also from this time frame, the Darius Vase was unearthed in 1851. It was covered with pictures, including a "treasurer" holding a wax tablet in one hand while manipulating counters on a table with the other.
Rome.
The normal method of calculation in ancient Rome, as in Greece, was by moving counters on a smooth table. Originally pebbles () were used. Marked lines indicated units, fives, tens, etc. as in the Roman numeral system.
Writing in the 1st century BC, Horace refers to the wax abacus, a board covered with a thin layer of black wax on which columns and figures were inscribed using a stylus.
One example of archaeological evidence of the Roman abacus, shown nearby in reconstruction, dates to the 1st century AD. It has eight long grooves containing up to five beads in each and eight shorter grooves having either one or no beads in each. The groove marked I indicates units, X tens, and so on up to millions. The beads in the shorter grooves denote fives (five units, five tens, etc.) resembling a bi-quinary coded decimal system related to the Roman numerals. The short grooves on the right may have been used for marking Roman "ounces" (i.e. fractions).
Medieval Europe.
The Roman system of 'counter casting' was used widely in medieval Europe, and persisted in limited use into the nineteenth century. Wealthy abacists used decorative minted counters, called jetons.
Due to Pope Sylvester II's reintroduction of the abacus with modifications, it became widely used in Europe again during the 11th century It used beads on wires, unlike the traditional Roman counting boards, which meant the abacus could be used much faster and was more easily moved.
China.
The earliest known written documentation of the Chinese abacus dates to the 2nd century BC.
The Chinese abacus, also known as the "suanpan" (算盤/算盘, lit. "calculating tray"), comes in various lengths and widths, depending on the operator. It usually has more than seven rods. There are two beads on each rod in the upper deck and five beads each in the bottom one, to represent numbers in a bi-quinary coded decimal-like system. The beads are usually rounded and made of hardwood. The beads are counted by moving them up or down towards the beam; beads moved toward the beam are counted, while those moved away from it are not. One of the top beads is 5, while one of the bottom beads is 1. Each rod has a number under it, showing the place value. The "suanpan" can be reset to the starting position instantly by a quick movement along the horizontal axis to spin all the beads away from the horizontal beam at the center.
The prototype of the Chinese abacus appeared during the Han dynasty, and the beads are oval. The Song dynasty and earlier used the 1:4 type or four-beads abacus similar to the modern abacus including the shape of the beads commonly known as Japanese-style abacus.
In the early Ming dynasty, the abacus began to appear in a 1:5 ratio. The upper deck had one bead and the bottom had five beads. In the late Ming dynasty, the abacus styles appeared in a 2:5 ratio. The upper deck had two beads, and the bottom had five.
Various calculation techniques were devised for "Suanpan" enabling efficient calculations. Some schools teach students how to use it.
In the long scroll "Along the River During the Qingming Festival" painted by Zhang Zeduan during the Song dynasty (960–1297), a "suanpan" is clearly visible beside an account book and doctor's prescriptions on the counter of an apothecary's (Feibao).
The similarity of the Roman abacus to the Chinese one suggests that one could have inspired the other, given evidence of a trade relationship between the Roman Empire and China. However, no direct connection has been demonstrated, and the similarity of the abacuses may be coincidental, both ultimately arising from counting with five fingers per hand. Where the Roman model (like most modern Korean and Japanese) has 4 plus 1 bead per decimal place, the standard "suanpan" has 5 plus 2. Incidentally, this allows use with a hexadecimal numeral system (or any base up to 18) which may have been used for traditional Chinese measures of weight. (Instead of running on wires as in the Chinese, Korean, and Japanese models, the Roman model used grooves, presumably making arithmetic calculations much slower.)
Another possible source of the "suanpan" is Chinese counting rods, which operated with a decimal system but lacked the concept of zero as a placeholder. The zero was probably introduced to the Chinese in the Tang dynasty (618–907) when travel in the Indian Ocean and the Middle East would have provided direct contact with India, allowing them to acquire the concept of zero and the decimal point from Indian merchants and mathematicians.
India.
The "Abhidharmakośabhāṣya" of Vasubandhu (316–396), a Sanskrit work on Buddhist philosophy, says that the second-century CE philosopher Vasumitra said that "placing a wick (Sanskrit "vartikā") on the number one ("ekāṅka") means it is a one while placing the wick on the number hundred means it is called a hundred, and on the number one thousand means it is a thousand". It is unclear exactly what this arrangement may have been. Around the 5th century, Indian clerks were already finding new ways of recording the contents of the abacus. Hindu texts used the term "śūnya" (zero) to indicate the empty column on the abacus.
Japan.
In Japan, the abacus is called "soroban" (, lit. "counting tray"). It was imported from China in the 14th century. It was probably in use by the working class a century or more before the ruling class adopted it, as the class structure obstructed such changes. The 1:4 abacus, which removes the seldom-used second and fifth bead, became popular in the 1940s.
Today's Japanese abacus is a 1:4 type, four-bead abacus, introduced from China in the Muromachi era. It adopts the form of the upper deck one bead and the bottom four beads. The top bead on the upper deck was equal to five and the bottom one is similar to the Chinese or Korean abacus, and the decimal number can be expressed, so the abacus is designed as a one:four device. The beads are always in the shape of a diamond. The quotient division is generally used instead of the division method; at the same time, in order to make the multiplication and division digits consistently use the division multiplication. Later, Japan had a 3:5 abacus called 天三算盤, which is now in the Ize Rongji collection of Shansi Village in Yamagata City. Japan also used a 2:5 type abacus.
The four-bead abacus spread, and became common around the world. Improvements to the Japanese abacus arose in various places. In China an aluminium frame plastic bead abacus was used. The file is next to the four beads, and pressing the "clearing" button put the upper bead in the upper position, and the lower bead in the lower position.
The abacus is still manufactured in Japan even with the proliferation, practicality, and affordability of pocket electronic calculators. The use of the soroban is still taught in Japanese primary schools as part of mathematics, primarily as an aid to faster mental calculation. Using visual imagery can complete a calculation as quickly as a physical instrument.
Korea.
The Chinese abacus migrated from China to Korea around 1400 AD. Koreans call it "jupan" (주판), "supan" (수판) or "jusan" (주산). The four-beads abacus (1:4) was introduced during the Goryeo Dynasty. The 5:1 abacus was introduced to Korea from China during the Ming Dynasty.
Native America.
Some sources mention the use of an abacus called a "nepohualtzintzin" in ancient Aztec culture. This Mesoamerican abacus used a 5-digit base-20 system. The word Nepōhualtzintzin comes from Nahuatl, formed by the roots; "Ne" – personal -; "pōhual" or "pōhualli" – the account -; and "tzintzin" – small similar elements. Its complete meaning was taken as: counting with small similar elements. Its use was taught in the Calmecac to the "temalpouhqueh" , who were students dedicated to taking the accounts of skies, from childhood.
The Nepōhualtzintzin was divided into two main parts separated by a bar or intermediate cord. In the left part were four beads. Beads in the first row have unitary values (1, 2, 3, and 4), and on the right side, three beads had values of 5, 10, and 15, respectively. In order to know the value of the respective beads of the upper rows, it is enough to multiply by 20 (by each row), the value of the corresponding count in the first row.
The device featured 13 rows with 7 beads, 91 in total. This was a basic number for this culture. It had a close relation to natural phenomena, the underworld, and the cycles of the heavens. One Nepōhualtzintzin (91) represented the number of days that a season of the year lasts, two Nepōhualtzitzin (182) is the number of days of the corn's cycle, from its sowing to its harvest, three Nepōhualtzintzin (273) is the number of days of a baby's gestation, and four Nepōhualtzintzin (364) completed a cycle and approximated one year. When translated into modern computer arithmetic, the Nepōhualtzintzin amounted to the rank from 10 to 18 in floating point, which precisely calculated large and small amounts, although round off was not allowed.
The rediscovery of the Nepōhualtzintzin was due to the Mexican engineer David Esparza Hidalgo, who in his travels throughout Mexico found diverse engravings and paintings of this instrument and reconstructed several of them in gold, jade, encrustations of shell, etc. Very old Nepōhualtzintzin are attributed to the Olmec culture, and some bracelets of Mayan origin, as well as a diversity of forms and materials in other cultures.
Sanchez wrote in "Arithmetic in Maya" that another base 5, base 4 abacus had been found in the Yucatán Peninsula that also computed calendar data. This was a finger abacus, on one hand, 0, 1, 2, 3, and 4 were used; and on the other hand 0, 1, 2, and 3 were used. Note the use of zero at the beginning and end of the two cycles.
The quipu of the Incas was a system of colored knotted cords used to record numerical data, like advanced tally sticks – but not used to perform calculations. Calculations were carried out using a yupana (Quechua for "counting tool"; see figure) which was still in use after the conquest of Peru. The working principle of a yupana is unknown, but in 2001 Italian mathematician De Pasquale proposed an explanation. By comparing the form of several yupanas, researchers found that calculations were based using the Fibonacci sequence 1, 1, 2, 3, 5 and powers of 10, 20, and 40 as place values for the different fields in the instrument. Using the Fibonacci sequence would keep the number of grains within any one field at a minimum.
Russia.
The Russian abacus, the "schoty" (, plural from , counting), usually has a single slanted deck, with ten beads on each wire (except one wire with four beads for quarter-ruble fractions). 4-bead wire was introduced for quarter-kopeks, which were minted until 1916. The Russian abacus is used vertically, with each wire running horizontally. The wires are usually bowed upward in the center, to keep the beads pinned to either side. It is cleared when all the beads are moved to the right. During manipulation, beads are moved to the left. For easy viewing, the middle 2 beads on each wire (the 5th and 6th bead) usually are of a different color from the other eight. Likewise, the left bead of the thousands wire (and the million wire, if present) may have a different color.
The Russian abacus was in use in shops and markets throughout the former Soviet Union, and its usage was taught in most schools until the 1990s. Even the 1874 invention of mechanical calculator, Odhner arithmometer, had not replaced them in Russia. According to Yakov Perelman, some businessmen attempting to import calculators into the Russian Empire were known to leave in despair after watching a skilled abacus operator. Likewise, the mass production of Felix arithmometers since 1924 did not significantly reduce abacus use in the Soviet Union. The Russian abacus began to lose popularity only after the mass production of domestic microcalculators in 1974.
The Russian abacus was brought to France around 1820 by mathematician Jean-Victor Poncelet, who had served in Napoleon's army and had been a prisoner of war in Russia. The abacus had fallen out of use in western Europe in the 16th century with the rise of decimal notation and algorismic methods. To Poncelet's French contemporaries, it was something new. Poncelet used it, not for any applied purpose, but as a teaching and demonstration aid. The Turks and the Armenian people used abacuses similar to the Russian schoty. It was named a "coulba" by the Turks and a "choreb" by the Armenians.
School abacus.
Around the world, abacuses have been used in pre-schools and elementary schools as an aid in teaching the numeral system and arithmetic.
In Western countries, a bead frame similar to the Russian abacus but with straight wires and a vertical frame is common (see image).
The wireframe may be used either with positional notation like other abacuses (thus the 10-wire version may represent numbers up to 9,999,999,999), or each bead may represent one unit (e.g. 74 can be represented by shifting all beads on 7 wires and 4 beads on the 8th wire, so numbers up to 100 may be represented). In the bead frame shown, the gap between the 5th and 6th wire, corresponding to the color change between the 5th and the 6th bead on each wire, suggests the latter use. Teaching multiplication, e.g. 6 times 7, may be represented by shifting 7 beads on 6 wires.
The red-and-white abacus is used in contemporary primary schools for a wide range of number-related lessons. The twenty bead version, referred to by its Dutch name "rekenrek" ("calculating frame"), is often used, either on a string of beads or on a rigid framework.
Feynman vs the abacus.
Physicist Richard Feynman was noted for facility in mathematical calculations. He wrote about an encounter in Brazil with a Japanese abacus expert, who challenged him to speed contests between Feynman's pen and paper, and the abacus. The abacus was much faster for addition, somewhat faster for multiplication, but Feynman was faster at division. When the abacus was used for more complex operations, i.e. cube roots, Feynman won easily. However, the number chosen at random was close to a number Feynman happened to know was an exact cube, allowing him to use approximate methods.
Neurological analysis.
Learning how to calculate with the abacus may improve capacity for mental calculation. Abacus-based mental calculation (AMC), which was derived from the abacus, is the act of performing calculations, including addition, subtraction, multiplication, and division, in the mind by manipulating an imagined abacus. It is a high-level cognitive skill that runs calculations with an effective algorithm. People doing long-term AMC training show higher numerical memory capacity and experience more effectively connected neural pathways. They are able to retrieve memory to deal with complex processes. AMC involves both visuospatial and visuomotor processing that generate the visual abacus and move the imaginary beads. Since it only requires that the final position of beads be remembered, it takes less memory and less computation time.
Binary abacus.
The binary abacus is used to explain how computers manipulate numbers. The abacus shows how numbers, letters, and signs can be stored in a binary system on a computer, or via ASCII. The device consists of a series of beads on parallel wires arranged in three separate rows. The beads represent a switch on the computer in either an "on" or "off" position.
Visually impaired users.
An adapted abacus, invented by Tim Cranmer, and called a Cranmer abacus is commonly used by visually impaired users. A piece of soft fabric or rubber is placed behind the beads, keeping them in place while the users manipulate them. The device is then used to perform the mathematical functions of multiplication, division, addition, subtraction, square root, and cube root.
Although blind students have benefited from talking calculators, the abacus is often taught to these students in early grades. Blind students can also complete mathematical assignments using a braille-writer and Nemeth code (a type of braille code for mathematics) but large multiplication and long division problems are tedious. The abacus gives these students a tool to compute mathematical problems that equals the speed and mathematical knowledge required by their sighted peers using pencil and paper. Many blind people find this number machine a useful tool throughout life.

</doc>
<doc id="656" url="?curid=656" title="Acid">
Acid

An acid is a molecule or ion capable of either donating a proton (i.e. hydrogen ion, H+), known as a Brønsted–Lowry acid, or forming a covalent bond with an electron pair, known as a Lewis acid.
The first category of acids are the proton donors, or Brønsted–Lowry acids. In the special case of aqueous solutions, proton donors form the hydronium ion H3O+ and are known as Arrhenius acids. Brønsted and Lowry generalized the Arrhenius theory to include non-aqueous solvents. A Brønsted or Arrhenius acid usually contains a hydrogen atom bonded to a chemical structure that is still energetically favorable after loss of H+.
Aqueous Arrhenius acids have characteristic properties that provide a practical description of an acid. Acids form aqueous solutions with a sour taste, can turn blue litmus red, and react with bases and certain metals (like calcium) to form salts. The word "acid" is derived from the Latin , meaning 'sour'. An aqueous solution of an acid has a pH less than 7 and is colloquially also referred to as "acid" (as in "dissolved in acid"), while the strict definition refers only to the solute. A lower pH means a higher acidity, and thus a higher concentration of positive hydrogen ions in the solution. Chemicals or substances having the property of an acid are said to be acidic.
Common aqueous acids include hydrochloric acid (a solution of hydrogen chloride that is found in gastric acid in the stomach and activates digestive enzymes), acetic acid (vinegar is a dilute aqueous solution of this liquid), sulfuric acid (used in car batteries), and citric acid (found in citrus fruits). As these examples show, acids (in the colloquial sense) can be solutions or pure substances, and can be derived from acids (in the strict sense) that are solids, liquids, or gases. Strong acids and some concentrated weak acids are corrosive, but there are exceptions such as carboranes and boric acid.
The second category of acids are Lewis acids, which form a covalent bond with an electron pair. An example is boron trifluoride (BF3), whose boron atom has a vacant orbital that can form a covalent bond by sharing a lone pair of electrons on an atom in a base, for example the nitrogen atom in ammonia (NH3). Lewis considered this as a generalization of the Brønsted definition, so that an acid is a chemical species that accepts electron pairs either directly "or" by releasing protons (H+) into the solution, which then accept electron pairs. Hydrogen chloride, acetic acid, and most other Brønsted–Lowry acids cannot form a covalent bond with an electron pair, however, and are therefore not Lewis acids. Conversely, many Lewis acids are not Arrhenius or Brønsted–Lowry acids. In modern terminology, an "acid" is implicitly a Brønsted acid and not a Lewis acid, since chemists almost always refer to a Lewis acid explicitly as such.
Definitions and concepts.
Modern definitions are concerned with the fundamental chemical reactions common to all acids.
Most acids encountered in everyday life are aqueous solutions, or can be dissolved in water, so the Arrhenius and Brønsted–Lowry definitions are the most relevant.
The Brønsted–Lowry definition is the most widely used definition; unless otherwise specified, acid–base reactions are assumed to involve the transfer of a proton (H+) from an acid to a base.
Hydronium ions are acids according to all three definitions. Although alcohols and amines can be Brønsted–Lowry acids, they can also function as Lewis bases due to the lone pairs of electrons on their oxygen and nitrogen atoms.
Arrhenius acids.
In 1884, Svante Arrhenius attributed the properties of acidity to hydrogen ions (H+), later described as protons or hydrons. An Arrhenius acid is a substance that, when added to water, increases the concentration of H+ ions in the water. Chemists often write H+("aq") and refer to the hydrogen ion when describing acid–base reactions but the free hydrogen nucleus, a proton, does not exist alone in water, it exists as the hydronium ion (H3O+) or other forms (H5O2+, H9O4+). Thus, an Arrhenius acid can also be described as a substance that increases the concentration of hydronium ions when added to water. Examples include molecular substances such as hydrogen chloride and acetic acid.
An Arrhenius base, on the other hand, is a substance that increases the concentration of hydroxide (OH−) ions when dissolved in water. This decreases the concentration of hydronium because the ions react to form H2O molecules:
Due to this equilibrium, any increase in the concentration of hydronium is accompanied by a decrease in the concentration of hydroxide. Thus, an Arrhenius acid could also be said to be one that decreases hydroxide concentration, while an Arrhenius base increases it.
In an acidic solution, the concentration of hydronium ions is greater than 10−7 moles per liter. Since pH is defined as the negative logarithm of the concentration of hydronium ions, acidic solutions thus have a pH of less than 7.
Brønsted–Lowry acids.
While the Arrhenius concept is useful for describing many reactions, it is also quite limited in its scope. In 1923, chemists Johannes Nicolaus Brønsted and Thomas Martin Lowry independently recognized that acid–base reactions involve the transfer of a proton. A Brønsted–Lowry acid (or simply Brønsted acid) is a species that donates a proton to a Brønsted–Lowry base. Brønsted–Lowry acid–base theory has several advantages over Arrhenius theory. Consider the following reactions of acetic acid (CH3COOH), the organic acid that gives vinegar its characteristic taste:
Both theories easily describe the first reaction: CH3COOH acts as an Arrhenius acid because it acts as a source of H3O+ when dissolved in water, and it acts as a Brønsted acid by donating a proton to water. In the second example CH3COOH undergoes the same transformation, in this case donating a proton to ammonia (NH3), but does not relate to the Arrhenius definition of an acid because the reaction does not produce hydronium. Nevertheless, CH3COOH is both an Arrhenius and a Brønsted–Lowry acid.
Brønsted–Lowry theory can be used to describe reactions of molecular compounds in nonaqueous solution or the gas phase. Hydrogen chloride (HCl) and ammonia combine under several different conditions to form ammonium chloride, NH4Cl. In aqueous solution HCl behaves as hydrochloric acid and exists as hydronium and chloride ions. The following reactions illustrate the limitations of Arrhenius's definition:
As with the acetic acid reactions, both definitions work for the first example, where water is the solvent and hydronium ion is formed by the HCl solute. The next two reactions do not involve the formation of ions but are still proton-transfer reactions. In the second reaction hydrogen chloride and ammonia (dissolved in benzene) react to form solid ammonium chloride in a benzene solvent and in the third gaseous HCl and NH3 combine to form the solid.
Lewis acids.
A third, only marginally related concept was proposed in 1923 by Gilbert N. Lewis, which includes reactions with acid–base characteristics that do not involve a proton transfer. A Lewis acid is a species that accepts a pair of electrons from another species; in other words, it is an electron pair acceptor. Brønsted acid–base reactions are proton transfer reactions while Lewis acid–base reactions are electron pair transfers. Many Lewis acids are not Brønsted–Lowry acids. Contrast how the following reactions are described in terms of acid–base chemistry:
In the first reaction a fluoride ion, F−, gives up an electron pair to boron trifluoride to form the product tetrafluoroborate. Fluoride "loses" a pair of valence electrons because the electrons shared in the B—F bond are located in the region of space between the two atomic nuclei and are therefore more distant from the fluoride nucleus than they are in the lone fluoride ion. BF3 is a Lewis acid because it accepts the electron pair from fluoride. This reaction cannot be described in terms of Brønsted theory because there is no proton transfer.
The second reaction can be described using either theory. A proton is transferred from an unspecified Brønsted acid to ammonia, a Brønsted base; alternatively, ammonia acts as a Lewis base and transfers a lone pair of electrons to form a bond with a hydrogen ion. The species that gains the electron pair is the Lewis acid; for example, the oxygen atom in H3O+ gains a pair of electrons when one of the H—O bonds is broken and the electrons shared in the bond become localized on oxygen. 
Depending on the context, a Lewis acid may also be described as an oxidizer or an electrophile. Organic Brønsted acids, such as acetic, citric, or oxalic acid, are not Lewis acids. They dissociate in water to produce a Lewis acid, H+, but at the same time, they also yield an equal amount of a Lewis base (acetate, citrate, or oxalate, respectively, for the acids mentioned). This article deals mostly with Brønsted acids rather than Lewis acids.
Dissociation and equilibrium.
Reactions of acids are often generalized in the form , where HA represents the acid and A− is the conjugate base. This reaction is referred to as protolysis. The protonated form (HA) of an acid is also sometimes referred to as the free acid.
Acid–base conjugate pairs differ by one proton, and can be interconverted by the addition or removal of a proton (protonation and deprotonation, respectively). The acid can be the charged species and the conjugate base can be neutral in which case the generalized reaction scheme could be written as . In solution there exists an equilibrium between the acid and its conjugate base. The equilibrium constant "K" is an expression of the equilibrium concentrations of the molecules or the ions in solution. Brackets indicate concentration, such that [H2O] means "the concentration of H2O". The acid dissociation constant "K"a is generally used in the context of acid–base reactions. The numerical value of "K"a is equal to the product (multiplication) of the concentrations of the products divided by the concentration of the reactants, where the reactant is the acid (HA) and the products are the conjugate base and H+.
The stronger of two acids will have a higher "K"a than the weaker acid; the ratio of hydrogen ions to acid will be higher for the stronger acid as the stronger acid has a greater tendency to lose its proton. Because the range of possible values for "K"a spans many orders of magnitude, a more manageable constant, p"K"a is more frequently used, where p"K"a = −log10 "K"a. Stronger acids have a smaller p"K"a than weaker acids. Experimentally determined p"K"a at 25 °C in aqueous solution are often quoted in textbooks and reference material.
Nomenclature.
Arrhenius acids are named according to their anions. In the classical naming system, the ionic suffix is dropped and replaced with a new suffix, according to the table following. The prefix "hydro-" is used when the acid is made up of just hydrogen and one other element. For example, HCl has chloride as its anion, so the hydro- prefix is used, and the -ide suffix makes the name take the form hydrochloric acid.
"Classical naming system:"
In the IUPAC naming system, "aqueous" is simply added to the name of the ionic compound. Thus, for hydrogen chloride, as an acid solution, the IUPAC name is aqueous hydrogen chloride.
Acid strength.
The strength of an acid refers to its ability or tendency to lose a proton. A strong acid is one that completely dissociates in water; in other words, one mole of a strong acid HA dissolves in water yielding one mole of H+ and one mole of the conjugate base, A−, and none of the protonated acid HA. In contrast, a weak acid only partially dissociates and at equilibrium both the acid and the conjugate base are in solution. Examples of strong acids are hydrochloric acid (HCl), hydroiodic acid (HI), hydrobromic acid (HBr), perchloric acid (HClO4), nitric acid (HNO3) and sulfuric acid (H2SO4). In water each of these essentially ionizes 100%. The stronger an acid is, the more easily it loses a proton, H+. Two key factors that contribute to the ease of deprotonation are the polarity of the H—A bond and the size of atom A, which determines the strength of the H—A bond. Acid strengths are also often discussed in terms of the stability of the conjugate base.
Stronger acids have a larger acid dissociation constant, "K"a and a lower p"K"a than weaker acids.
Sulfonic acids, which are organic oxyacids, are a class of strong acids. A common example is toluenesulfonic acid (tosylic acid). Unlike sulfuric acid itself, sulfonic acids can be solids. In fact, polystyrene functionalized into polystyrene sulfonate is a solid strongly acidic plastic that is filterable.
Superacids are acids stronger than 100% sulfuric acid. Examples of superacids are fluoroantimonic acid, magic acid and perchloric acid. The strongest known acid is helium hydride ion, with a proton affinity of 177.8kJ/mol. Superacids can permanently protonate water to give ionic, crystalline hydronium "salts". They can also quantitatively stabilize carbocations.
While "K"a measures the strength of an acid compound, the strength of an aqueous acid solution is measured by pH, which is an indication of the concentration of hydronium in the solution. The pH of a simple solution of an acid compound in water is determined by the dilution of the compound and the compound's "K"a.
Lewis acid strength in non-aqueous solutions.
Lewis acids have been classified in the ECW model and it has been shown that there is no one order of acid strengths. The relative acceptor strength of Lewis acids toward a series of bases, versus other Lewis acids, can be illustrated by C-B plots. It has been shown that to define the order of Lewis acid strength at least two properties must be considered. For Pearson's qualitative HSAB theory the two properties are hardness and strength while for Drago's quantitative ECW model the two properties are electrostatic and covalent.
Chemical characteristics.
Monoprotic acids.
Monoprotic acids, also known as monobasic acids, are those acids that are able to donate one proton per molecule during the process of dissociation (sometimes called ionization) as shown below (symbolized by HA):
Common examples of monoprotic acids in mineral acids include hydrochloric acid (HCl) and nitric acid (HNO3). On the other hand, for organic acids the term mainly indicates the presence of one carboxylic acid group and sometimes these acids are known as monocarboxylic acid. Examples in organic acids include formic acid (HCOOH), acetic acid (CH3COOH) and benzoic acid (C6H5COOH).
Polyprotic acids.
Polyprotic acids, also known as polybasic acids, are able to donate more than one proton per acid molecule, in contrast to monoprotic acids that only donate one proton per molecule. Specific types of polyprotic acids have more specific names, such as diprotic (or dibasic) acid (two potential protons to donate), and triprotic (or tribasic) acid (three potential protons to donate). Some macromolecules such as proteins and nucleic acids can have a very large number of acidic protons.
A diprotic acid (here symbolized by H2A) can undergo one or two dissociations depending on the pH. Each dissociation has its own dissociation constant, Ka1 and Ka2.
The first dissociation constant is typically greater than the second (i.e., "K"a1 &gt; "K"a2). For example, sulfuric acid (H2SO4) can donate one proton to form the bisulfate anion (HSO), for which "K"a1 is very large; then it can donate a second proton to form the sulfate anion (SO), wherein the "K"a2 is intermediate strength. The large "K"a1 for the first dissociation makes sulfuric a strong acid. In a similar manner, the weak unstable carbonic acid can lose one proton to form bicarbonate anion and lose a second to form carbonate anion (CO). Both "K"a values are small, but "K"a1 &gt; "K"a2 .
A triprotic acid (H3A) can undergo one, two, or three dissociations and has three dissociation constants, where "K"a1 &gt; "K"a2 &gt; "K"a3.
An inorganic example of a triprotic acid is orthophosphoric acid (H3PO4), usually just called phosphoric acid. All three protons can be successively lost to yield H2PO, then HPO, and finally PO, the orthophosphate ion, usually just called phosphate. Even though the positions of the three protons on the original phosphoric acid molecule are equivalent, the successive "K"a values differ since it is energetically less favorable to lose a proton if the conjugate base is more negatively charged. An organic example of a triprotic acid is citric acid, which can successively lose three protons to finally form the citrate ion.
Although the subsequent loss of each hydrogen ion is less favorable, all of the conjugate bases are present in solution. The fractional concentration, "α" (alpha), for each species can be calculated. For example, a generic diprotic acid will generate 3 species in solution: H2A, HA−, and A2−. The fractional concentrations can be calculated as below when given either the pH (which can be converted to the [H+]) or the concentrations of the acid with all its conjugate bases:
A plot of these fractional concentrations against pH, for given "K"1 and "K"2, is known as a Bjerrum plot. A pattern is observed in the above equations and can be expanded to the general "n" -protic acid that has been deprotonated "i" -times:
where "K"0 = 1 and the other K-terms are the dissociation constants for the acid.
Neutralization.
Neutralization is the reaction between an acid and a base, producing a salt and neutralized base; for example, hydrochloric acid and sodium hydroxide form sodium chloride and water:
Neutralization is the basis of titration, where a pH indicator shows equivalence point when the equivalent number of moles of a base have been added to an acid. It is often wrongly assumed that neutralization should result in a solution with pH 7.0, which is only the case with similar acid and base strengths during a reaction.
Neutralization with a base weaker than the acid results in a weakly acidic salt. An example is the weakly acidic ammonium chloride, which is produced from the strong acid hydrogen chloride and the weak base ammonia. Conversely, neutralizing a weak acid with a strong base gives a weakly basic salt (e.g., sodium fluoride from hydrogen fluoride and sodium hydroxide).
Weak acid–weak base equilibrium.
In order for a protonated acid to lose a proton, the pH of the system must rise above the p"K"a of the acid. The decreased concentration of H+ in that basic solution shifts the equilibrium towards the conjugate base form (the deprotonated form of the acid). In lower-pH (more acidic) solutions, there is a high enough H+ concentration in the solution to cause the acid to remain in its protonated form.
Solutions of weak acids and salts of their conjugate bases form buffer solutions.
Titration.
To determine the concentration of an acid in an aqueous solution, an acid–base titration is commonly performed. A strong base solution with a known concentration, usually NaOH or KOH, is added to neutralize the acid solution according to the color change of the indicator with the amount of base added. The titration curve of an acid titrated by a base has two axes, with the base volume on the x-axis and the solution's pH value on the y-axis. The pH of the solution always goes up as the base is added to the solution.
Example: Diprotic acid.
For each diprotic acid titration curve, from left to right, there are two midpoints, two equivalence points, and two buffer regions.
Equivalence points.
Due to the successive dissociation processes, there are two equivalence points in the titration curve of a diprotic acid. The first equivalence point occurs when all first hydrogen ions from the first ionization are titrated. In other words, the amount of OH− added equals the original amount of H2A at the first equivalence point. The second equivalence point occurs when all hydrogen ions are titrated. Therefore, the amount of OH− added equals twice the amount of H2A at this time. For a weak diprotic acid titrated by a strong base, the second equivalence point must occur at pH above 7 due to the hydrolysis of the resulted salts in the solution. At either equivalence point, adding a drop of base will cause the steepest rise of the pH value in the system.
Buffer regions and midpoints.
A titration curve for a diprotic acid contains two midpoints where pH=pKa. Since there are two different Ka values, the first midpoint occurs at pH=pKa1 and the second one occurs at pH=pKa2. Each segment of the curve that contains a midpoint at its center is called the buffer region. Because the buffer regions consist of the acid and its conjugate base, it can resist pH changes when base is added until the next equivalent points.
Applications of acids.
In industry.
Acids are fundamental reagents in treating almost all processes in modern industry. Sulfuric acid, a diprotic acid, is the most widely used acid in industry, and is also the most-produced industrial chemical in the world. It is mainly used in producing fertilizer, detergent, batteries and dyes, as well as used in processing many products such like removing impurities. According to the statistics data in 2011, the annual production of sulfuric acid was around 200 million tonnes in the world. For example, phosphate minerals react with sulfuric acid to produce phosphoric acid for the production of phosphate fertilizers, and zinc is produced by dissolving zinc oxide into sulfuric acid, purifying the solution and electrowinning.
In the chemical industry, acids react in neutralization reactions to produce salts. For example, nitric acid reacts with ammonia to produce ammonium nitrate, a fertilizer. Additionally, carboxylic acids can be esterified with alcohols, to produce esters.
Acids are often used to remove rust and other corrosion from metals in a process known as pickling. They may be used as an electrolyte in a wet cell battery, such as sulfuric acid in a car battery.
In food.
Tartaric acid is an important component of some commonly used foods like unripened mangoes and tamarind. Natural fruits and vegetables also contain acids. Citric acid is present in oranges, lemon and other citrus fruits. Oxalic acid is present in tomatoes, spinach, and especially in carambola and rhubarb; rhubarb leaves and unripe carambolas are toxic because of high concentrations of oxalic acid. Ascorbic acid (Vitamin C) is an essential vitamin for the human body and is present in such foods as amla (Indian gooseberry), lemon, citrus fruits, and guava.
Many acids can be found in various kinds of food as additives, as they alter their taste and serve as preservatives. Phosphoric acid, for example, is a component of cola drinks. Acetic acid is used in day-to-day life as vinegar. Citric acid is used as a preservative in sauces and pickles.
Carbonic acid is one of the most common acid additives that are widely added in soft drinks. During the manufacturing process, CO2 is usually pressurized to dissolve in these drinks to generate carbonic acid. Carbonic acid is very unstable and tends to decompose into water and CO2 at room temperature and pressure. Therefore, when bottles or cans of these kinds of soft drinks are opened, the soft drinks fizz and effervesce as CO2 bubbles come out.
Certain acids are used as drugs. Acetylsalicylic acid (Aspirin) is used as a pain killer and for bringing down fevers.
In human bodies.
Acids play important roles in the human body. The hydrochloric acid present in the stomach aids digestion by breaking down large and complex food molecules. Amino acids are required for synthesis of proteins required for growth and repair of body tissues. Fatty acids are also required for growth and repair of body tissues. Nucleic acids are important for the manufacturing of DNA and RNA and transmitting of traits to offspring through genes. Carbonic acid is important for maintenance of pH equilibrium in the body.
Human bodies contain a variety of organic and inorganic compounds, among those dicarboxylic acids play an essential role in many biological behaviors. Many of those acids are amino acids, which mainly serve as materials for the synthesis of proteins. Other weak acids serve as buffers with their conjugate bases to keep the body's pH from undergoing large scale changes that would be harmful to cells. The rest of the dicarboxylic acids also participate in the synthesis of various biologically important compounds in human bodies.
Acid catalysis.
Acids are used as catalysts in industrial and organic chemistry; for example, sulfuric acid is used in very large quantities in the alkylation process to produce gasoline. Some acids, such as sulfuric, phosphoric, and hydrochloric acids, also effect dehydration and condensation reactions. In biochemistry, many enzymes employ acid catalysis.
Biological occurrence.
Many biologically important molecules are acids. Nucleic acids, which contain acidic phosphate groups, include DNA and RNA. Nucleic acids contain the genetic code that determines many of an organism's characteristics, and is passed from parents to offspring. DNA contains the chemical blueprint for the synthesis of proteins, which are made up of amino acid subunits. Cell membranes contain fatty acid esters such as phospholipids.
An α-amino acid has a central carbon (the α or "alpha" carbon) that is covalently bonded to a carboxyl group (thus they are carboxylic acids), an amino group, a hydrogen atom and a variable group. The variable group, also called the R group or side chain, determines the identity and many of the properties of a specific amino acid. In glycine, the simplest amino acid, the R group is a hydrogen atom, but in all other amino acids it is contains one or more carbon atoms bonded to hydrogens, and may contain other elements such as sulfur, oxygen or nitrogen. With the exception of glycine, naturally occurring amino acids are chiral and almost invariably occur in the . Peptidoglycan, found in some bacterial cell walls contains some -amino acids. At physiological pH, typically around 7, free amino acids exist in a charged form, where the acidic carboxyl group (-COOH) loses a proton (-COO−) and the basic amine group (-NH2) gains a proton (-NH). The entire molecule has a net neutral charge and is a zwitterion, with the exception of amino acids with basic or acidic side chains. Aspartic acid, for example, possesses one protonated amine and two deprotonated carboxyl groups, for a net charge of −1 at physiological pH.
Fatty acids and fatty acid derivatives are another group of carboxylic acids that play a significant role in biology. These contain long hydrocarbon chains and a carboxylic acid group on one end. The cell membrane of nearly all organisms is primarily made up of a phospholipid bilayer, a micelle of hydrophobic fatty acid esters with polar, hydrophilic phosphate "head" groups. Membranes contain additional components, some of which can participate in acid–base reactions.
In humans and many other animals, hydrochloric acid is a part of the gastric acid secreted within the stomach to help hydrolyze proteins and polysaccharides, as well as converting the inactive pro-enzyme, pepsinogen into the enzyme, pepsin. Some organisms produce acids for defense; for example, ants produce formic acid.
Acid–base equilibrium plays a critical role in regulating mammalian breathing. Oxygen gas (O2) drives cellular respiration, the process by which animals release the chemical potential energy stored in food, producing carbon dioxide (CO2) as a byproduct. Oxygen and carbon dioxide are exchanged in the lungs, and the body responds to changing energy demands by adjusting the rate of ventilation. For example, during periods of exertion the body rapidly breaks down stored carbohydrates and fat, releasing CO2 into the blood stream. In aqueous solutions such as blood CO2 exists in equilibrium with carbonic acid and bicarbonate ion.
It is the decrease in pH that signals the brain to breathe faster and deeper, expelling the excess CO2 and resupplying the cells with O2.
 Cell membranes are generally impermeable to charged or large, polar molecules because of the lipophilic fatty acyl chains comprising their interior. Many biologically important molecules, including a number of pharmaceutical agents, are organic weak acids that can cross the membrane in their protonated, uncharged form but not in their charged form (i.e., as the conjugate base). For this reason the activity of many drugs can be enhanced or inhibited by the use of antacids or acidic foods. The charged form, however, is often more soluble in blood and cytosol, both aqueous environments. When the extracellular environment is more acidic than the neutral pH within the cell, certain acids will exist in their neutral form and will be membrane soluble, allowing them to cross the phospholipid bilayer. Acids that lose a proton at the intracellular pH will exist in their soluble, charged form and are thus able to diffuse through the cytosol to their target. Ibuprofen, aspirin and penicillin are examples of drugs that are weak acids.
Common acids.
Sulfonic acids.
A sulfonic acid has the general formula RS(=O)2–OH, where R is an organic radical.
Carboxylic acids.
A carboxylic acid has the general formula R-C(O)OH, where R is an organic radical. The carboxyl group -C(O)OH contains a carbonyl group, C=O, and a hydroxyl group, O-H.
Halogenated carboxylic acids.
Halogenation at alpha position increases acid strength, so that the following acids are all stronger than acetic acid.
Vinylogous carboxylic acids.
Normal carboxylic acids are the direct union of a carbonyl group and a hydroxyl group. In vinylogous carboxylic acids, a carbon-carbon double bond separates the carbonyl and hydroxyl groups.

</doc>
<doc id="657" url="?curid=657" title="Bitumen">
Bitumen

Bitumen ( , ) is an immensely viscous constituent of petroleum. Depending on its exact composition it can be a sticky, black liquid or an apparently solid mass that behaves as a liquid over very large time scales. In American English, the material is commonly referred to as asphalt. Whether found in natural deposits or refined from petroleum, the substance is classed as a pitch. Prior to the 20th century, the term asphaltum was in general use. The word derives from the Ancient Greek word (), which referred to natural bitumen or pitch. The largest natural deposit of bitumen in the world is the Pitch Lake of southwest Trinidad, which is estimated to contain 10 million tons.
About 70% of annual bitumen production is destined for road construction, its primary use. In this application, bitumen is used to bind aggregate particles like gravel and forms a substance referred to as asphalt concrete, which is colloquially termed "asphalt". Its other main uses lie in bituminous waterproofing products, such as roofing felt and roof sealant.
In material sciences and engineering, the terms "asphalt" and "bitumen" are often used interchangeably and refer both to natural and manufactured forms of the substance, although there is regional variation as to which term is most common. Worldwide, geologists tend to favor the term "bitumen" for the naturally occurring material. For the manufactured material, which is a refined residue from the distillation process of selected crude oils, "bitumen" is the prevalent term in much of the world; however, in American English, "asphalt" is more commonly used. To help avoid confusion, the terms "liquid asphalt", "asphalt binder", or "asphalt cement" are used in the U.S. to distinguish it from asphalt concrete. Colloquially, various forms of bitumen are sometimes referred to as "tar", as in the name of the La Brea Tar Pits.
Naturally occurring bitumen is sometimes specified by the term "crude bitumen". Its viscosity is similar to that of cold molasses while the material obtained from the fractional distillation of crude oil boiling at is sometimes referred to as "refined bitumen". The Canadian province of Alberta has most of the world's reserves of natural bitumen in the Athabasca oil sands, which cover , an area larger than England.
Terminology.
Etymology.
The Latin word traces to the Proto-Indo-European root "*gʷet-" "pitch".
The expression "bitumen" originated in the Sanskrit, where we find the words "jatu", meaning "pitch", and "jatu-krit", meaning "pitch creating", "pitch producing" (referring to coniferous or resinous trees). The Latin equivalent is claimed by some to be originally "gwitu-men" (pertaining to pitch), and by others, "pixtumens" (exuding or bubbling pitch), which was subsequently shortened to "bitumen", thence passing via French into English. From the same root is derived the Anglo Saxon word "cwidu" (Mastix), the German word "Kitt" (cement or mastic) and the old Norse word "kvada".
The word "ašphalt" is claimed to have been derived from the Accadian term "asphaltu" or "sphallo", meaning "to split". It was later adopted by the Homeric Greeks in the form of the adjective ἄσφαλἤς, ἐς signifying "firm", "stable", "secure", and the corresponding verb ἄσφαλίξω, ίσω meaning "to make firm or stable", "to secure".
The word "asphalt" is derived from the late Middle English, in turn from French "asphalte", based on Late Latin "asphalton", "asphaltum", which is the latinisation of the Greek ("ásphaltos", "ásphalton"), a word meaning "asphalt/bitumen/pitch", which perhaps derives from , "not, without", i.e. the alpha privative, and ("sphallein"), "to cause to fall, baffle, (in passive) err, (in passive) be balked of".
The first use of asphalt by the ancients was as a cement to secure or join various objects, and it thus seems likely that the name itself was expressive of this application. Specifically, Herodotus mentioned that bitumen was brought to Babylon to build its gigantic fortification wall.
From the Greek, the word passed into late Latin, and thence into French ("asphalte") and English ("asphaltum" and "asphalt"). In French, the term "asphalte" is used for naturally occurring asphalt-soaked limestone deposits, and for specialised manufactured products with fewer voids or greater bitumen content than the "asphaltic concrete" used to pave roads.
Modern terminology.
Bitumen mixed with clay was usually called "asphaltum", but the term is less commonly used today.
In American English, "asphalt" is equivalent to the British "bitumen". However, "asphalt" is also commonly used as a shortened form of "asphalt concrete" (therefore equivalent to the British "asphalt" or "tarmac").
In Canadian English, the word "bitumen" is used to refer to the vast Canadian deposits of extremely heavy crude oil, while "asphalt" is used for the oil refinery product. Diluted bitumen (diluted with naphtha to make it flow in pipelines) is known as "dilbit" in the Canadian petroleum industry, while bitumen "upgraded" to synthetic crude oil is known as "syncrude", and syncrude blended with bitumen is called "synbit".
"Bitumen" is still the preferred geological term for naturally occurring deposits of the solid or semi-solid form of petroleum. "Bituminous rock" is a form of sandstone impregnated with bitumen. The oil sands of Alberta, Canada are a similar material.
Neither of the terms "asphalt" or "bitumen" should be confused with tar or coal tars. Tar is the thick liquid product of the dry distillation and pyrolysis of organic hydrocarbons primarily sourced from vegetation masses, whether fossilized as with coal, or freshly harvested. The majority of bitumen, on the other hand, was formed naturally when vast quantities of organic animal materials were deposited by water and buried hundreds of metres deep at the diagenetic point, where the disorganized fatty hydrocarbon molecules joined in long chains in the absence of oxygen. Bitumen occurs as a solid or highly viscous liquid. It may even be mixed in with coal deposits. Bitumen, and coal using the Bergius process, can be refined into petrols such as gasoline, and bitumen may be distilled into tar, not the other way around.
Composition.
Normal composition.
The components of bitumen include four main classes of compounds:
Bitumen typically contains, elementally 80% by weight of carbon; 10% hydrogen; up to 6% sulfur; and molecularly, between 5 and 25% by weight of asphaltenes dispersed in 90% to 65% maltenes. Most natural bitumens also contain organosulfur compounds, Nickel and vanadium are found at &lt;10 parts per million, as is typical of some petroleum. The substance is soluble in carbon disulfide. It is commonly modelled as a colloid, with asphaltenes as the dispersed phase and maltenes as the continuous phase. "It is almost impossible to separate and identify all the different molecules of bitumen, because the number of molecules with different chemical structure is extremely large".
Asphalt may be confused with coal tar, which is a visually similar black, thermoplastic material produced by the destructive distillation of coal. During the early and mid-20th century, when town gas was produced, coal tar was a readily available byproduct and extensively used as the binder for road aggregates. The addition of coal tar to macadam roads led to the word "tarmac", which is now used in common parlance to refer to road-making materials. However, since the 1970s, when natural gas succeeded town gas, bitumen has completely overtaken the use of coal tar in these applications. Other examples of this confusion include La Brea Tar Pits and the Canadian tar sands, both of which actually contain natural bitumen rather than tar. "Pitch" is another term sometimes informally used at times to refer to asphalt, as in Pitch Lake.
Additives, mixtures and contaminants.
For economic and other reasons, bitumen is sometimes sold combined with other materials, often without being labeled as anything other than simply "bitumen".
Of particular note is the use of re-refined engine oil bottoms – "REOB" or "REOBs"the residue of recycled automotive engine oil collected from the bottoms of re-refining vacuum distillation towers, in the manufacture of asphalt. REOB contains various elements and compounds found in recycled engine oil: additives to the original oil and materials accumulating from its circulation in the engine (typically iron and copper). Some research has indicated a correlation between this adulteration of bitumen and poorer-performing pavement.
Occurrence.
The majority of bitumen used commercially is obtained from petroleum. Nonetheless, large amounts of bitumen occur in concentrated form in nature. Naturally occurring deposits of bitumen are formed from the remains of ancient, microscopic algae (diatoms) and other once-living things. These natural deposits of bitumen have been formed during the Carboniferous period, when giant swamp forests dominated many parts of the Earth. They were deposited in the mud on the bottom of the ocean or lake where the organisms lived. Under the heat (above 50 °C) and pressure of burial deep in the earth, the remains were transformed into materials such as bitumen, kerogen, or petroleum.
Natural deposits of bitumen include lakes such as the Pitch Lake in Trinidad and Tobago and Lake Bermudez in Venezuela. Natural seeps occur in the La Brea Tar Pits and the McKittrick Tar Pits in California, as well as in the Dead Sea.
Bitumen also occurs in unconsolidated sandstones known as "oil sands" in Alberta, Canada, and the similar "tar sands" in Utah, US.
The Canadian province of Alberta has most of the world's reserves, in three huge deposits covering , an area larger than England or New York state. These bituminous sands contain of commercially established oil reserves, giving Canada the third largest oil reserves in the world. Although historically it was used without refining to pave roads, nearly all of the output is now used as raw material for oil refineries in Canada and the United States.
The world's largest deposit of natural bitumen, known as the Athabasca oil sands, is located in the McMurray Formation of Northern Alberta. This formation is from the early Cretaceous, and is composed of numerous lenses of oil-bearing sand with up to 20% oil. Isotopic studies show the oil deposits to be about 110 million years old. Two smaller but still very large formations occur in the Peace River oil sands and the Cold Lake oil sands, to the west and southeast of the Athabasca oil sands, respectively. Of the Alberta deposits, only parts of the Athabasca oil sands are shallow enough to be suitable for surface mining. The other 80% has to be produced by oil wells using enhanced oil recovery techniques like steam-assisted gravity drainage.
Much smaller heavy oil or bitumen deposits also occur in the Uinta Basin in Utah, US. The Tar Sand Triangle deposit, for example, is roughly 6% bitumen.
Bitumen may occur in hydrothermal veins. An example of this is within the Uinta Basin of Utah, in the US, where there is a swarm of laterally and vertically extensive veins composed of a solid hydrocarbon termed Gilsonite. These veins formed by the polymerization and solidification of hydrocarbons that were mobilized from the deeper oil shales of the Green River Formation during burial and diagenesis.
Bitumen is similar to the organic matter in carbonaceous meteorites. However, detailed studies have shown these materials to be distinct. The vast Alberta bitumen resources are considered to have started out as living material from marine plants and animals, mainly algae, that died millions of years ago when an ancient ocean covered Alberta. They were covered by mud, buried deeply over time, and gently cooked into oil by geothermal heat at a temperature of . Due to pressure from the rising of the Rocky Mountains in southwestern Alberta, 80 to 55 million years ago, the oil was driven northeast hundreds of kilometres and trapped into underground sand deposits left behind by ancient river beds and ocean beaches, thus forming the oil sands.
History.
Paleolithic times.
The earliest estimated use of bitumen dates back 40,000 years to the paleolithic age in which Bitumen was used to adhere handles onto primitive stone tools.
A re-examination of artifacts uncovered in 1908 at Le Moustier rock shelters in France has identified Mousterian stone tools that were attached to grips made of ochre and bitumen. The grips were formulated with 55% ground goethite ochre and 45% cooked liquid bitumen to create a moldable putty that hardened into handles. Earlier excavations at Le Moustier prevent conclusive identification of the archaeological culture and age, but the European Mousterian style of these tools suggests they are associated with Neanderthals during the late Middle Paleolithic, between 60,000 and 35,000 years before present. It is the earliest evidence of compound adhesive use in Europe.
Ancient times.
The use of natural bitumen for waterproofing, and as an adhesive dates at least to the fifth millennium BC, with a crop storage basket discovered in Mehrgarh, of the Indus Valley civilization, lined with it. By the 3rd millennium BC refined rock asphalt was in use in the region, and was used to waterproof the Great Bath in Mohenjo-daro.
In the ancient Near East, the Sumerians used natural bitumen deposits for mortar between bricks and stones, to cement parts of carvings, such as eyes, into place, for ship caulking, and for waterproofing. The Greek historian Herodotus said hot bitumen was used as mortar in the walls of Babylon.
The long Euphrates Tunnel beneath the river Euphrates at Babylon in the time of Queen Semiramis () was reportedly constructed of burnt bricks covered with bitumen as a waterproofing agent.
Bitumen was used by ancient Egyptians to embalm mummies. The Persian word for asphalt is "moom", which is related to the English word mummy. The Egyptians' primary source of bitumen was the Dead Sea, which the Romans knew as "Palus Asphaltites" (Asphalt Lake).
In approximately 40 AD, Dioscorides described the Dead Sea material as "Judaicum bitumen", and noted other places in the region where it could be found. The Sidon bitumen is thought to refer to material found at Hasbeya in Lebanon. Pliny also refers to bitumen being found in Epirus. Bitumen was a valuable strategic resource. It was the object of the first known battle for a hydrocarbon deposit – between the Seleucids and the Nabateans in 312 BC.
In the ancient Far East, natural bitumen was slowly boiled to get rid of the higher fractions, leaving a thermoplastic material of higher molecular weight that, when layered on objects, became hard upon cooling. This was used to cover objects that needed waterproofing, such as scabbards and other items. Statuettes of household deities were also cast with this type of material in Japan, and probably also in China.
In North America, archaeological recovery has indicated that bitumen was sometimes used to adhere stone projectile points to wooden shafts. In Canada, aboriginal people used bitumen seeping out of the banks of the Athabasca and other rivers to waterproof birch bark canoes, and also heated it in smudge pots to ward off mosquitoes in the summer. Bitumen was also used to waterproof plank canoes used by indigenous peoples in pre-colonial southern California. 
Continental Europe.
In 1553, Pierre Belon described in his work "Observations" that "pissasphalto", a mixture of pitch and bitumen, was used in the Republic of Ragusa (now Dubrovnik, Croatia) for tarring of ships.
An 1838 edition of "Mechanics Magazine" cites an early use of asphalt in France. A pamphlet dated 1621, by "a certain Monsieur d'Eyrinys, states that he had discovered the existence (of asphaltum) in large quantities in the vicinity of Neufchatel", and that he proposed to use it in a variety of ways – "principally in the construction of air-proof granaries, and in protecting, by means of the arches, the water-courses in the city of Paris from the intrusion of dirt and filth", which at that time made the water unusable. "He expatiates also on the excellence of this material for forming level and durable terraces" in palaces, "the notion of forming such terraces in the streets not one likely to cross the brain of a Parisian of that generation".
But the substance was generally neglected in France until the revolution of 1830. In the 1830s there was a surge of interest, and asphalt became widely used "for pavements, flat roofs, and the lining of cisterns, and in England, some use of it had been made of it for similar purposes". Its rise in Europe was "a sudden phenomenon", after natural deposits were found "in France at Osbann (Bas-Rhin), the Parc (Ain) and the Puy-de-la-Poix (Puy-de-Dôme)", although it could also be made artificially. One of the earliest uses in France was the laying of about 24,000 square yards of Seyssel asphalt at the Place de la Concorde in 1835.
United Kingdom.
Among the earlier uses of bitumen in the United Kingdom was for etching. William Salmon's "Polygraphice" (1673) provides a recipe for varnish used in etching, consisting of three ounces of virgin wax, two ounces of mastic, and one ounce of asphaltum. By the fifth edition in 1685, he had included more asphaltum recipes from other sources.
The first British patent for the use of asphalt was "Cassell's patent asphalte or bitumen" in 1834. Then on 25 November 1837, Richard Tappin Claridge patented the use of Seyssel asphalt (patent #7849), for use in asphalte pavement, having seen it employed in France and Belgium when visiting with Frederick Walter Simms, who worked with him on the introduction of asphalt to Britain. Dr T. Lamb Phipson writes that his father, Samuel Ryland Phipson, a friend of Claridge, was also "instrumental in introducing the asphalte pavement (in 1836)".
Claridge obtained a patent in Scotland on 27 March 1838, and obtained a patent in Ireland on 23 April 1838. In 1851, extensions for the 1837 patent and for both 1838 patents were sought by the trustees of a company previously formed by Claridge. "Claridge's Patent Asphalte Company"formed in 1838 for the purpose of introducing to Britain "Asphalte in its natural state from the mine at Pyrimont Seysell in France","laid one of the first asphalt pavements in Whitehall". Trials were made of the pavement in 1838 on the footway in Whitehall, the stable at Knightsbridge Barracks, "and subsequently on the space at the bottom of the steps leading from Waterloo Place to St. James Park". "The formation in 1838 of Claridge's Patent Asphalte Company (with a distinguished list of aristocratic patrons, and Marc and Isambard Brunel as, respectively, a trustee and consulting engineer), gave an enormous impetus to the development of a British asphalt industry". "By the end of 1838, at least two other companies, Robinson's and the Bastenne company, were in production", with asphalt being laid as paving at Brighton, Herne Bay, Canterbury, Kensington, the Strand, and a large floor area in Bunhill-row, while meantime Claridge's Whitehall paving "continue(d) in good order". The Bonnington Chemical Works manufactured asphalt using coal tar and by 1839 had installed it in Bonnington.
In 1838, there was a flurry of entrepreneurial activity involving bitumen, which had uses beyond paving. For example, bitumen could also be used for flooring, damp proofing in buildings, and for waterproofing of various types of pools and baths, both of which were also proliferating in the 19th century. One of the earliest surviving examples of its use can be seen at Highgate Cemetery where it was used in 1839 to seal the roof of the terrace catacombs. On the London stockmarket, there were various claims as to the exclusivity of bitumen quality from France, Germany and England. And numerous patents were granted in France, with similar numbers of patent applications being denied in England due to their similarity to each other. In England, "Claridge's was the type most used in the 1840s and 50s".
In 1914, Claridge's Company entered into a joint venture to produce tar-bound macadam, with materials manufactured through a subsidiary company called Clarmac Roads Ltd. Two products resulted, namely "Clarmac", and "Clarphalte", with the former being manufactured by Clarmac Roads and the latter by Claridge's Patent Asphalte Co., although "Clarmac" was more widely used. However, the First World War ruined the Clarmac Company, which entered into liquidation in 1915. The failure of Clarmac Roads Ltd had a flow-on effect to Claridge's Company, which was itself compulsorily wound up, ceasing operations in 1917, having invested a substantial amount of funds into the new venture, both at the outset and in a subsequent attempt to save the Clarmac Company.
Bitumen was thought in 19th century Britain to contain chemicals with medicinal properties. Extracts from bitumen were used to treat catarrh and some forms of asthma and as a remedy against worms, especially the tapeworm.
United States.
The first use of bitumen in the New World was by aboriginal peoples. On the west coast, as early as the 13th century, the Tongva, Luiseño and Chumash peoples collected the naturally occurring bitumen that seeped to the surface above underlying petroleum deposits. All three groups used the substance as an adhesive. It is found on many different artifacts of tools and ceremonial items. For example, it was used on rattles to adhere gourds or turtle shells to rattle handles. It was also used in decorations. Small round shell beads were often set in asphaltum to provide decorations. It was used as a sealant on baskets to make them watertight for carrying water, possibly poisoning those who drank the water. Asphalt was used also to seal the planks on ocean-going canoes.
Asphalt was first used to pave streets in the 1870s. At first naturally occurring "bituminous rock" was used, such as at Ritchie Mines in Macfarlan in Ritchie County, West Virginia from 1852 to 1873. In 1876, asphalt-based paving was used to pave Pennsylvania Avenue in Washington DC, in time for the celebration of the national centennial.
In the horse-drawn era, US streets were mostly unpaved and covered with dirt or gravel. Especially where mud or trenching often made streets difficult to pass, pavements were sometimes made of diverse materials including wooden planks, cobble stones or other stone blocks, or bricks. Unpaved roads produced uneven wear and hazards for pedestrians. In the late 19th century with the rise of the popular bicycle, bicycle clubs were important in pushing for more general pavement of streets. Advocacy for pavement increased in the early 20th century with the rise of the automobile. Asphalt gradually became an ever more common method of paving. St. Charles Avenue in New Orleans was paved its whole length with asphalt by 1889.
In 1900, Manhattan alone had 130,000 horses, pulling streetcars, wagons, and carriages, and leaving their waste behind. They were not fast, and pedestrians could dodge and scramble their way across the crowded streets. Small towns continued to rely on dirt and gravel, but larger cities wanted much better streets. They looked to wood or granite blocks by the 1850s. In 1890, a third of Chicago's 2000 miles of streets were paved, chiefly with wooden blocks, which gave better traction than mud. Brick surfacing was a good compromise, but even better was asphalt paving, which was easy to install and to cut through to get at sewers. With London and Paris serving as models, Washington laid 400,000 square yards of asphalt paving by 1882; it became the model for Buffalo, Philadelphia and elsewhere. By the end of the century, American cities boasted 30 million square yards of asphalt paving, well ahead of brick. The streets became faster and more dangerous so electric traffic lights were installed. Electric trolleys (at 12 miles per hour) became the main transportation service for middle class shoppers and office workers until they bought automobiles after 1945 and commuted from more distant suburbs in privacy and comfort on asphalt highways.
Canada.
Canada has the world's largest deposit of natural bitumen in the Athabasca oil sands, and Canadian First Nations along the Athabasca River had long used it to waterproof their canoes. In 1719, a Cree named Wa-Pa-Su brought a sample for trade to Henry Kelsey of the Hudson's Bay Company, who was the first recorded European to see it. However, it wasn't until 1787 that fur trader and explorer Alexander MacKenzie saw the Athabasca oil sands and said, "At about 24 miles from the fork (of the Athabasca and Clearwater Rivers) are some bituminous fountains into which a pole of 20 feet long may be inserted without the least resistance."
The value of the deposit was obvious from the start, but the means of extracting the bitumen was not. The nearest town, Fort McMurray, Alberta, was a small fur trading post, other markets were far away, and transportation costs were too high to ship the raw bituminous sand for paving. In 1915, Sidney Ells of the Federal Mines Branch experimented with separation techniques and used the product to pave 600 feet of road in Edmonton, Alberta. Other roads in Alberta were paved with material extracted from oil sands, but it was generally not economic. During the 1920s Dr. Karl A. Clark of the Alberta Research Council patented a hot water oil separation process and entrepreneur Robert C. Fitzsimmons built the Bitumount oil separation plant, which between 1925 and 1958 produced up to per day of bitumen using Dr. Clark's method. Most of the bitumen was used for waterproofing roofs, but other uses included fuels, lubrication oils, printers ink, medicines, rust- and acid-proof paints, fireproof roofing, street paving, patent leather, and fence post preservatives. Eventually Fitzsimmons ran out of money and the plant was taken over by the Alberta government. Today the Bitumount plant is a Provincial Historic Site.
Photography and art.
Bitumen was used in early photographic technology. In 1826, or 1827, it was used by French scientist Joseph Nicéphore Niépce to make the oldest surviving photograph from nature. The bitumen was thinly coated onto a pewter plate which was then exposed in a camera. Exposure to light hardened the bitumen and made it insoluble, so that when it was subsequently rinsed with a solvent only the sufficiently light-struck areas remained. Many hours of exposure in the camera were required, making bitumen impractical for ordinary photography, but from the 1850s to the 1920s it was in common use as a photoresist in the production of printing plates for various photomechanical printing processes.
Bitumen was the nemesis of many artists during the 19th century. Although widely used for a time, it ultimately proved unstable for use in oil painting, especially when mixed with the most common diluents, such as linseed oil, varnish and turpentine. Unless thoroughly diluted, bitumen never fully solidifies and will in time corrupt the other pigments with which it comes into contact. The use of bitumen as a glaze to set in shadow or mixed with other colors to render a darker tone resulted in the eventual deterioration of many paintings, for instance those of Delacroix. Perhaps the most famous example of the destructiveness of bitumen is Théodore Géricault's Raft of the Medusa (1818–1819), where his use of bitumen caused the brilliant colors to degenerate into dark greens and blacks and the paint and canvas to buckle.
Modern use.
Global use.
The vast majority of refined bitumen is used in construction: primarily as a constituent of products used in paving and roofing applications. According to the requirements of the end use, bitumen is produced to specification. This is achieved either by refining or blending. It is estimated that the current world use of bitumen is approximately 102 million tonnes per year. Approximately 85% of all the bitumen produced is used as the binder in asphalt concrete for roads. It is also used in other paved areas such as airport runways, car parks and footways. Typically, the production of asphalt concrete involves mixing fine and coarse aggregates such as sand, gravel and crushed rock with asphalt, which acts as the binding agent. Other materials, such as recycled polymers (e.g., rubber tyres), may be added to the bitumen to modify its properties according to the application for which the bitumen is ultimately intended.
A further 10% of global bitumen production is used in roofing applications, where its waterproofing qualities are invaluable.
The remaining 5% of bitumen is used mainly for sealing and insulating purposes in a variety of building materials, such as pipe coatings, carpet tile backing and paint. Bitumen is applied in the construction and maintenance of many structures, systems, and components, such as:
Rolled asphalt concrete.
The largest use of bitumen is for making asphalt concrete for road surfaces; this accounts for approximately 85% of the bitumen consumed in the United States. There are about 4,000 asphalt concrete mixing plants in the US, and a similar number in Europe.
Asphalt concrete pavement mixes are typically composed of 5% bitumen (known as asphalt cement in the US) and 95% aggregates (stone, sand, and gravel). Due to its highly viscous nature, bitumen must be heated so it can be mixed with the aggregates at the asphalt mixing facility. The temperature required varies depending upon characteristics of the bitumen and the aggregates, but warm-mix asphalt technologies allow producers to reduce the temperature required.
The weight of an asphalt pavement depends upon the aggregate type, the bitumen, and the air void content. An average example in the United States is about 112 pounds per square yard, per inch of pavement thickness.
When maintenance is performed on asphalt pavements, such as milling to remove a worn or damaged surface, the removed material can be returned to a facility for processing into new pavement mixtures. The bitumen in the removed material can be reactivated and put back to use in new pavement mixes. With some 95% of paved roads being constructed of or surfaced with asphalt, a substantial amount of asphalt pavement material is reclaimed each year. According to industry surveys conducted annually by the Federal Highway Administration and the National Asphalt Pavement Association, more than 99% of the bitumen removed each year from road surfaces during widening and resurfacing projects is reused as part of new pavements, roadbeds, shoulders and embankments or stockpiled for future use.
Asphalt concrete paving is widely used in airports around the world. Due to the sturdiness and ability to be repaired quickly, it is widely used for runways.
Mastic asphalt.
Mastic asphalt is a type of asphalt that differs from dense graded asphalt (asphalt concrete) in that it has a higher bitumen (binder) content, usually around 7–10% of the whole aggregate mix, as opposed to rolled asphalt concrete, which has only around 5% asphalt. This thermoplastic substance is widely used in the building industry for waterproofing flat roofs and tanking underground. Mastic asphalt is heated to a temperature of and is spread in layers to form an impervious barrier about thick.
Bitumen emulsion.
Bitumen emulsions are colloidal mixtures of bitumen and water. Due to the different surface tensions of the two liquids, stable emulsions cannot be created simply by mixing. Therefore, various emulsifiers and stabilizers are added. Emulsifiers are amphiphilic molecules that differ in the charge of their polar head group. They reduce the surface tension of the emulsion and thus prevent bitumen particles from fusing. The emulsifier charge defines the type of emulsion: anionic (negatively charged) and cationic (positively charged). The concentration of an emulsifier is a critical parameter affecting the size of the bitumen particles—higher concentrations lead to smaller bitumen particles. Thus, emulsifiers have a great impact on the stability, viscosity, breaking strength, and adhesion of the bitumen emulsion. The size of bitumen particles is usually between 0.1 and 50 μm with a main fraction between 1 μm and 10 μm. Laser diffraction techniques can be used to determine the particle size distribution quickly and easily. Cationic emulsifiers primarily include long-chain amines such as imidazolines, amido-amines, and diamines, which acquire a positive charge when an acid is added. Anionic emulsifiers are often fatty acids extracted from lignin, tall oil, or tree resin saponified with bases such as NaOH, which creates a negative charge.
During the storage of bitumen emulsions, bitumen particles sediment, agglomerate (flocculation), or fuse (coagulation), which leads to a certain instability of the bitumen emulsion. How fast this process occurs depends on the formulation of the bitumen emulsion but also storage conditions such as temperature and humidity. When emulsified bitumen gets into contact with aggregates, emulsifiers lose their effectiveness, the emulsion breaks down, and an adhering bitumen film is formed referred to as 'breaking'. Bitumen particles almost instantly create a continuous bitumen film by coagulating and separating from water which evaporates. Not each asphalt emulsion reacts as fast as the other when it gets into contact with aggregates. That enables a classification into Rapid-setting (R), Slow-setting (SS), and Medium-setting (MS) emulsions, but also an individual, application-specific optimization of the formulation and a wide field of application (1). For example, Slow-breaking emulsions ensure a longer processing time which is particularly advantageous for fine aggregates (1).
Adhesion problems are reported for anionic emulsions in contact with quartz-rich aggregates. They are substituted by cationic emulsions achieving better adhesion. The extensive range of bitumen emulsions is covered insufficiently by standardization. DIN EN 13808 for cationic asphalt emulsions has been existing since July 2005. Here, a classification of bitumen emulsions based on letters and numbers is described, considering charges, viscosities, and the type of bitumen. The production process of bitumen emulsions is very complex. Two methods are commonly used, the "Colloid mill" method and the "High Internal Phase Ratio (HIPR)" method. In the "Colloid mill" method, a rotor moves at high speed within a stator by adding bitumen and a water-emulsifier mixture. The resulting shear forces generate bitumen particles between 5 μm and 10 μm coated with emulsifiers. The "High Internal Phase Ratio (HIPR)" method is used for creating smaller bitumen particles, monomodal, narrow particle size distributions, and very high bitumen concentrations. Here, a highly concentrated bitumen emulsion is produced first by moderate stirring and diluted afterward. In contrast to the "Colloid-Mill" method, the aqueous phase is introduced into hot bitumen, enabling very high bitumen concentrations.
T The "High Internal Phase Ratio (HIPR)" method is used for creating smaller bitumen particles, monomodal, narrow particle size distributions, and very high bitumen concentrations. Here, a highly concentrated bitumen emulsion is produced first by moderate stirring and diluted afterward. In contrast to the "Colloid-Mill" method, the aqueous phase is introduced into hot bitumen, enabling very high bitumen concentrations (1).he "High Internal Phase Ratio (HIPR)" method is used for creating smaller bitumen particles, monomodal, narrow particle size distributions, and very high bitumen concentrations. Here, a highly concentrated bitumen emulsion is produced first by moderate stirring and diluted afterward. In contrast to the "Colloid-Mill" method, the aqueous phase is introduced into hot bitumen, enabling very high bitumen concentrations (1).
Bitumen emulsions are used in a wide variety of applications. They are used in road construction and building protection and primarily include the application in cold recycling mixtures, adhesive coating, and surface treatment (1). Due to the lower viscosity in comparison to hot bitumen, processing requires less energy and is associated with significantly less risk of fire and burns. Chipseal involves spraying the road surface with bitumen emulsion followed by a layer of crushed rock, gravel or crushed slag. Slurry seal is a mixture of bitumen emulsion and fine crushed aggregate that is spread on the surface of a road. Cold-mixed asphalt can also be made from bitumen emulsion to create pavements similar to hot-mixed asphalt, several inches in depth, and bitumen emulsions are also blended into recycled hot-mix asphalt to create low-cost pavements. Bitumen emulsion based techniques are known to be useful for all classes of roads, their use may also be possible in the following applications: 1. Asphalts for heavily trafficked roads (based on the use of polymer modified emulsions) 2. Warm emulsion based mixtures, to improve both their maturation time and mechanical properties 3. Half-warm technology, in which aggregates are heated up to 100 degrees, producing mixtures with similar properties to those of hot asphalts 4. High performance surface dressing.
Synthetic crude oil.
Synthetic crude oil, also known as syncrude, is the output from a bitumen upgrader facility used in connection with oil sand production in Canada. Bituminous sands are mined using enormous (100-ton capacity) power shovels and loaded into even larger (400-ton capacity) dump trucks for movement to an upgrading facility. The process used to extract the bitumen from the sand is a hot water process originally developed by Dr. Karl Clark of the University of Alberta during the 1920s. After extraction from the sand, the bitumen is fed into a bitumen upgrader which converts it into a light crude oil equivalent. This synthetic substance is fluid enough to be transferred through conventional oil pipelines and can be fed into conventional oil refineries without any further treatment. By 2015 Canadian bitumen upgraders were producing over per day of synthetic crude oil, of which 75% was exported to oil refineries in the United States.
In Alberta, five bitumen upgraders produce synthetic crude oil and a variety of other products: The Suncor Energy upgrader near Fort McMurray, Alberta produces synthetic crude oil plus diesel fuel; the Syncrude Canada, Canadian Natural Resources, and Nexen upgraders near Fort McMurray produce synthetic crude oil; and the Shell Scotford Upgrader near Edmonton produces synthetic crude oil plus an intermediate feedstock for the nearby Shell Oil Refinery. A sixth upgrader, under construction in 2015 near Redwater, Alberta, will upgrade half of its crude bitumen directly to diesel fuel, with the remainder of the output being sold as feedstock to nearby oil refineries and petrochemical plants.
Non-upgraded crude bitumen.
Canadian bitumen does not differ substantially from oils such as Venezuelan extra-heavy and Mexican heavy oil in chemical composition, and the real difficulty is moving the extremely viscous bitumen through oil pipelines to the refinery. Many modern oil refineries are extremely sophisticated and can process non-upgraded bitumen directly into products such as gasoline, diesel fuel, and refined asphalt without any preprocessing. This is particularly common in areas such as the US Gulf coast, where refineries were designed to process Venezuelan and Mexican oil, and in areas such as the US Midwest where refineries were rebuilt to process heavy oil as domestic light oil production declined. Given the choice, such heavy oil refineries usually prefer to buy bitumen rather than synthetic oil because the cost is lower, and in some cases because they prefer to produce more diesel fuel and less gasoline. By 2015 Canadian production and exports of non-upgraded bitumen exceeded that of synthetic crude oil at over per day, of which about 65% was exported to the United States.
Because of the difficulty of moving crude bitumen through pipelines, non-upgraded bitumen is usually diluted with natural-gas condensate in a form called dilbit or with synthetic crude oil, called synbit. However, to meet international competition, much non-upgraded bitumen is now sold as a blend of multiple grades of bitumen, conventional crude oil, synthetic crude oil, and condensate in a standardized benchmark product such as Western Canadian Select. This sour, heavy crude oil blend is designed to have uniform refining characteristics to compete with internationally marketed heavy oils such as Mexican Mayan or Arabian Dubai Crude.
Radioactive waste encapsulation matrix.
Bitumen was used starting in the 1960s as a hydrophobic matrix aiming to encapsulate radioactive waste such as medium-activity salts (mainly soluble sodium nitrate and sodium sulfate) produced by the reprocessing of spent nuclear fuels or radioactive sludges from sedimentation ponds. Bituminised radioactive waste containing highly radiotoxic alpha-emitting transuranic elements from nuclear reprocessing plants have been produced at industrial scale in France, Belgium and Japan, but this type of waste conditioning has been abandoned because operational safety issues (risks of fire, as occurred in a bituminisation plant at Tokai Works in Japan) and long-term stability problems related to their geological disposal in deep rock formations. One of the main problems is the swelling of bitumen exposed to radiation and to water. Bitumen swelling is first induced by radiation because of the presence of hydrogen gas bubbles generated by alpha and gamma radiolysis. A second mechanism is the matrix swelling when the encapsulated hygroscopic salts exposed to water or moisture start to rehydrate and to dissolve. The high concentration of salt in the pore solution inside the bituminised matrix is then responsible for osmotic effects inside the bituminised matrix. The water moves in the direction of the concentrated salts, the bitumen acting as a semi-permeable membrane. This also causes the matrix to swell. The swelling pressure due to osmotic effect under constant volume can be as high as 200 bar. If not properly managed, this high pressure can cause fractures in the near field of a disposal gallery of bituminised medium-level waste. When the bituminised matrix has been altered by swelling, encapsulated radionuclides are easily leached by the contact of ground water and released in the geosphere. The high ionic strength of the concentrated saline solution also favours the migration of radionuclides in clay host rocks. The presence of chemically reactive nitrate can also affect the redox conditions prevailing in the host rock by establishing oxidizing conditions, preventing the reduction of redox-sensitive radionuclides. Under their higher valences, radionuclides of elements such as selenium, technetium, uranium, neptunium and plutonium have a higher solubility and are also often present in water as non-retarded anions. This makes the disposal of medium-level bituminised waste very challenging.
Different types of bitumen have been used: blown bitumen (partly oxidized with air oxygen at high temperature after distillation, and harder) and direct distillation bitumen (softer). Blown bitumens like Mexphalte, with a high content of saturated hydrocarbons, are more easily biodegraded by microorganisms than direct distillation bitumen, with a low content of saturated hydrocarbons and a high content of aromatic hydrocarbons.
Concrete encapsulation of radwaste is presently considered a safer alternative by the nuclear industry and the waste management organisations.
Other uses.
Roofing shingles and roll roofing account for most of the remaining bitumen consumption. Other uses include cattle sprays, fence-post treatments, and waterproofing for fabrics. Bitumen is used to make Japan black, a lacquer known especially for its use on iron and steel, and it is also used in paint and marker inks by some exterior paint supply companies to increase the weather resistance and permanence of the paint or ink, and to make the color darker. Bitumen is also used to seal some alkaline batteries during the manufacturing process.
Production.
About 164,000,000 tons were produced in 2019. It is obtained as the "heavy" (i.e., difficult to distill) fraction. Material with a boiling point greater than around 500 °C is considered asphalt. Vacuum distillation separates it from the other components in crude oil (such as naphtha, gasoline and diesel). The resulting material is typically further treated to extract small but valuable amounts of lubricants and to adjust the properties of the material to suit applications. In a de-asphalting unit, the crude bitumen is treated with either propane or butane in a supercritical phase to extract the lighter molecules, which are then separated. Further processing is possible by "blowing" the product: namely reacting it with oxygen. This step makes the product harder and more viscous.
Bitumen is typically stored and transported at temperatures around . Sometimes diesel oil or kerosene are mixed in before shipping to retain liquidity; upon delivery, these lighter materials are separated out of the mixture. This mixture is often called "bitumen feedstock", or BFS. Some dump trucks route the hot engine exhaust through pipes in the dump body to keep the material warm. The backs of tippers carrying asphalt, as well as some handling equipment, are also commonly sprayed with a releasing agent before filling to aid release. Diesel oil is no longer used as a release agent due to environmental concerns.
Oil sands.
Naturally occurring crude bitumen impregnated in sedimentary rock is the prime feed stock for petroleum production from "oil sands", currently under development in Alberta, Canada. Canada has most of the world's supply of natural bitumen, covering 140,000 square kilometres (an area larger than England), giving it the second-largest proven oil reserves in the world. The Athabasca oil sands are the largest bitumen deposit in Canada and the only one accessible to surface mining, although recent technological breakthroughs have resulted in deeper deposits becoming producible by "in situ" methods. Because of oil price increases after 2003, producing bitumen became highly profitable, but as a result of the decline after 2014 it became uneconomic to build new plants again. By 2014, Canadian crude bitumen production averaged about per day and was projected to rise to per day by 2020. The total amount of crude bitumen in Alberta that could be extracted is estimated to be about , which at a rate of would last about 200 years.
Alternatives and bioasphalt.
Although uncompetitive economically, bitumen can be made from nonpetroleum-based renewable resources such as sugar, molasses and rice, corn and potato starches. Bitumen can also be made from waste material by fractional distillation of used motor oil, which is sometimes otherwise disposed of by burning or dumping into landfills. Use of motor oil may cause premature cracking in colder climates, resulting in roads that need to be repaved more frequently.
Nonpetroleum-based asphalt binders can be made light-colored. Lighter-colored roads absorb less heat from solar radiation, reducing their contribution to the urban heat island effect. Parking lots that use bitumen alternatives are called green parking lots.
Albanian deposits.
Selenizza is a naturally occurring solid hydrocarbon bitumen found in native deposits in Selenice, in Albania, the only European asphalt mine still in use. The bitumen is found in the form of veins, filling cracks in a more or less horizontal direction. The bitumen content varies from 83% to 92% (soluble in carbon disulphide), with a penetration value near to zero and a softening point (ring and ball) around 120 °C. The insoluble matter, consisting mainly of silica ore, ranges from 8% to 17%.
Albanian bitumen extraction has a long history and was practiced in an organized way by the Romans. After centuries of silence, the first mentions of Albanian bitumen appeared only in 1868, when the Frenchman Coquand published the first geological description of the deposits of Albanian bitumen. In 1875, the exploitation rights were granted to the Ottoman government and in 1912, they were transferred to the Italian company Simsa. Since 1945, the mine was exploited by the Albanian government and from 2001 to date, the management passed to a French company, which organized the mining process for the manufacture of the natural bitumen on an industrial scale.
Today the mine is predominantly exploited in an open pit quarry but several of the many underground mines (deep and extending over several km) still remain viable. Selenizza is produced primarily in granular form, after melting the bitumen pieces selected in the mine.
Selenizza is mainly used as an additive in the road construction sector. It is mixed with traditional bitumen to improve both the viscoelastic properties and the resistance to ageing. It may be blended with the hot bitumen in tanks, but its granular form allows it to be fed in the mixer or in the recycling ring of normal asphalt plants. Other typical applications include the production of mastic asphalts for sidewalks, bridges, car-parks and urban roads as well as drilling fluid additives for the oil and gas industry. Selenizza is available in powder or in granular material of various particle sizes and is packaged in sacks or in thermal fusible polyethylene bags.
A life-cycle assessment study of the natural selenizza compared with petroleum bitumen has shown that the environmental impact of the selenizza is about half the impact of the road asphalt produced in oil refineries in terms of carbon dioxide emission.
Recycling.
Bitumen is a commonly recycled material in the construction industry. The two most common recycled materials that contain bitumen are reclaimed asphalt pavement (RAP) and reclaimed asphalt shingles (RAS). RAP is recycled at a greater rate than any other material in the United States, and typically contains approximately 5–6% bitumen binder. Asphalt shingles typically contain 20–40% bitumen binder.
Bitumen naturally becomes stiffer over time due to oxidation, evaporation, exudation, and physical hardening. For this reason, recycled asphalt is typically combined with virgin asphalt, softening agents, and/or rejuvenating additives to restore its physical and chemical properties.
Economics.
Although bitumen typically makes up only 4 to 5 percent (by weight) of the pavement mixture, as the pavement's binder, it is also the most expensive part of the cost of the road-paving material.
During bitumen's early use in modern paving, oil refiners gave it away. However, bitumen is a highly traded commodity today. Its prices increased substantially in the early 21st Century. A U.S. government report states:
The report indicates that an "average" 1-mile (1.6-kilometer)-long, four-lane highway would include "300 tons of asphalt," which, "in 2002 would have cost around $48,000. By 2006 this would have increased to $96,000 and by 2012 to $183,000... an increase of about $135,000 for every mile of highway in just 10 years."
The Middle East is a significant exporter of bitumen, particularly to India and China. According to the Argus Bitumen Report (2024/07/12), India is the largest importer, driven by extensive infrastructure projects. The report projects a CAGR of 4.5% for India's bitumen imports over the next five years, while China's imports are expected to grow at a CAGR of 3.8%. The current export price to India is approximately $350 per metric ton, and for China, it is around $360 per metric ton. The Middle East's strategic advantage in crude oil production underpins its capacity to meet these demands.
Health and safety.
People can be exposed to bitumen in the workplace by breathing in fumes or skin absorption. The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit of 5 mg/m3 over a 15-minute period.
Bitumen is a largely inert material that must be heated or diluted to a point where it becomes workable for the production of materials for paving, roofing, and other applications. In examining the potential health hazards associated with bitumen, the International Agency for Research on Cancer (IARC) determined that it is the application parameters, predominantly temperature, that affect occupational exposure and the potential bioavailable carcinogenic hazard/risk of the bitumen emissions. In particular, temperatures greater than 199 °C (390 °F), were shown to produce a greater exposure risk than when bitumen was heated to lower temperatures, such as those typically used in asphalt pavement mix production and placement. IARC has classified paving asphalt fumes as a Class 2B possible carcinogen, indicating inadequate evidence of carcinogenicity in humans.
In 2020, scientists reported that bitumen currently is a significant and largely overlooked source of air pollution in urban areas, especially during hot and sunny periods.
A bitumen-like substance found in the Himalayas and known as "shilajit" is sometimes used as an Ayurveda medicine, but is not in fact a tar, resin or bitumen.

</doc>
<doc id="659" url="?curid=659" title="American National Standards Institute">
American National Standards Institute

The American National Standards Institute (ANSI ) is a private nonprofit organization that oversees the development of voluntary consensus standards for products, services, processes, systems, and personnel in the United States. The organization also coordinates U.S. standards with international standards so that American products can be used worldwide.
ANSI accredits standards that are developed by representatives of other standards organizations, government agencies, consumer groups, companies, and others. These standards ensure that the characteristics and performance of products are consistent, that people use the same definitions and terms, and that products are tested the same way. ANSI also accredits organizations that carry out product or personnel certification in accordance with requirements defined in international standards.
The organization's headquarters are in Washington, D.C. ANSI's operations office is located in New York City. The ANSI annual operating budget is funded by the sale of publications, membership dues and fees, accreditation services, fee-based programs, and international standards programs.
Many ANSI regulations are incorporated by reference into United States federal statutes (i.e. by OSHA regulations referring to individual ANSI specifications). ANSI does not make these standards publicly available, and charges money for access to these documents; it further claims that it is copyright infringement for them to be provided to the public by others free of charge. These assertions have been the subject of criticism and litigation.
History.
ANSI was most likely formed in 1918, when five engineering societies and three government agencies founded the American Engineering Standards Committee (AESC). In 1928, the AESC became the American Standards Association (ASA). In 1966, the ASA was reorganized and became United States of America Standards Institute (USASI). The present name was adopted in 1969.
Prior to 1918, these five founding engineering societies:
had been members of the United Engineering Society (UES). At the behest of the AIEE, they invited the U.S. government Departments of War, Navy (combined in 1947 to become the Department of Defense or DOD) and Commerce to join in founding a national standards organization.
According to Adam Stanton, the first permanent secretary and head of staff in 1919, AESC started as an ambitious program and little else. Staff for the first year consisted of one executive, Clifford B. LePage, who was on loan from a founding member, ASME. An annual budget of $7,500 was provided by the founding bodies.
In 1931, the organization (renamed ASA in 1928) became affiliated with the U.S. National Committee of the International Electrotechnical Commission (IEC), which had been formed in 1904 to develop electrical and electronics standards.
Members.
ANSI's members are government agencies, organizations, academic and international bodies, and individuals. In total, the Institute represents the interests of more than 270,000 companies and organizations and 30 million professionals worldwide. 
ANSI's market-driven, decentralized approach has been criticized in comparison with more planned and organized international approaches to standardization. An underlying issue is the difficulty of balancing "the interests of both the nation's industrial and commercial sectors and the nation as a whole."
Process.
Although ANSI itself does not develop standards, the Institute oversees the development and use of standards by accrediting the procedures of standards developing organizations. ANSI accreditation signifies that the procedures used by standards developing organizations meet the institute's requirements for openness, balance, consensus, and due process.
ANSI also designates specific standards as American National Standards, or ANS, when the Institute determines that the standards were developed in an environment that is equitable, accessible and responsive to the requirements of various stakeholders.
Voluntary consensus standards quicken the market acceptance of products while making clear how to improve the safety of those products for the protection of consumers. There are approximately 9,500 American National Standards that carry the ANSI designation.
The American National Standards process involves:
International activities.
In addition to facilitating the formation of standards in the United States, ANSI promotes the use of U.S. standards internationally, advocates U.S. policy and technical positions in international and regional standards organizations, and encourages the adoption of international standards as national standards where appropriate.
The institute is the official U.S. representative to the two major international standards organizations, the International Organization for Standardization (ISO), as a founding member, and the International Electrotechnical Commission (IEC), via the U.S. National Committee (USNC). ANSI participates in almost the entire technical program of both the ISO and the IEC, and administers many key committees and subgroups. In many instances, U.S. standards are taken forward to ISO and IEC, through ANSI or the USNC, where they are adopted in whole or in part as international standards.
Adoption of ISO and IEC standards as American standards increased from 0.2% in 1986 to 15.5% in May 2012.
Standards panels.
The Institute administers nine standards panels:
Each of the panels works to identify, coordinate, and harmonize voluntary standards relevant to these areas.
In 2009, ANSI and the National Institute of Standards and Technology (NIST) formed the Nuclear Energy Standards Coordination Collaborative (NESCC). NESCC is a joint initiative to identify and respond to the current need for standards in the nuclear industry.

</doc>
<doc id="661" url="?curid=661" title="Argument (disambiguation)">
Argument (disambiguation)

In logic and philosophy, an argument is an attempt to persuade someone of something, or give evidence or reasons for accepting a particular conclusion.
Argument may also refer to: 

</doc>
<doc id="662" url="?curid=662" title="Apollo 11">
Apollo 11

Apollo 11 (July 16–24, 1969) was the American spaceflight that first landed humans on the Moon. Commander Neil Armstrong and Lunar Module Pilot Buzz Aldrin landed the Apollo Lunar Module "Eagle" on July 20, 1969, at 20:17 UTC, and Armstrong became the first person to step onto the Moon's surface six hours and 39 minutes later, on July 21 at 02:56 UTC. Aldrin joined him 19 minutes later, and they spent about two and a quarter hours together exploring the site they had named Tranquility Base upon landing. Armstrong and Aldrin collected of lunar material to bring back to Earth as pilot Michael Collins flew the Command Module "Columbia" in lunar orbit, and were on the Moon's surface for 21 hours, 36 minutes before lifting off to rejoin "Columbia".
Apollo 11 was launched by a Saturn V rocket from Kennedy Space Center on Merritt Island, Florida, on July 16 at 13:32 UTC, and it was the fifth crewed mission of NASA's Apollo program. The Apollo spacecraft had three parts: a command module (CM) with a cabin for the three astronauts, the only part that returned to Earth; a service module (SM), which supported the command module with propulsion, electrical power, oxygen, and water; and a lunar module (LM) that had two stages—a descent stage for landing on the Moon and an ascent stage to place the astronauts back into lunar orbit.
After being sent to the Moon by the Saturn V's third stage, the astronauts separated the spacecraft from it and traveled for three days until they entered lunar orbit. Armstrong and Aldrin then moved into "Eagle" and landed in the Sea of Tranquility on July 20. The astronauts used "Eagle"s ascent stage to lift off from the lunar surface and rejoin Collins in the command module. They jettisoned "Eagle" before they performed the maneuvers that propelled "Columbia" out of the last of its 30 lunar orbits onto a trajectory back to Earth. They returned to Earth and splashed down in the Pacific Ocean on July 24 after more than eight days in space.
Armstrong's first step onto the lunar surface was broadcast on live TV to a worldwide audience. He described the event as "one small step for [a] man, one giant leap for mankind." Apollo 11 effectively proved U.S. victory in the Space Race to demonstrate spaceflight superiority, by fulfilling a national goal proposed in 1961 by President John F. Kennedy, "before this decade is out, of landing a man on the Moon and returning him safely to the Earth."
Background.
In the late 1950s and early 1960s, the United States was engaged in the Cold War, a geopolitical rivalry with the Soviet Union. On October 4, 1957, the Soviet Union launched Sputnik 1, the first artificial satellite. This surprise success fired fears and imaginations around the world. It demonstrated that the Soviet Union had the capability to deliver nuclear weapons over intercontinental distances, and challenged American claims of military, economic, and technological superiority. This precipitated the Sputnik crisis, and triggered the Space Race to prove which superpower would achieve superior spaceflight capability. President Dwight D. Eisenhower responded to the Sputnik challenge by creating the National Aeronautics and Space Administration (NASA), and initiating Project Mercury, which aimed to launch a man into Earth orbit. But on April 12, 1961, Soviet cosmonaut Yuri Gagarin became the first person in space, and the first to orbit the Earth. Nearly a month later, on May 5, 1961, Alan Shepard became the first American in space, completing a 15-minute suborbital journey. After being recovered from the Atlantic Ocean, he received a congratulatory telephone call from Eisenhower's successor, John F. Kennedy.
Since the Soviet Union had higher lift capacity launch vehicles, Kennedy chose, from among options presented by NASA, a challenge beyond the capacity of the existing generation of rocketry, so that the US and Soviet Union would be starting from a position of equality. A crewed mission to the Moon would serve this purpose.
On May 25, 1961, Kennedy addressed the United States Congress on "Urgent National Needs" and declared:
On September 12, 1962, Kennedy delivered another speech before a crowd of about 40,000 people in the Rice University football stadium in Houston, Texas. A widely quoted refrain from the middle portion of the speech reads as follows:
In spite of that, the proposed program faced the opposition of many Americans and was dubbed a "moondoggle" by Norbert Wiener, a mathematician at the Massachusetts Institute of Technology. The effort to land a man on the Moon already had a name: Project Apollo. When Kennedy met with Nikita Khrushchev, the Premier of the Soviet Union in June 1961, he proposed making the Moon landing a joint project, but Khrushchev did not take up the offer. Kennedy again proposed a joint expedition to the Moon in a speech to the United Nations General Assembly on September 20, 1963. The idea of a joint Moon mission was abandoned after Kennedy's death.
An early and crucial decision was choosing lunar orbit rendezvous over both direct ascent and Earth orbit rendezvous. A space rendezvous is an orbital maneuver in which two spacecraft navigate through space and meet up. In July 1962 NASA head James Webb announced that lunar orbit rendezvous would be used and that the Apollo spacecraft would have three major parts: a command module (CM) with a cabin for the three astronauts, and the only part that returned to Earth; a service module (SM), which supported the command module with propulsion, electrical power, oxygen, and water; and a lunar module (LM) that had two stages—a descent stage for landing on the Moon, and an ascent stage to place the astronauts back into lunar orbit. This design meant the spacecraft could be launched by a single Saturn V rocket that was then under development.
Technologies and techniques required for Apollo were developed by Project Gemini. The Apollo project was enabled by NASA's adoption of new advances in semiconductor electronic technology, including metal–oxide–semiconductor field-effect transistors (MOSFETs) in the Interplanetary Monitoring Platform (IMP) and silicon integrated circuit (IC) chips in the Apollo Guidance Computer (AGC).
Project Apollo was abruptly halted by the Apollo 1 fire on January 27, 1967, in which astronauts Gus Grissom, Ed White, and Roger B. Chaffee died, and the subsequent investigation. In October 1968, Apollo 7 evaluated the command module in Earth orbit, and in December Apollo 8 tested it in lunar orbit. In March 1969, Apollo 9 put the lunar module through its paces in Earth orbit, and in May Apollo 10 conducted a "dress rehearsal" in lunar orbit. By July 1969, all was in readiness for Apollo 11 to take the final step onto the Moon.
The Soviet Union appeared to be winning the Space Race by beating the US to firsts, but its early lead was overtaken by the US Gemini program and Soviet failure to develop the N1 launcher, which would have been comparable to the Saturn V. The Soviets tried to beat the US to return lunar material to the Earth by means of uncrewed probes. On July 13, three days before Apollo 11's launch, the Soviet Union launched Luna 15, which reached lunar orbit before Apollo 11. During descent, a malfunction caused Luna 15 to crash in Mare Crisium about two hours before Armstrong and Aldrin took off from the Moon's surface to begin their voyage home. The Nuffield Radio Astronomy Laboratories radio telescope in England recorded transmissions from Luna 15 during its descent, and these were released in July 2009 for the 40th anniversary of Apollo 11.
Personnel.
Prime crew.
The initial crew assignment of Commander Neil Armstrong, Command Module Pilot (CMP) Jim Lovell, and Lunar Module Pilot (LMP) Buzz Aldrin on the backup crew for Apollo 9 was officially announced on November 20, 1967. Lovell and Aldrin had previously flown together as the crew of Gemini 12. Due to design and manufacturing delays in the LM, Apollo 8 and Apollo 9 swapped prime and backup crews, and Armstrong's crew became the backup for Apollo 8. Based on the normal crew rotation scheme, Armstrong was then expected to command Apollo 11.
There would be one change. Michael Collins, the CMP on the Apollo 8 crew, began experiencing trouble with his legs. Doctors diagnosed the problem as a bony growth between his fifth and sixth vertebrae, requiring surgery. Lovell took his place on the Apollo 8 crew, and when Collins recovered he joined Armstrong's crew as CMP. In the meantime, Fred Haise filled in as backup LMP, and Aldrin as backup CMP for Apollo 8. Apollo 11 was the second American mission where all the crew members had prior spaceflight experience, the first being Apollo 10. The next was STS-26 in 1988.
Deke Slayton gave Armstrong the option to replace Aldrin with Lovell, since some thought Aldrin was difficult to work with. Armstrong had no issues working with Aldrin but thought it over for a day before declining. He thought Lovell deserved to command his own mission (eventually Apollo 13).
The Apollo 11 prime crew had none of the close cheerful camaraderie characterized by that of Apollo 12. Instead, they forged an amiable working relationship. Armstrong in particular was notoriously aloof, but Collins, who considered himself a loner, confessed to rebuffing Aldrin's attempts to create a more personal relationship. Aldrin and Collins described the crew as "amiable strangers". Armstrong did not agree with the assessment, and said "... all the crews I was on worked very well together."
Backup crew.
The backup crew consisted of Lovell as Commander, William Anders as CMP, and Haise as LMP. Anders had flown with Lovell on Apollo 8. In early 1969, Anders accepted a job with the National Aeronautics and Space Council effective August 1969, and announced he would retire as an astronaut at that time. Ken Mattingly was moved from the support crew into parallel training with Anders as backup CMP in case Apollo 11 was delayed past its intended July launch date, at which point Anders would be unavailable.
By the normal crew rotation in place during Apollo, Lovell, Mattingly, and Haise were scheduled to fly on Apollo 14, but the three of them were bumped to Apollo 13: there was a crew issue for Apollo 13 as none of them except Edgar Mitchell flew in space again. George Mueller rejected the crew and this was the first time an Apollo crew was rejected. To give Alan Shepard more training time, Lovell's crew were bumped to Apollo 13. Mattingly would later be replaced by Jack Swigert as CMP on Apollo 13.
Support crew.
During Projects Mercury and Gemini, each mission had a prime and a backup crew. For Apollo, a third crew of astronauts was added, known as the support crew. The support crew maintained the flight plan, checklists and mission ground rules, and ensured the prime and backup crews were apprised of changes. They developed procedures, especially those for emergency situations, so these were ready for when the prime and backup crews came to train in the simulators, allowing them to concentrate on practicing and mastering them. For Apollo 11, the support crew consisted of Ken Mattingly, Ronald Evans and Bill Pogue.
Capsule communicators.
The capsule communicator (CAPCOM) was an astronaut at the Mission Control Center in Houston, Texas, who was the only person who communicated directly with the flight crew. For Apollo 11, the CAPCOMs were: Charles Duke, Ronald Evans, Bruce McCandless II, James Lovell, William Anders, Ken Mattingly, Fred Haise, Don L. Lind, Owen K. Garriott and Harrison Schmitt.
Flight directors.
The flight directors for this mission were:
Other key personnel.
Other key personnel who played important roles in the Apollo 11 mission include the following.
Preparations.
Insignia.
The Apollo 11 mission emblem was designed by Collins, who wanted a symbol for "peaceful lunar landing by the United States". At Lovell's suggestion, he chose the bald eagle, the national bird of the United States, as the symbol. Tom Wilson, a simulator instructor, suggested an olive branch in its beak to represent their peaceful mission. Collins added a lunar background with the Earth in the distance. The sunlight in the image was coming from the wrong direction; the shadow should have been in the lower part of the Earth instead of the left. Aldrin, Armstrong and Collins decided the Eagle and the Moon would be in their natural colors, and decided on a blue and gold border. Armstrong was concerned that "eleven" would not be understood by non-English speakers, so they went with "Apollo 11", and they decided not to put their names on the patch, so it would "be representative of "everyone" who had worked toward a lunar landing".
An illustrator at the Manned Spacecraft Center (MSC) did the artwork, which was then sent off to NASA officials for approval. The design was rejected. Bob Gilruth, the director of the MSC felt the talons of the eagle looked "too warlike". After some discussion, the olive branch was moved to the talons. When the Eisenhower dollar coin was released in 1971, the patch design provided the eagle for its reverse side. The design was also used for the smaller Susan B. Anthony dollar unveiled in 1979.
Call signs.
After the crew of Apollo 10 named their spacecraft "Charlie Brown" and "Snoopy", assistant manager for public affairs Julian Scheer wrote to George Low, the Manager of the Apollo Spacecraft Program Office at the MSC, to suggest the Apollo 11 crew be less flippant in naming their craft. The name "Snowcone" was used for the CM and "Haystack" was used for the LM in both internal and external communications during early mission planning.
The LM was named "Eagle" after the motif which was featured prominently on the mission insignia. At Scheer's suggestion, the CM was named "Columbia" after "Columbiad", the giant cannon that launched a spacecraft (also from Florida) in Jules Verne's 1865 novel "From the Earth to the Moon". It also referred to Columbia, a historical name of the United States. In Collins' 1976 book, he said "Columbia" was in reference to Christopher Columbus.
Mementos.
The astronauts had personal preference kits (PPKs), small bags containing personal items of significance they wanted to take with them on the mission. Five PPKs were carried on Apollo 11: three (one for each astronaut) were stowed on "Columbia" before launch, and two on "Eagle".
Neil Armstrong's LM PPK contained a piece of wood from the Wright brothers' 1903 "Wright Flyer"s left propeller and a piece of fabric from its wing, along with a diamond-studded astronaut pin originally given to Slayton by the widows of the Apollo 1 crew. This pin had been intended to be flown on that mission and given to Slayton afterwards, but following the disastrous launch pad fire and subsequent funerals, the widows gave the pin to Slayton. Armstrong took it with him on Apollo 11.
Site selection.
NASA's Apollo Site Selection Board announced five potential landing sites on February 8, 1968. These were the result of two years' worth of studies based on high-resolution photography of the lunar surface by the five uncrewed probes of the Lunar Orbiter program and information about surface conditions provided by the Surveyor program. The best Earth-bound telescopes could not resolve features with the resolution Project Apollo required. The landing site had to be close to the lunar equator to minimize the amount of propellant required, clear of obstacles to minimize maneuvering, and flat to simplify the task of the landing radar. Scientific value was not a consideration.
Areas that appeared promising on photographs taken on Earth were often found to be totally unacceptable. The original requirement that the site be free of craters had to be relaxed, as no such site was found. Five sites were considered: Sites 1 and 2 were in the Sea of Tranquility ("Mare Tranquillitatis"); Site 3 was in the Central Bay ("Sinus Medii"); and Sites 4 and 5 were in the Ocean of Storms ("Oceanus Procellarum").
The final site selection was based on seven criteria:
The requirement for the Sun angle was particularly restrictive, limiting the launch date to one day per month. A landing just after dawn was chosen to limit the temperature extremes the astronauts would experience. The Apollo Site Selection Board selected Site 2, with Sites 3 and 5 as backups in the event of the launch being delayed. In May 1969, Apollo 10's lunar module flew to within of Site 2, and reported it was acceptable.
First-step decision.
During the first press conference after the Apollo 11 crew was announced, the first question was, "Which one of you gentlemen will be the first man to step onto the lunar surface?" Slayton told the reporter it had not been decided, and Armstrong added that it was "not based on individual desire".
One of the first versions of the egress checklist had the lunar module pilot exit the spacecraft before the commander, which matched what had been done on Gemini missions, where the commander had never performed the spacewalk. Reporters wrote in early 1969 that Aldrin would be the first man to walk on the Moon, and Associate Administrator George Mueller told reporters he would be first as well. Aldrin heard that Armstrong would be the first because Armstrong was a civilian, which made Aldrin livid. Aldrin attempted to persuade other lunar module pilots he should be first, but they responded cynically about what they perceived as a lobbying campaign. Attempting to stem interdepartmental conflict, Slayton told Aldrin that Armstrong would be first since he was the commander. The decision was announced in a press conference on April 14, 1969.
For decades, Aldrin believed the final decision was largely driven by the lunar module's hatch location. Because the astronauts had their spacesuits on and the spacecraft was so small, maneuvering to exit the spacecraft was difficult. The crew tried a simulation in which Aldrin left the spacecraft first, but he damaged the simulator while attempting to egress. While this was enough for mission planners to make their decision, Aldrin and Armstrong were left in the dark on the decision until late spring. Slayton told Armstrong the plan was to have him leave the spacecraft first, if he agreed. Armstrong said, "Yes, that's the way to do it."
The media accused Armstrong of exercising his commander's prerogative to exit the spacecraft first. Chris Kraft revealed in his 2001 autobiography that a meeting occurred between Gilruth, Slayton, Low, and himself to make sure Aldrin would not be the first to walk on the Moon. They argued that the first person to walk on the Moon should be like Charles Lindbergh, a calm and quiet person. They made the decision to change the flight plan so the commander was the first to egress from the spacecraft.
Pre-launch.
The ascent stage of LM-5 "Eagle" arrived at the Kennedy Space Center on January 8, 1969, followed by the descent stage four days later, and CSM-107 "Columbia" on January 23. There were several differences between "Eagle" and Apollo 10's LM-4 "Snoopy"; "Eagle" had a VHF radio antenna to facilitate communication with the astronauts during their EVA on the lunar surface; a lighter ascent engine; more thermal protection on the landing gear; and a package of scientific experiments known as the Early Apollo Scientific Experiments Package (EASEP). The only change in the configuration of the command module was the removal of some insulation from the forward hatch. The CSM was mated on January 29, and moved from the Operations and Checkout Building to the Vehicle Assembly Building on April 14.
The S-IVB third stage of Saturn V AS-506 had arrived on January 18, followed by the S-II second stage on February 6, S-IC first stage on February 20, and the Saturn V Instrument Unit on February 27. At 12:30 on May 20, the assembly departed the Vehicle Assembly Building atop the crawler-transporter, bound for Launch Pad 39A, part of Launch Complex 39, while Apollo 10 was still on its way to the Moon. A countdown test commenced on June 26, and concluded on July 2. The launch complex was floodlit on the night of July 15, when the crawler-transporter carried the mobile service structure back to its parking area. In the early hours of the morning, the fuel tanks of the S-II and S-IVB stages were filled with liquid hydrogen. Fueling was completed by three hours before launch. Launch operations were partly automated, with 43 programs written in the ATOLL programming language.
Slayton roused the crew shortly after 04:00, and they showered, shaved, and had the traditional pre-flight breakfast of steak and eggs with Slayton and the backup crew. They then donned their space suits and began breathing pure oxygen. At 06:30, they headed out to Launch Complex 39. Haise entered "Columbia" about three hours and ten minutes before launch time. Along with a technician, he helped Armstrong into the left-hand couch at 06:54. Five minutes later, Collins joined him, taking up his position on the right-hand couch. Finally, Aldrin entered, taking the center couch. Haise left around two hours and ten minutes before launch. The closeout crew sealed the hatch, and the cabin was purged and pressurized. The closeout crew then left the launch complex about an hour before launch time. The countdown became automated at three minutes and twenty seconds before launch time. Over 450 personnel were at the consoles in the firing room.
Mission.
Launch and flight to lunar orbit.
An estimated one million spectators watched the launch of Apollo 11 from the highways and beaches in the vicinity of the launch site. Dignitaries included the Chief of Staff of the United States Army, General William Westmoreland, four cabinet members, 19 state governors, 40 mayors, 60 ambassadors and 200 congressmen. Vice President Spiro Agnew viewed the launch with former president Lyndon B. Johnson and his wife Lady Bird Johnson. Around 3,500 media representatives were present. About two-thirds were from the United States; the rest came from 55 other countries. The launch was televised live in 33 countries, with an estimated 25 million viewers in the United States alone. Millions more around the world listened to radio broadcasts. President Richard Nixon viewed the launch from his office in the White House with his NASA liaison officer, Apollo astronaut Frank Borman.
Saturn V AS-506 launched Apollo 11 on July 16, 1969, at 13:32:00 UTC (9:32:00 EDT). At 13.2 seconds into the flight, the launch vehicle began to roll into its flight azimuth of 72.058°. Full shutdown of the first-stage engines occurred about 2 minutes and 42 seconds into the mission, followed by separation of the S-IC and ignition of the S-II engines. The second stage engines then cut off and separated at about 9 minutes and 8 seconds, allowing the first ignition of the S-IVB engine a few seconds later.
Apollo 11 entered a near-circular Earth orbit at an altitude of by , twelve minutes into its flight. After one and a half orbits, a second ignition of the S-IVB engine pushed the spacecraft onto its trajectory toward the Moon with the trans-lunar injection (TLI) burn at 16:22:13 UTC. About 30 minutes later, with Collins in the left seat and at the controls, the transposition, docking, and extraction maneuver was performed. This involved separating "Columbia" from the spent S-IVB stage, turning around, and docking with "Eagle" still attached to the stage. After the LM was extracted, the combined spacecraft headed for the Moon, while the rocket stage flew on a trajectory past the Moon. This was done to avoid the third stage colliding with the spacecraft, the Earth, or the Moon. A slingshot effect from passing around the Moon threw it into an orbit around the Sun.
On July 19 at 17:21:50 UTC, Apollo 11 passed behind the Moon and fired its service propulsion engine to enter lunar orbit. In the thirty orbits that followed, the crew saw passing views of their landing site in the southern Sea of Tranquility about southwest of the crater Sabine D. The site was selected in part because it had been characterized as relatively flat and smooth by the automated Ranger 8 and Surveyor 5 landers and the Lunar Orbiter mapping spacecraft, and because it was unlikely to present major landing or EVA challenges. It lay about southeast of the Surveyor 5 landing site, and southwest of Ranger 8's crash site.
Lunar descent.
At 12:52:00 UTC on July 20, Aldrin and Armstrong entered "Eagle", and began the final preparations for lunar descent. At 17:44:00 "Eagle" separated from "Columbia". Collins, alone aboard "Columbia", inspected "Eagle" as it pirouetted before him to ensure the craft was not damaged, and that the landing gear was correctly deployed. Armstrong exclaimed: "The "Eagle" has wings!"
As the descent began, Armstrong and Aldrin found themselves passing landmarks on the surface two or three seconds early, and reported that they were "long"; they would land miles west of their target point. "Eagle" was traveling too fast. The problem could have been mascons—concentrations of high mass in a region or regions of the Moon's crust that contains a gravitational anomaly, potentially altering "Eagle" trajectory. Flight Director Gene Kranz speculated that it could have resulted from extra air pressure in the docking tunnel, or a result of "Eagle"s pirouette maneuver.
Five minutes into the descent burn, and above the surface of the Moon, the LM guidance computer (LGC) distracted the crew with the first of several unexpected 1201 and 1202 program alarms. Inside Mission Control Center, computer engineer Jack Garman told Guidance Officer Steve Bales it was safe to continue the descent, and this was relayed to the crew. The program alarms indicated "executive overflows", meaning the guidance computer could not complete all its tasks in real-time and had to postpone some of them. Margaret Hamilton, the Director of Apollo Flight Computer Programming at the MIT Charles Stark Draper Laboratory later recalled:
During the mission, the cause was diagnosed as the rendezvous radar switch being in the wrong position, causing the computer to process data from both the rendezvous and landing radars at the same time. Software engineer Don Eyles concluded in a 2005 Guidance and Control Conference paper that the problem was due to a hardware design bug previously seen during testing of the first uncrewed LM in Apollo 5. Having the rendezvous radar on (so it was warmed up in case of an emergency landing abort) should have been irrelevant to the computer, but an electrical phasing mismatch between two parts of the rendezvous radar system could cause the stationary antenna to appear to the computer as dithering back and forth between two positions, depending upon how the hardware randomly powered up. The extra spurious cycle stealing, as the rendezvous radar updated an involuntary counter, caused the computer alarms.
Landing.
When Armstrong again looked outside, he saw that the computer's landing target was in a boulder-strewn area just north and east of a crater (later determined to be West crater), so he took semi-automatic control. Armstrong considered landing short of the boulder field so they could collect geological samples from it, but could not since their horizontal velocity was too high. Throughout the descent, Aldrin called out navigation data to Armstrong, who was busy piloting "Eagle". Now above the surface, Armstrong knew their propellant supply was dwindling and was determined to land at the first possible landing site.
Armstrong found a clear patch of ground and maneuvered the spacecraft towards it. As he got closer, now above the surface, he discovered his new landing site had a crater in it. He cleared the crater and found another patch of level ground. They were now from the surface, with only 90 seconds of propellant remaining. Lunar dust kicked up by the LM's engine began to impair his ability to determine the spacecraft's motion. Some large rocks jutted out of the dust cloud, and Armstrong focused on them during his descent so he could determine the spacecraft's speed.
A light informed Aldrin that at least one of the probes hanging from "Eagle" footpads had touched the surface a few moments before the landing and he said: "Contact light!" Armstrong was supposed to immediately shut the engine down, as the engineers suspected the pressure caused by the engine's own exhaust reflecting off the lunar surface could make it explode, but he forgot. Three seconds later, "Eagle" landed and Armstrong shut the engine down. Aldrin immediately said "Okay, engine stop. ACA—out of detent." Armstrong acknowledged: "Out of detent. Auto." Aldrin continued: "Mode control—both auto. Descent engine command override off. Engine arm—off. 413 is in."
ACA was the Attitude Control Assembly—the LM's control stick. Output went to the LGC to command the reaction control system (RCS) jets to fire. "Out of Detent" meant the stick had moved away from its centered position; it was spring-centered like the turn indicator in a car. Address 413 of the Abort Guidance System (AGS) contained the variable that indicated the LM had landed.
"Eagle" landed at 20:17:40 UTC on Sunday July 20 with of usable fuel remaining. Information available to the crew and mission controllers during the landing showed the LM had enough fuel for another 25 seconds of powered flight before an abort without touchdown would have become unsafe, but post-mission analysis showed that the real figure was probably closer to 50 seconds. Apollo 11 landed with less fuel than most subsequent missions, and the astronauts encountered a premature low fuel warning. This was later found to be the result of the propellant sloshing more than expected, uncovering a fuel sensor. On subsequent missions, extra anti-slosh baffles were added to the tanks to prevent this.
Armstrong acknowledged Aldrin's completion of the post-landing checklist with "Engine arm is off", before responding to the CAPCOM, Charles Duke, with the words, "Houston, Tranquility Base here. The "Eagle" has landed." Armstrong's unrehearsed change of call sign from "Eagle" to "Tranquility Base" emphasized to listeners that landing was complete and successful. Duke expressed the relief at Mission Control: "Roger, Twan—Tranquility, we copy you on the ground. You got a bunch of guys about to turn blue. We're breathing again. Thanks a lot."
Two and a half hours after landing, before preparations began for the EVA, Aldrin radioed to Earth:
He then took communion privately. At this time NASA was still fighting a lawsuit brought by atheist Madalyn Murray O'Hair (who had objected to the Apollo 8 crew reading from the Book of Genesis) demanding that their astronauts refrain from broadcasting religious activities while in space. For this reason, Aldrin chose to refrain from directly mentioning taking communion on the Moon. Aldrin was an elder at the Webster Presbyterian Church, and his communion kit was prepared by the pastor of the church, Dean Woodruff. Webster Presbyterian possesses the chalice used on the Moon and commemorates the event each year on the Sunday closest to July 20. The schedule for the mission called for the astronauts to follow the landing with a five-hour sleep period, but they chose to begin preparations for the EVA early, thinking they would be unable to sleep.
Lunar surface operations.
Preparations for Neil Armstrong and Buzz Aldrin to walk on the Moon began at 23:43 UTC. These took longer than expected; three and a half hours instead of two. During training on Earth, everything required had been neatly laid out in advance, but on the Moon the cabin contained a large number of other items as well, such as checklists, food packets, and tools. Six hours and thirty-nine minutes after landing, Armstrong and Aldrin were ready to go outside, and "Eagle" was depressurized.
"Eagle"s hatch was opened at 02:39:33. Armstrong initially had some difficulties squeezing through the hatch with his portable life support system (PLSS). Some of the highest heart rates recorded from Apollo astronauts occurred during LM egress and ingress. At 02:51 Armstrong began his descent to the lunar surface. The remote control unit on his chest kept him from seeing his feet. Climbing down the nine-rung ladder, Armstrong pulled a D-ring to deploy the modular equipment stowage assembly (MESA) folded against "Eagle" side and activate the TV camera.
Apollo 11 used slow-scan television (TV) incompatible with broadcast TV, so it was displayed on a special monitor and a conventional TV camera viewed this monitor (thus, a broadcast of a broadcast), significantly reducing the quality of the picture. The signal was received at Goldstone in the United States, but with better fidelity by Honeysuckle Creek Tracking Station near Canberra in Australia. Minutes later the feed was switched to the more sensitive Parkes radio telescope in Australia. Despite some technical and weather difficulties, black and white images of the first lunar EVA were received and broadcast to at least 600 million people on Earth. Copies of this video in broadcast format were saved and are widely available, but recordings of the original slow scan source transmission from the lunar surface were likely destroyed during routine magnetic tape re-use at NASA.
After describing the surface dust as "very fine-grained" and "almost like a powder", at 02:56:15, six and a half hours after landing, Armstrong stepped off "Eagle" landing pad and declared: "That's one small step for [a] man, one giant leap for mankind."
Armstrong intended to say "That's one small step for a man", but the word "a" is not audible in the transmission, and thus was not initially reported by most observers of the live broadcast. When later asked about his quote, Armstrong said he believed he said "for a man", and subsequent printed versions of the quote included the "a" in square brackets. One explanation for the absence may be that his accent caused him to slur the words "for a" together; another is the intermittent nature of the audio and video links to Earth, partly because of storms near Parkes Observatory. A more recent digital analysis of the tape claims to reveal the "a" may have been spoken but obscured by static. Other analysis points to the claims of static and slurring as "face-saving fabrication", and that Armstrong himself later admitted to misspeaking the line.
About seven minutes after stepping onto the Moon's surface, Armstrong collected a contingency soil sample using a sample bag on a stick. He then folded the bag and tucked it into a pocket on his right thigh. This was to guarantee there would be some lunar soil brought back in case an emergency required the astronauts to abandon the EVA and return to the LM. Twelve minutes after the sample was collected, he removed the TV camera from the MESA and made a panoramic sweep, then mounted it on a tripod. The TV camera cable remained partly coiled and presented a tripping hazard throughout the EVA. Still photography was accomplished with a Hasselblad camera that could be operated hand-held or mounted on Armstrong's Apollo space suit. Aldrin joined Armstrong on the surface. He described the view with the simple phrase: "Magnificent desolation."
Armstrong said moving in the lunar gravity, one-sixth of Earth's, was "even perhaps easier than the simulations ... It's absolutely no trouble to walk around." Aldrin joined him on the surface and tested methods for moving around, including two-footed kangaroo hops. The PLSS backpack created a tendency to tip backward, but neither astronaut had serious problems maintaining balance. Loping became the preferred method of movement. The astronauts reported that they needed to plan their movements six or seven steps ahead. The fine soil was quite slippery. Aldrin remarked that moving from sunlight into "Eagle" shadow produced no temperature change inside the suit, but the helmet was warmer in sunlight, so he felt cooler in shadow. The MESA failed to provide a stable work platform and was in shadow, slowing work somewhat. As they worked, the moonwalkers kicked up gray dust, which soiled the outer part of their suits.
The astronauts planted the Lunar Flag Assembly containing a flag of the United States on the lunar surface, in clear view of the TV camera. Aldrin remembered, "Of all the jobs I had to do on the Moon the one I wanted to go the smoothest was the flag raising." But the astronauts struggled with the telescoping rod and could only insert the pole about into the hard lunar surface. Aldrin was afraid it might topple in front of TV viewers, but gave "a crisp West Point salute". Before Aldrin could take a photo of Armstrong with the flag, President Richard Nixon spoke to them through a telephone-radio transmission, which Nixon called "the most historic phone call ever made from the White House." Nixon originally had a long speech prepared to read during the phone call, but Frank Borman, who was at the White House as a NASA liaison during Apollo 11, convinced Nixon to keep his words brief.
They deployed the EASEP, which included a Passive Seismic Experiment Package used to measure moonquakes and a retroreflector array used for the lunar laser ranging experiment. Then Armstrong walked from the LM to take photographs at the rim of Little West Crater while Aldrin collected two core samples. He used the geologist's hammer to pound in the tubes—the only time the hammer was used on Apollo 11—but was unable to penetrate more than deep. The astronauts then collected rock samples using scoops and tongs on extension handles. Many of the surface activities took longer than expected, so they had to stop documenting sample collection halfway through the allotted 34 minutes. Aldrin shoveled of soil into the box of rocks to pack them in tightly. Two types of rocks were found in the geological samples: basalt and breccia. Three new minerals were discovered in the rock samples collected by the astronauts: armalcolite, tranquillityite, and pyroxferroite. Armalcolite was named after Armstrong, Aldrin, and Collins. All have subsequently been found on Earth.
While on the surface, Armstrong uncovered a plaque mounted on the LM ladder, bearing two drawings of Earth (of the Western and Eastern Hemispheres), an inscription, and signatures of the astronauts and President Nixon. The inscription read:
At the behest of the Nixon administration to add a reference to God, NASA included the vague date as a reason to include A.D., which stands for Anno Domini ("in the year of our Lord").
Mission Control used a coded phrase to warn Armstrong his metabolic rates were high, and that he should slow down. He was moving rapidly from task to task as time ran out. As metabolic rates remained generally lower than expected for both astronauts throughout the walk, Mission Control granted the astronauts a 15-minute extension. In a 2010 interview, Armstrong explained that NASA limited the first moonwalk's time and distance because there was no empirical proof of how much cooling water the astronauts' PLSS backpacks would consume to handle their body heat generation while working on the Moon.
Lunar ascent.
Aldrin entered "Eagle" first. With some difficulty the astronauts lifted film and two sample boxes containing of lunar surface material to the LM hatch using a flat cable pulley device called the Lunar Equipment Conveyor (LEC). This proved to be an inefficient tool, and later missions preferred to carry equipment and samples up to the LM by hand. Armstrong reminded Aldrin of a bag of memorial items in his sleeve pocket, and Aldrin tossed the bag down. Armstrong then jumped onto the ladder's third rung, and climbed into the LM. After transferring to LM life support, the explorers lightened the ascent stage for the return to lunar orbit by tossing out their PLSS backpacks, lunar overshoes, an empty Hasselblad camera, and other equipment. The hatch was closed again at 05:11:13. They then pressurized the LM and settled down to sleep.
Presidential speech writer William Safire had prepared an "In Event of Moon Disaster" announcement for Nixon to read in the event the Apollo 11 astronauts were stranded on the Moon. The remarks were in a memo from Safire to Nixon's White House Chief of Staff H. R. Haldeman, in which Safire suggested a protocol the administration might follow in reaction to such a disaster. According to the plan, Mission Control would "close down communications" with the LM, and a clergyman would "commend their souls to the deepest of the deep" in a public ritual likened to burial at sea. The last line of the prepared text contained an allusion to Rupert Brooke's World War I poem "The Soldier". The script for the speech does not make reference to Collins; as he remained onboard "Columbia" in orbit around the Moon, it was expected that he would be able to return the module to Earth in the event of a mission failure.
While moving inside the cabin, Aldrin accidentally damaged the circuit breaker that would arm the main engine for liftoff from the Moon. There was a concern this would prevent firing the engine, stranding them on the Moon. The nonconductive tip of a Duro felt-tip pen was sufficient to activate the switch.
After more than hours on the lunar surface, in addition to the scientific instruments, the astronauts left behind: an Apollo 1 mission patch in memory of astronauts Roger Chaffee, Gus Grissom, and Edward White, who died when their command module caught fire during a test in January 1967; two memorial medals of Soviet cosmonauts Vladimir Komarov and Yuri Gagarin, who died in 1967 and 1968 respectively; a memorial bag containing a gold replica of an olive branch as a traditional symbol of peace; and a silicon message disk carrying the goodwill statements by presidents Eisenhower, Kennedy, Johnson, and Nixon along with messages from leaders of 73 countries around the world. The disk also carries a listing of the leadership of the US Congress, a listing of members of the four committees of the House and Senate responsible for the NASA legislation, and the names of NASA's past and then-current top management.
After about seven hours of rest, the crew was awakened by Houston to prepare for the return flight. 
At that time, unknown to them, some hundred kilometers away from them the Soviet probe Luna 15 was about to descend and impact. Despite having been known to be orbiting the Moon at the same time, through a ground-breaking precautious goodwill exchange of data, the mission control of Luna 15 unexpectedly hastened its robotic sample-return mission, initiating descent, in an attempt to return before Apollo 11. Just two hours before Apollo 11's launch Luna 15 crashed at 15:50 UTC, with British astronomers monitoring Luna 15 and recording the situation one commented: "I say, this has really been drama of the highest order", bringing the Space Race to a culmination.
Roughly two hours later, at 17:54:00 UTC, the Apollo 11 crew on the surface safely lifted off in "Eagle" ascent stage to rejoin Collins aboard "Columbia" in lunar orbit. Film taken from the LM ascent stage upon liftoff from the Moon reveals the American flag, planted some from the descent stage, whipping violently in the exhaust of the ascent stage engine. Aldrin looked up in time to witness the flag topple: "The ascent stage of the LM separated ... I was concentrating on the computers, and Neil was studying the attitude indicator, but I looked up long enough to see the flag fall over." Subsequent Apollo missions planted their flags farther from the LM.
"Columbia" in lunar orbit.
During his day flying solo around the Moon, Collins never felt lonely. Although it has been said "not since Adam has any human known such solitude", Collins felt very much a part of the mission. In his autobiography he wrote: "this venture has been structured for three men, and I consider my third to be as necessary as either of the other two". In the 48 minutes of each orbit when he was out of radio contact with the Earth while "Columbia" passed round the far side of the Moon, the feeling he reported was not fear or loneliness, but rather "awareness, anticipation, satisfaction, confidence, almost exultation".
One of Collins' first tasks was to identify the lunar module on the ground. To give Collins an idea where to look, Mission Control radioed that they believed the lunar module landed about off target. Each time he passed over the suspected lunar landing site, he tried in vain to find the module. On his first orbits on the back side of the Moon, Collins performed maintenance activities such as dumping excess water produced by the fuel cells and preparing the cabin for Armstrong and Aldrin to return.
Just before he reached the dark side on the third orbit, Mission Control informed Collins there was a problem with the temperature of the coolant. If it became too cold, parts of "Columbia" might freeze. Mission Control advised him to assume manual control and implement Environmental Control System Malfunction Procedure 17. Instead, Collins flicked the switch on the system from automatic to manual and back to automatic again, and carried on with normal housekeeping chores, while keeping an eye on the temperature. When "Columbia" came back around to the near side of the Moon again, he was able to report that the problem had been resolved. For the next couple of orbits, he described his time on the back side of the Moon as "relaxing". After Aldrin and Armstrong completed their EVA, Collins slept so he could be rested for the rendezvous. While the flight plan called for "Eagle" to meet up with "Columbia", Collins was prepared for a contingency in which he would fly "Columbia" down to meet "Eagle".
Return.
"Eagle" rendezvoused with "Columbia" at 21:24 UTC on July 21, and the two docked at 21:35. "Eagle"s ascent stage was jettisoned into lunar orbit at 23:41. Just before the Apollo 12 flight, it was noted that "Eagle" was still likely to be orbiting the Moon. Later NASA reports mentioned that "Eagle" orbit had decayed, resulting in it impacting in an "uncertain location" on the lunar surface. In 2021, however, some calculations show that the lander may still be in orbit.
On July 23, the last night before splashdown, the three astronauts made a television broadcast in which Collins commented:
Aldrin added:
Armstrong concluded:
On the return to Earth, a bearing at the Guam tracking station failed, potentially preventing communication on the last segment of the Earth return. A regular repair was not possible in the available time but the station director, Charles Force, had his ten-year-old son Greg use his small hands to reach into the housing and pack it with grease. Greg was later thanked by Armstrong.
Splashdown and quarantine.
The aircraft carrier , under the command of Captain Carl J. Seiberlich, was selected as the primary recovery ship (PRS) for Apollo 11 on June 5, replacing its sister ship, the LPH , which had recovered Apollo 10 on May 26. "Hornet" was then at her home port of Long Beach, California. On reaching Pearl Harbor on July 5, "Hornet" embarked the Sikorsky SH-3 Sea King helicopters of HS-4, a unit which specialized in recovery of Apollo spacecraft, specialized divers of UDT Detachment Apollo, a 35-man NASA recovery team, and about 120 media representatives. To make room, most of "Hornet"s air wing was left behind in Long Beach. Special recovery equipment was also loaded, including a boilerplate command module used for training.
On July 12, with Apollo 11 still on the launch pad, "Hornet" departed Pearl Harbor for the recovery area in the central Pacific, in the vicinity of . A presidential party consisting of Nixon, Borman, Secretary of State William P. Rogers and National Security Advisor Henry Kissinger flew to Johnston Atoll on Air Force One, then to the command ship "USS Arlington" in Marine One. After a night on board, they would fly to "Hornet" in Marine One for a few hours of ceremonies. On arrival aboard "Hornet", the party was greeted by the Commander-in-Chief, Pacific Command (CINCPAC), Admiral John S. McCain Jr., and NASA Administrator Thomas O. Paine, who flew to "Hornet" from Pago Pago in one of "Hornet"s carrier onboard delivery aircraft.
Weather satellites were not yet common, but US Air Force Captain Hank Brandli had access to top-secret spy satellite images. He realized that a storm front was headed for the Apollo recovery area. Poor visibility which could make locating the capsule difficult, and strong upper-level winds which "would have ripped their parachutes to shreds" according to Brandli, posed a serious threat to the safety of the mission. Brandli alerted Navy Captain Willard S. Houston Jr., the commander of the Fleet Weather Center at Pearl Harbor, who had the required security clearance. On their recommendation, Rear Admiral Donald C. Davis, commander of Manned Spaceflight Recovery Forces, Pacific, advised NASA to change the recovery area, each man risking his career. A new location was selected northeast.
This altered the flight plan. A different sequence of computer programs was used, one never before attempted. In a conventional entry, trajectory event P64 was followed by P67. For a skip-out re-entry, P65 and P66 were employed to handle the exit and entry parts of the skip. In this case, because they were extending the re-entry but not actually skipping out, P66 was not invoked and instead, P65 led directly to P67. The crew were also warned they would not be in a full-lift (heads-down) attitude when they entered P67. The first program's acceleration subjected the astronauts to ; the second, to .
Before dawn on July 24, "Hornet" launched four Sea King helicopters and three Grumman E-1 Tracers. Two of the E-1s were designated as "air boss" while the third acted as a communications relay aircraft. Two of the Sea Kings carried divers and recovery equipment. The third carried photographic equipment, and the fourth carried the decontamination swimmer and the flight surgeon. At 16:44 UTC (05:44 local time) "Columbia"s drogue parachutes were deployed. This was observed by the helicopters. Seven minutes later "Columbia" struck the water forcefully east of Wake Island, south of Johnston Atoll, and from "Hornet", at . with seas and winds at from the east were reported under broken clouds at with visibility of at the recovery site. Reconnaissance aircraft flying to the original splashdown location reported the conditions Brandli and Houston had predicted.
During splashdown, "Columbia" landed upside down but was righted within ten minutes by flotation bags activated by the astronauts. A diver from the Navy helicopter hovering above attached a sea anchor to prevent it from drifting. More divers attached flotation collars to stabilize the module and positioned rafts for astronaut extraction.
The divers then passed biological isolation garments (BIGs) to the astronauts, and assisted them into the life raft. The possibility of bringing back pathogens from the lunar surface was considered remote, but NASA took precautions at the recovery site. The astronauts were rubbed down with a sodium hypochlorite solution and "Columbia" wiped with Povidone-iodine to remove any lunar dust that might be present. The astronauts were winched on board the recovery helicopter. BIGs were worn until they reached isolation facilities on board "Hornet". The raft containing decontamination materials was intentionally sunk.
After touchdown on "Hornet" at 17:53 UTC, the helicopter was lowered by the elevator into the hangar bay, where the astronauts walked the to the Mobile quarantine facility (MQF), where they would begin the Earth-based portion of their 21 days of quarantine. This practice would continue for two more Apollo missions, Apollo 12 and Apollo 14, before the Moon was proven to be barren of life, and the quarantine process dropped. Nixon welcomed the astronauts back to Earth. He told them: "[A]s a result of what you've done, the world has never been closer together before."
After Nixon departed, "Hornet" was brought alongside the "Columbia", which was lifted aboard by the ship's crane, placed on a dolly and moved next to the MQF. It was then attached to the MQF with a flexible tunnel, allowing the lunar samples, film, data tapes and other items to be removed. "Hornet" returned to Pearl Harbor, where the MQF was loaded onto a Lockheed C-141 Starlifter and airlifted to the Manned Spacecraft Center. The astronauts arrived at the Lunar Receiving Laboratory at 10:00 UTC on July 28. "Columbia" was taken to Ford Island for deactivation, and its pyrotechnics made safe. It was then taken to Hickham Air Force Base, from whence it was flown to Houston in a Douglas C-133 Cargomaster, reaching the Lunar Receiving Laboratory on July 30.
In accordance with the Extra-Terrestrial Exposure Law, a set of regulations promulgated by NASA on July 16 to codify its quarantine protocol, the astronauts continued in quarantine. After three weeks in confinement (first in the Apollo spacecraft, then in their trailer on "Hornet", and finally in the Lunar Receiving Laboratory), the astronauts were given a clean bill of health. On August 10, 1969, the Interagency Committee on Back Contamination met in Atlanta and lifted the quarantine on the astronauts, on those who had joined them in quarantine (NASA physician William Carpentier and MQF project engineer John Hirasaki), and on "Columbia" itself. Loose equipment from the spacecraft remained in isolation until the lunar samples were released for study.
Celebrations.
On August 13, the three astronauts rode in ticker-tape parades in their honor in New York and Chicago, with an estimated six million attendees. On the same evening in Los Angeles there was an official state dinner to celebrate the flight, attended by members of Congress, 44 governors, Chief Justice of the United States Warren E. Burger and his predecessor, Earl Warren, and ambassadors from 83 nations at the Century Plaza Hotel. Nixon and Agnew honored each astronaut with a presentation of the Presidential Medal of Freedom.
The three astronauts spoke before a joint session of Congress on September 16, 1969. They presented two US flags, one to the House of Representatives and the other to the Senate, that they had carried with them to the surface of the Moon. The flag of American Samoa on Apollo 11 is on display at the Jean P. Haydon Museum in Pago Pago, the capital of American Samoa.
This celebration began a 38-day world tour that brought the astronauts to 22 countries and included visits with many world leaders. The crew toured from September 29 to November 5. The world tour started in Mexico City and ended in Tokyo. Stops on the tour in order were: Mexico City, Bogota, Buenos Aires, Rio de Janeiro, Las Palmas in the Canary Islands, Madrid, Paris, Amsterdam, Brussels, Oslo, Cologne, Berlin, London, Rome, Belgrade, Ankara, Kinshasa, Tehran, Mumbai, Dhaka, Bangkok, Darwin, Sydney, Guam, Seoul, Tokyo and Honolulu.
Many nations honored the first human Moon landing with special features in magazines or by issuing Apollo 11 commemorative postage stamps or coins.
Legacy.
Cultural significance.
Humans walking on the Moon and returning safely to Earth accomplished Kennedy's goal set eight years earlier. In Mission Control during the Apollo 11 landing, Kennedy's speech flashed on the screen, followed by the words "TASK ACCOMPLISHED, July 1969". The success of Apollo 11 demonstrated the United States' technological superiority; and with the success of Apollo 11, America had won the Space Race.
New phrases permeated into the English language. "If they can send a man to the Moon, why can't they ...?" became a common saying following Apollo 11. Armstrong's words on the lunar surface also spun off various parodies.
While most people celebrated the accomplishment, disenfranchised Americans saw it as a symbol of the divide in America, evidenced by protesters led by Ralph Abernathy outside of Kennedy Space Center the day before Apollo 11 launched. NASA Administrator Thomas Paine met with Abernathy at the occasion, both hoping that the space program can spur progress also in other regards, such as poverty in the US. Paine was then asked, and agreed, to host protesters as spectators at the launch, and Abernathy, awestruck by the spectacle, prayed for the astronauts. Racial and financial inequalities frustrated citizens who wondered why money spent on the Apollo program was not spent taking care of humans on Earth. A poem by Gil Scott-Heron called "Whitey on the Moon" (1970) illustrated the racial inequality in the United States that was highlighted by the Space Race. The poem starts with:
Twenty percent of the world's population watched humans walk on the Moon for the first time. While Apollo 11 sparked the interest of the world, the follow-on Apollo missions did not hold the interest of the nation. One possible explanation was the shift in complexity. Landing someone on the Moon was an easy goal to understand; lunar geology was too abstract for the average person. Another is that Kennedy's goal of landing humans on the Moon had already been accomplished. A well-defined objective helped Project Apollo accomplish its goal, but after it was completed it was hard to justify continuing the lunar missions.
While most Americans were proud of their nation's achievements in space exploration, only once during the late 1960s did the Gallup Poll indicate that a majority of Americans favored "doing more" in space as opposed to "doing less". By 1973, 59 percent of those polled favored cutting spending on space exploration. The Space Race had been won, and Cold War tensions were easing as the US and Soviet Union entered the era of détente. This was also a time when inflation was rising, which put pressure on the government to reduce spending. What saved the space program was that it was one of the few government programs that had achieved something great. Drastic cuts, warned Caspar Weinberger, the deputy director of the Office of Management and Budget, might send a signal that "our best years are behind us".
After the Apollo 11 mission, officials from the Soviet Union said landing humans on the Moon was dangerous and unnecessary. At the time the Soviet Union was attempting to retrieve lunar samples robotically. The Soviets publicly denied there was a race to the Moon, and indicated they were not making an attempt. Mstislav Keldysh said in July 1969, "We are concentrating wholly on the creation of large satellite systems." It was revealed in 1989 that the Soviets had tried to send people to the Moon, but were unable due to technological difficulties. The public's reaction in the Soviet Union was mixed. The Soviet government limited the release of information about the lunar landing, which affected the reaction. A portion of the populace did not give it any attention, and another portion was angered by it.
The Apollo 11 landing is referenced in the songs "Armstrong, Aldrin and Collins" by the Byrds on the 1969 album "Ballad of Easy Rider", "Coon on the Moon" by Howlin' Wolf on the 1973 album "The Back Door Wolf", and " by Ayreon on the 2000 album ".
Spacecraft.
The command module "Columbia" went on a tour of the United States, visiting 49 state capitals, the District of Columbia, and Anchorage, Alaska. In 1971, it was transferred to the Smithsonian Institution, and was displayed at the National Air and Space Museum (NASM) in Washington, DC. It was in the central "Milestones of Flight" exhibition hall in front of the Jefferson Drive entrance, sharing the main hall with other pioneering flight vehicles such as the "Wright Flyer", "Spirit of St. Louis", Bell X-1, North American X-15 and "Friendship 7".
"Columbia" was moved in 2017 to the NASM Mary Baker Engen Restoration Hangar at the Steven F. Udvar-Hazy Center in Chantilly, Virginia, to be readied for a four-city tour titled "Destination Moon: The Apollo 11 Mission". This included Space Center Houston from October 14, 2017, to March 18, 2018, the Saint Louis Science Center from April 14 to September 3, 2018, the Senator John Heinz History Center in Pittsburgh from September 29, 2018, to February 18, 2019, and its last location at Museum of Flight in Seattle from March 16 to September 2, 2019. Continued renovations at the Smithsonian allowed time for an additional stop for the capsule, and it was moved to the Cincinnati Museum Center. The ribbon cutting ceremony was on September 29, 2019.
For 40 years Armstrong's and Aldrin's space suits were displayed in the museum's "Apollo to the Moon" exhibit, until it permanently closed on December 3, 2018, to be replaced by a new gallery which was scheduled to open in 2022. A special display of Armstrong's suit was unveiled for the 50th anniversary of Apollo 11 in July 2019. The quarantine trailer, the flotation collar and the flotation bags are in the Smithsonian's Steven F. Udvar-Hazy Center annex near Washington Dulles International Airport in Chantilly, Virginia, where they are on display along with a test lunar module.
The descent stage of the LM "Eagle" remains on the Moon. In 2009, the Lunar Reconnaissance Orbiter (LRO) imaged the various Apollo landing sites on the surface of the Moon, for the first time with sufficient resolution to see the descent stages of the lunar modules, scientific instruments, and foot trails made by the astronauts. 
The remains of the ascent stage are assumed to lie at an unknown location on the lunar surface. The ascent stage, "Eagle", was not tracked after it was jettisoned. The lunar gravity field is sufficiently non-uniform to make low Moon orbits unstable after a short time, leading the orbiting object to impact the surface. However, using a program developed by NASA, and high-resolution lunar gravity data, a paper was published, in 2021, indicating that "Eagle" might still be in orbit as late as 2020. Using the orbital elements published by NASA, a Monte Carlo method was used to generate parameter sets that bracket the uncertainties in these elements. All simulations, of the orbit, predicted that "Eagle" would never impact the lunar surface.
In March 2012 a team of specialists financed by Amazon founder Jeff Bezos located the F-1 engines from the S-IC stage that launched Apollo 11 into space. They were found on the Atlantic seabed using advanced sonar scanning. His team brought parts of two of the five engines to the surface. In July 2013, a conservator discovered a serial number under the rust on one of the engines raised from the Atlantic, which NASA confirmed was from Apollo 11. The S-IVB third stage which performed Apollo 11's trans-lunar injection remains in a solar orbit near to that of Earth.
Moon rocks.
The main repository for the Apollo Moon rocks is the Lunar Sample Laboratory Facility at the Lyndon B. Johnson Space Center in Houston, Texas. For safekeeping, there is also a smaller collection stored at White Sands Test Facility near Las Cruces, New Mexico. Most of the rocks are stored in nitrogen to keep them free of moisture. They are handled only indirectly, using special tools. Over 100 research laboratories worldwide conduct studies of the samples; approximately 500 samples are prepared and sent to investigators every year.
In November 1969, Nixon asked NASA to make up about 250 presentation Apollo 11 lunar sample displays for 135 nations, the fifty states of the United States and its possessions, and the United Nations. Each display included Moon dust from Apollo 11 and flags, including one of the Soviet Union, taken along by Apollo 11. The rice-sized particles were four small pieces of Moon soil weighing about 50 mg and were enveloped in a clear acrylic button about as big as a United States half-dollar coin. This acrylic button magnified the grains of lunar dust. Nixon gave the Apollo 11 lunar sample displays as goodwill gifts in 1970.
Experiment results.
The Passive Seismic Experiment ran until the command uplink failed on August 25, 1969. The downlink failed on December 14, 1969. , the Lunar Laser Ranging experiment remains operational.
Moonwalk camera.
The Hasselblad camera used during the moonwalk was thought to be lost or left on the Moon surface.
LM memorabilia.
In 2015, after Armstrong died in 2012, his widow contacted the National Air and Space Museum to inform them she had found a white cloth bag in one of Armstrong's closets. The bag contained various items, which should have been left behind in the lunar module, including the 16mm Data Acquisition Camera that had been used to capture images of the first Moon landing. The camera is currently on display at the National Air and Space Museum.
Anniversary events.
40th anniversary.
On July 15, 2009, Life.com released a photo gallery of previously unpublished photos of the astronauts taken by "Life" photographer Ralph Morse prior to the Apollo 11 launch. From July 16 to 24, 2009, NASA streamed the original mission audio on its website in real time 40 years to the minute after the events occurred. It is in the process of restoring the video footage and has released a preview of key moments. In July 2010, air-to-ground voice recordings and film footage shot in Mission Control during the Apollo 11 powered descent and landing was re-synchronized and released for the first time. The John F. Kennedy Presidential Library and Museum set up an Adobe Flash website that rebroadcasts the transmissions of Apollo 11 from launch to landing on the Moon.
On July 20, 2009, Armstrong, Aldrin, and Collins met with US President Barack Obama at the White House. "We expect that there is, as we speak, another generation of kids out there who are looking up at the sky and are going to be the next Armstrong, Collins, and Aldrin", Obama said. "We want to make sure that NASA is going to be there for them when they want to take their journey." On August 7, 2009, an act of Congress awarded the three astronauts a Congressional Gold Medal, the highest civilian award in the United States. The bill was sponsored by Florida Senator Bill Nelson and Florida Representative Alan Grayson.
A group of British scientists interviewed as part of the anniversary events reflected on the significance of the Moon landing:
50th anniversary.
On June 10, 2015, Congressman Bill Posey introduced resolution H.R. 2726 to the 114th session of the United States House of Representatives directing the United States Mint to design and sell commemorative coins in gold, silver and clad for the 50th anniversary of the Apollo 11 mission. On January 24, 2019, the Mint released the Apollo 11 Fiftieth Anniversary commemorative coins to the public on its website.
A documentary film, "Apollo 11", with restored footage of the 1969 event, premiered in IMAX on March 1, 2019, and broadly in theaters on March 8.
The Smithsonian Institute's National Air and Space Museum and NASA sponsored the "Apollo 50 Festival" on the National Mall in Washington DC. The three-day (July 18 to 20, 2019) outdoor festival featured hands-on exhibits and activities, live performances, and speakers such as Adam Savage and NASA scientists.
As part of the festival, a projection of the tall Saturn V rocket was displayed on the east face of the tall Washington Monument from July 16 through the 20th from 9:30 pm until 11:30 pm (EDT). The program also included a 17-minute show that combined full-motion video projected on the Washington Monument to recreate the assembly and launch of the Saturn V rocket. The projection was joined by a wide recreation of the Kennedy Space Center countdown clock and two large video screens showing archival footage to recreate the time leading up to the moon landing. There were three shows per night on July 19–20, with the last show on Saturday, delayed slightly so the portion where Armstrong first set foot on the Moon would happen exactly 50 years to the second after the actual event.
On July 19, 2019, the Google Doodle paid tribute to the Apollo 11 Moon Landing, complete with a link to an animated YouTube video with voiceover by astronaut Michael Collins.
Aldrin, Collins, and Armstrong's sons were hosted by President Donald Trump in the Oval Office.
References.
Citations.
In some of the following sources, times are shown in the format "hours:minutes:seconds" (e.g. 109:24:15), referring to the mission's Ground Elapsed Time (GET), based on the official launch time of July 16, 1969, 13:32:00 UTC (000:00:00 GET).

</doc>
<doc id="663" url="?curid=663" title="Apollo 8">
Apollo 8

Apollo 8 (December 21–27, 1968) was the first crewed spacecraft to leave Earth's gravitational sphere of influence, and the first human spaceflight to reach the Moon. The crew orbited the Moon ten times without landing, and then departed safely back to Earth. These three astronauts—Frank Borman, James Lovell, and William Anders—were the first humans to witness and photograph the far side of the Moon and an Earthrise.
Apollo 8 launched on December 21, 1968, and was the second crewed spaceflight mission flown in the United States Apollo space program (the first, Apollo7, stayed in Earth orbit). Apollo8 was the third flight and the first crewed launch of the Saturn V rocket, and was the first human spaceflight from the Kennedy Space Center, located adjacent to Cape Kennedy Air Force Station in Florida.
Originally planned as the second crewed Apollo Lunar Module and command module test, to be flown in an elliptical medium Earth orbit in early 1969, the mission profile was changed in August 1968 to a more ambitious command-module-only lunar orbital flight to be flown in December, as the lunar module was not yet ready to make its first flight. Astronaut Jim McDivitt's crew, who were training to fly the first lunar module flight in low Earth orbit, became the crew for the Apollo9 mission, and Borman's crew were moved to the Apollo8 mission. This left Borman's crew with two to three months' less training and preparation time than originally planned, and replaced the planned lunar module training with translunar navigation training.
Apollo 8 took 68 hours to travel the distance to the Moon. The crew orbited the Moon ten times over the course of twenty hours, during which they made a Christmas Eve television broadcast in which they read the first ten verses from the Book of Genesis. At the time, the broadcast was the most watched TV program ever. Apollo8's successful mission paved the way for Apollo 10 and, with Apollo11 in July 1969, the fulfillment of U.S. president John F. Kennedy's goal of landing a man on the Moon before the end of the decade. The Apollo8 astronauts returned to Earth on December 27, 1968, when their spacecraft splashed down in the northern Pacific Ocean. The crew members were named "Time" magazine's "Men of the Year" for 1968 upon their return.
Background.
In the late 1950s and early 1960s, the United States was engaged in the Cold War, a geopolitical rivalry with the Soviet Union. On October 4, 1957, the Soviet Union launched Sputnik 1, the first artificial satellite. This unexpected success stoked fears and imaginations around the world. It not only demonstrated that the Soviet Union had the capability to deliver nuclear weapons over intercontinental distances, it challenged American claims of military, economic, and technological superiority. The launch precipitated the Sputnik crisis and triggered the Space Race.
President John F. Kennedy believed that not only was it in the national interest of the United States to be superior to other nations, but that the perception of American power was at least as important as the actuality. It was therefore intolerable to him for the Soviet Union to be more advanced in the field of space exploration. He was determined that the United States should compete, and sought a challenge that maximized its chances of winning.
The Soviet Union had heavier-lifting carrier rockets, which meant Kennedy needed to choose a goal that was beyond the capacity of the existing generation of rocketry, one where the US and Soviet Union would be starting from a position of equality—something spectacular, even if it could not be justified on military, economic, or scientific grounds. After consulting with his experts and advisors, he chose such a project: to land a man on the Moon and return him to the Earth. This project already had a name: Project Apollo.
An early and crucial decision was the adoption of lunar orbit rendezvous, under which a specialized spacecraft would land on the lunar surface. The Apollo spacecraft therefore had three primary components: a command module (CM) with a cabin for the three astronauts, and the only part that would return to Earth; a service module (SM) to provide the command module with propulsion, electrical power, oxygen, and water; and a two-stage lunar module (LM), which comprised a descent stage for landing on the Moon and an ascent stage to return the astronauts to lunar orbit. This configuration could be launched by the Saturn V rocket that was then under development.
Framework.
Prime crew.
The initial crew assignment of Frank Borman as Commander, Michael Collins as Command Module Pilot (CMP) and William Anders as Lunar Module Pilot (LMP) for the third crewed Apollo flight was officially announced on November 20, 1967. Collins was replaced by Jim Lovell in July 1968, after suffering a cervical disc herniation that required surgery to repair. This crew was unique among pre-Space Shuttle era missions in that the commander was not the most experienced member of the crew: Lovell had flown twice before, on Gemini VII and Gemini XII. This would also be the first case of a commander of a previous mission (Lovell, Gemini XII) flying as a non-commander. This was also the first mission to reunite crewmates from a previous mission (Lovell and Borman, Gemini VII).
As of June 2024, James Lovell is the last surviving Apollo 8 astronaut. Frank Borman and William Anders died on November 7, 2023, and on June 7, 2024, respectively.
Backup crew.
The backup crew assignment of Neil Armstrong as Commander, Lovell as CMP, and Buzz Aldrin as LMP for the third crewed Apollo flight was officially announced at the same time as the prime crew. When Lovell was reassigned to the prime crew, Aldrin was moved to CMP, and Fred Haise was brought in as backup LMP. Armstrong would later command Apollo11, with Aldrin as LMP and Collins as CMP. Haise served on the backup crew of Apollo11 as LMP and flew on Apollo13 as LMP.
Support personnel.
During Projects Mercury and Gemini, each mission had a prime and a backup crew. For Apollo, a third crew of astronauts was added, known as the support crew. The support crew maintained the flight plan, checklists, and mission ground rules, and ensured that the prime and backup crews were apprised of any changes. The support crew developed procedures in the simulators, especially those for emergency situations, so that the prime and backup crews could practice and master them in their simulator training. For Apollo8, the support crew consisted of Ken Mattingly, Vance Brand, and Gerald Carr.
The capsule communicator (CAPCOM) was an astronaut at the Mission Control Center in Houston, Texas, who was the only person who communicated directly with the flight crew. For Apollo8, the CAPCOMs were Michael Collins, Gerald Carr, Ken Mattingly, Neil Armstrong, Buzz Aldrin, Vance Brand, and Fred Haise.
The mission control teams rotated in three shifts, each led by a flight director. The directors for Apollo8 were Clifford E. Charlesworth (Green team), Glynn Lunney (Black team), and Milton Windler (Maroon team).
Mission insignia and callsign.
The triangular shape of the insignia refers to the shape of the Apollo CM. It shows a red figure8 looping around the Earth and Moon to reflect both the mission number and the circumlunar nature of the mission. On the bottom of the8 are the names of the three astronauts. The initial design of the insignia was developed by Jim Lovell, who reportedly sketched it while riding in the back seat of a T-38 flight from California to Houston shortly after learning of Apollo8's re-designation as a lunar-orbital mission.
The crew wanted to name their spacecraft, but NASA did not allow it. The crew would have likely chosen "Columbiad", the name of the giant cannon that launches a space vehicle in Jules Verne's 1865 novel "From the Earth to the Moon". The Apollo11 CM was named "Columbia" in part for that reason.
Preparations.
Mission schedule.
On September 20, 1967, NASA adopted a seven-step plan for Apollo missions, with the final step being a Moon landing. Apollo4 and Apollo6 were "A" missions, tests of the SaturnV launch vehicle using an uncrewed Block I production model of the command and service module (CSM) in Earth orbit. Apollo5 was a "B" mission, a test of the LM in Earth orbit. Apollo7, scheduled for October 1968, would be a "C" mission, a crewed Earth-orbit flight of the CSM. Further missions depended on the readiness of the LM. It had been decided as early as May 1967 that there would be at least four additional missions. Apollo8 was planned as the "D" mission, a test of the LM in a low Earth orbit in December 1968 by James McDivitt, David Scott, and Russell Schweickart, while Borman's crew would fly the "E" mission, a more rigorous LM test in an elliptical medium Earth orbit as Apollo9, in early 1969. The "F" Mission would test the CSM and LM in lunar orbit, and the "G" mission would be the finale, the Moon landing.
Production of the LM fell behind schedule, and when Apollo8's LM-3 arrived at the Kennedy Space Center (KSC) in June 1968, more than a hundred significant defects were discovered, leading Bob Gilruth, the director of the Manned Spacecraft Center (MSC), and others to conclude that there was no prospect of LM-3 being ready to fly in 1968. Indeed, it was possible that delivery would slip to February or March 1969. Following the original seven-step plan would have meant delaying the "D" and subsequent missions, and endangering the program's goal of a lunar landing before the end of 1969. George Low, the Manager of the Apollo Spacecraft Program Office, proposed a solution in August 1968 to keep the program on track despite the LM delay. Since the next CSM (designated as "CSM-103") would be ready three months before LM-3, a CSM-only mission could be flown in December 1968. Instead of repeating the "C" mission flight of Apollo7, this CSM could be sent all the way to the Moon, with the possibility of entering a lunar orbit and returning to Earth. The new mission would also allow NASA to test lunar landing procedures that would otherwise have had to wait until Apollo10, the scheduled "F" mission. This also meant that the medium Earth orbit "E" mission could be dispensed with. The net result was that only the "D" mission had to be delayed, and the plan for lunar landing in mid-1969 could remain on timeline.
On August 9, 1968, Low discussed the idea with Gilruth, Flight Director Chris Kraft, and the Director of Flight Crew Operations, Donald Slayton. They then flew to the Marshall Space Flight Center (MSFC) in Huntsville, Alabama, where they met with KSC Director Kurt Debus, Apollo Program Director Samuel C. Phillips, Rocco Petrone, and Wernher von Braun. Jerry Wittenstein, deputy chief of flight mechanics, presented trajectories for the new mission. Kraft considered the proposal feasible from a flight control standpoint; Debus and Petrone agreed that the next Saturn V, AS-503, could be made ready by December 1; and von Braun was confident the pogo oscillation problems that had afflicted Apollo6 had been fixed. Almost every senior manager at NASA agreed with this new mission, citing confidence in both the hardware and the personnel, along with the potential for a circumlunar flight providing a significant morale boost. The only person who needed some convincing was James E. Webb, the NASA administrator. Backed by the full support of his agency, Webb authorized the mission. Apollo8 was officially changed from a "D" mission to a "C-Prime" lunar-orbit mission.
With the change in mission for Apollo 8, Slayton asked McDivitt if he still wanted to fly it. McDivitt turned it down; his crew had spent a great deal of time preparing to test the LM, and that was what he still wanted to do. Slayton then decided to swap the prime and backup crews of the Dand Emissions. This swap also meant a swap of spacecraft, requiring Borman's crew to use CSM-103, while McDivitt's crew would use CSM-104, since CM-104 could not be made ready by December. David Scott was not happy about giving up CM-103, the testing of which he had closely supervised, for CM-104, although the two were almost identical, and Anders was less than enthusiastic about being an LMP on a flight with no LM. Instead, Apollo8 would carry the LM test article, a boilerplate model that would simulate the correct weight and balance of LM-3.
Added pressure on the Apollo program to make its 1969 landing goal was provided by the Soviet Union's Zond5 mission, which flew some living creatures, including Russian tortoises, in a cislunar loop around the Moon and returned them to Earth on September 21. There was speculation within NASA and the press that they might be preparing to launch cosmonauts on a similar circumlunar mission before the end of 1968. Compounding these concerns, American reconnaissance satellites observed a mockup N1 being rolled to the pad at Baikonur on November 25, 1967.
The Apollo 8 crew, now living in the crew quarters at Kennedy Space Center, received a visit from Charles Lindbergh and his wife, Anne Morrow Lindbergh, the night before the launch. They talked about how, before his 1927 flight, Lindbergh had used a piece of string to measure the distance from New York City to Paris on a globe and from that calculated the fuel needed for the flight. The total he had carried was a tenth of the amount that the Saturn V would burn every second. The next day, the Lindberghs watched the launch of Apollo8 from a nearby dune.
Saturn V redesign.
The Saturn V rocket used by Apollo8 was designated AS-503, or the "03rd" model of the SaturnV ("5") rocket to be used in the Apollo-Saturn ("AS") program. When it was erected in the Vehicle Assembly Building on December 20, 1967, it was thought that the rocket would be used for an uncrewed Earth-orbit test flight carrying a boilerplate command and service module. Apollo6 had suffered several major problems during its April 1968 flight, including severe pogo oscillation during its first stage, two second-stage engine failures, and a third stage that failed to reignite in orbit. Without assurances that these problems had been rectified, NASA administrators could not justify risking a crewed mission until additional uncrewed test flights proved the Saturn V was ready.
Teams from the MSFC went to work on the problems. Of primary concern was the pogo oscillation, which would not only hamper engine performance, but could exert significant g-forces on a crew. A task force of contractors, NASA agency representatives, and MSFC researchers concluded that the engines vibrated at a frequency similar to the frequency at which the spacecraft itself vibrated, causing a resonance effect that induced oscillations in the rocket. A system that used helium gas to absorb some of these vibrations was installed.
Of equal importance was the failure of three engines during flight. Researchers quickly determined that a leaking hydrogen fuel line ruptured when exposed to vacuum, causing a loss of fuel pressure in engine two. When an automatic shutoff attempted to close the liquid hydrogen valve and shut down engine two, it had accidentally shut down engine three's liquid oxygen due to a miswired connection. As a result, engine three failed within one second of engine two's shutdown. Further investigation revealed the same problem for the third-stage engine—a faulty igniter line. The team modified the igniter lines and fuel conduits, hoping to avoid similar problems on future launches.
The teams tested their solutions in August 1968 at the MSFC. A Saturn stage IC was equipped with shock-absorbing devices to demonstrate the team's solution to the problem of pogo oscillation, while a Saturn Stage II was retrofitted with modified fuel lines to demonstrate their resistance to leaks and ruptures in vacuum conditions. Once NASA administrators were convinced that the problems had been solved, they gave their approval for a crewed mission using AS-503.
The Apollo 8 spacecraft was placed on top of the rocket on September 21, and the rocket made the slow journey to the launch pad atop one of NASA's two massive crawler-transporters on October9. Testing continued all through December until the day before launch, including various levels of readiness testing from December5 through 11. Final testing of modifications to address the problems of pogo oscillation, ruptured fuel lines, and bad igniter lines took place on December 18, three days before the scheduled launch.
Mission.
Parameter summary.
As the first crewed spacecraft to orbit more than one celestial body, Apollo8's profile had two different sets of orbital parameters, separated by a translunar injection maneuver. Apollo lunar missions would begin with a nominal circular Earth parking orbit. Apollo8 was launched into an initial orbit with an apogee of and a perigee of , with an inclination of 32.51° to the Equator, and an orbital period of 88.19 minutes. Propellant venting increased the apogee by over the 2hours, 44 minutes, and 30 seconds spent in the parking orbit.
This was followed by a trans-lunar injection (TLI) burn of the S-IVB third stage for 318 seconds, accelerating the command and service module and LM test article from an orbital velocity of to the injection velocity of which set a record for the highest speed, relative to Earth, that humans had ever traveled. This speed was slightly less than the Earth's escape velocity of , but put Apollo8 into an elongated elliptical Earth orbit, close enough to the Moon to be captured by the Moon's gravity.
The standard lunar orbit for Apollo missions was planned as a nominal circular orbit above the Moon's surface. Initial lunar orbit insertion was an ellipse with a perilune of and an apolune of , at an inclination of 12° from the lunar equator. This was then circularized at , with an orbital period of 128.7 minutes. The effect of lunar mass concentrations ("mascons") on the orbit was found to be greater than initially predicted; over the course of the ten lunar orbits lasting twenty hours, the orbital distance was perturbated to .
Apollo 8 achieved a maximum distance from Earth of .
Launch and trans-lunar injection.
Apollo 8 was launched at 12:51:00 UTC (07:51:00 Eastern Standard Time) on December 21, 1968, using the Saturn V's three stages to achieve Earth orbit. The S-IC first stage landed in the Atlantic Ocean at , and the S-II second stage landed at . The S-IVB third stage injected the craft into Earth orbit and remained attached to perform the TLI burn that would put the spacecraft on a trajectory to the Moon.
Once the vehicle reached Earth orbit, both the crew and Houston flight controllers spent the next 2hours and 38 minutes checking that the spacecraft was in proper working order and ready for TLI. The proper operation of the S-IVB third stage of the rocket was crucial, and in the last uncrewed test, it had failed to reignite for this burn. Collins was the first CAPCOM on duty, and at 2hours, 27 minutes and 22 seconds after launch he radioed, "Apollo8. You are Go for TLI." This communication meant that Mission Control had given official permission for Apollo8 to go to the Moon. The S-IVB engine ignited on time and performed the TLI burn perfectly. Over the next five minutes, the spacecraft's speed increased from .
After the S-IVB had placed the mission on course for the Moon, the command and service modules (CSM), the remaining Apollo8 spacecraft, separated from it. The crew then rotated the spacecraft to take photographs of the spent stage and then practiced flying in formation with it. As the crew rotated the spacecraft, they had their first views of the Earth as they moved away from it—this marked the first time humans had viewed the whole Earth at once. Borman became worried that the S-IVB was staying too close to the CSM and suggested to Mission Control that the crew perform a separation maneuver. Mission Control first suggested pointing the spacecraft towards Earth and using the small reaction control system (RCS) thrusters on the service module (SM) to add to their velocity away from the Earth, but Borman did not want to lose sight of the S-IVB. After discussion, the crew and Mission Control decided to burn in the Earth direction to increase speed, but at instead. The time needed to prepare and perform the additional burn put the crew an hour behind their onboard tasks.
Five hours after launch, Mission Control sent a command to the S-IVB to vent its remaining fuel, changing its trajectory. The S-IVB, with the test article attached, posed no further hazard to Apollo8, passing the orbit of the Moon and going into a solar orbit with an inclination of 23.47° from the Earth's equatorial plane, and an orbital period of 340.80 days. It became a , and will continue to orbit the Sun for many years, if not retrieved.
The Apollo 8 crew were the first humans to pass through the Van Allen radiation belts, which extend up to from Earth. Scientists predicted that passing through the belts quickly at the spacecraft's high speed would cause a radiation dosage of no more than a chest X-ray, or 1milligray (mGy; during a year, the average human receives a dose of 2to 3mGy from background radiation). To record the actual radiation dosages, each crew member wore a Personal Radiation Dosimeter that transmitted data to Earth, as well as three passive film dosimeters that showed the cumulative radiation experienced by the crew. By the end of the mission, the crew members experienced an average radiation dose of 1.6 mGy.
Lunar trajectory.
Lovell's main job as Command Module Pilot was as navigator. Although Mission Control normally performed all the actual navigation calculations, it was necessary to have a crew member adept at navigation so that the crew could return to Earth in case communication with Mission Control was lost. Lovell navigated by star sightings using a sextant built into the spacecraft, measuring the angle between a star and the Earth's (or the Moon's) horizon. This task was made difficult by a large cloud of debris around the spacecraft, which made it hard to distinguish the stars.
By seven hours into the mission, the crew was about 1hour and 40 minutes behind flight plan because of the problems in moving away from the S-IVB and Lovell's obscured star sightings. The crew placed the spacecraft into Passive Thermal Control (PTC), also called "barbecue roll", in which the spacecraft rotated about once per hour around its long axis to ensure even heat distribution across the surface of the spacecraft. In direct sunlight, parts of the spacecraft's outer surface could be heated to over , while the parts in shadow would be . These temperatures could cause the heat shield to crack and propellant lines to burst. Because it was impossible to get a perfect roll, the spacecraft swept out a cone as it rotated. The crew had to make minor adjustments every half hour as the cone pattern got larger and larger.
The first mid-course correction came eleven hours into the flight. The crew had been awake for more than 16 hours. Before launch, NASA had decided at least one crew member should be awake at all times to deal with problems that might arise. Borman started the first sleep shift but found sleeping difficult because of the constant radio chatter and mechanical noises. Testing on the ground had shown that the service propulsion system (SPS) engine had a small chance of exploding when burned for long periods unless its combustion chamber was "coated" first by burning the engine for a short period. This first correction burn was only 2.4 seconds and added about velocity prograde (in the direction of travel). This change was less than the planned , because of a bubble of helium in the oxidizer lines, which caused unexpectedly low propellant pressure. The crew had to use the small RCS thrusters to make up the shortfall. Two later planned mid-course corrections were canceled because the Apollo8 trajectory was found to be perfect.
About an hour after starting his sleep shift, Borman obtained permission from ground control to take a Seconal sleeping pill. The pill had little effect. Borman eventually fell asleep, and then awoke feeling ill. He vomited twice and had a bout of diarrhea; this left the spacecraft full of small globules of vomit and feces, which the crew cleaned up as well as they could. Borman initially did not want everyone to know about his medical problems, but Lovell and Anders wanted to inform Mission Control. The crew decided to use the Data Storage Equipment (DSE), which could tape voice recordings and telemetry and dump them to Mission Control at high speed. After recording a description of Borman's illness they asked Mission Control to check the recording, stating that they "would like an evaluation of the voice comments".
The Apollo 8 crew and Mission Control medical personnel held a conference using an unoccupied second-floor control room (there were two identical control rooms in Houston, on the second and third floors, only one of which was used during a mission). The conference participants concluded that there was little to worry about and that Borman's illness was either a 24-hour flu, as Borman thought, or a reaction to the sleeping pill. Researchers now believe that he was suffering from space adaptation syndrome, which affects about a third of astronauts during their first day in space as their vestibular system adapts to weightlessness. Space adaptation syndrome had not occurred on previous spacecraft (Mercury and Gemini), because those astronauts could not move freely in the small cabins of those spacecraft. The increased cabin space in the Apollo command module afforded astronauts greater freedom of movement, contributing to symptoms of space sickness for Borman and, later, astronaut Rusty Schweickart during Apollo9.
The cruise phase was a relatively uneventful part of the flight, except for the crew's checking that the spacecraft was in working order and that they were on course. During this time, NASA scheduled a television broadcast at 31 hours after launch. The Apollo8 crew used a camera that broadcast in black-and-white only, using a Vidicon tube. The camera had two lenses, a very wide-angle (160°) lens, and a telephoto (9°) lens.
During this first broadcast, the crew gave a tour of the spacecraft and attempted to show how the Earth appeared from space. However, difficulties aiming the narrow-angle lens without the aid of a monitor to show what it was looking at made showing the Earth impossible. Additionally, without proper filters, the Earth image became saturated by any bright source. In the end, all the crew could show the people watching back on Earth was a bright blob. After broadcasting for 17 minutes, the rotation of the spacecraft took the high-gain antenna out of view of the receiving stations on Earth and they ended the transmission with Lovell wishing his mother a happy birthday.
By this time, the crew had completely abandoned the planned sleep shifts. Lovell went to sleep hours into the flight – three-and-a-half hours before he had planned to. A short while later, Anders also went to sleep after taking a sleeping pill. The crew was unable to see the Moon for much of the outward cruise. Two factors made the Moon almost impossible to see from inside the spacecraft: three of the five windows fogging up due to out-gassed oils from the silicone sealant, and the attitude required for passive thermal control. It was not until the crew had gone behind the Moon that they would be able to see it for the first time.
Apollo 8 made a second television broadcast at 55 hours into the flight. This time, the crew rigged up filters meant for the still cameras so they could acquire images of the Earth through the telephoto lens. Although difficult to aim, as they had to maneuver the entire spacecraft, the crew was able to broadcast back to Earth the first television pictures of the Earth. The crew spent the transmission describing the Earth, what was visible, and the colors they could see. The transmission lasted 23 minutes.
Lunar sphere of influence.
At about 55 hours and 40 minutes into the flight, and 13 hours before entering lunar orbit, the crew of Apollo8 became the first humans to enter the gravitational sphere of influence of another celestial body. In other words, the effect of the Moon's gravitational force on Apollo8 became stronger than that of the Earth. At the time it happened, Apollo8 was from the Moon and had a speed of relative to the Moon. This historic moment was of little interest to the crew, since they were still calculating their trajectory with respect to the launch pad at Kennedy Space Center. They would continue to do so until they performed their last mid-course correction, switching to a reference frame based on ideal orientation for the second engine burn they would make in lunar orbit.
The last major event before Lunar Orbit Insertion (LOI) was a second mid-course correction. It was in retrograde (against the direction of travel) and slowed the spacecraft down by , effectively reducing the closest distance at which the spacecraft would pass the Moon. At exactly 61 hours after launch, about from the Moon, the crew burned the RCS for 11 seconds. They would now pass from the lunar surface.
At 64 hours into the flight, the crew began to prepare for Lunar Orbit Insertion1 (LOI-1). This maneuver had to be performed perfectly, and due to orbital mechanics had to be on the far side of the Moon, out of contact with the Earth. After Mission Control was polled for a "go/no go" decision, the crew was told at 68 hours that they were Go and "riding the best bird we can find". Lovell replied, "We'll see you on the other side", and for the first time in history, humans travelled behind the Moon and out of radio contact with the Earth. Frances "Poppy" Northcutt, who was the first woman in NASA's mission control and helped calculate the return to earth trajectory for this mission, recounts what it was like when they went behind the moon for the first time in an interview: "That was a very nerve-racking period on the team I was on, and I think it was a very nerve-racking period in general because of this thing with losing signal. You’ve got this big mystery going on there on the backside of the Moon. You do not know what’s happening and there’s not a darn thing anybody here can do about it until we hear from them"
With ten minutes remaining before LOI-1, the crew began one last check of the spacecraft systems and made sure that every switch was in its correct position. At that time, they finally got their first glimpses of the Moon. They had been flying over the unlit side, and it was Lovell who saw the first shafts of sunlight obliquely illuminating the lunar surface. The LOI burn was only two minutes away, so the crew had little time to appreciate the view.
Lunar orbit.
The SPS was ignited at 69 hours, 8minutes, and 16 seconds after launch and burned for 4minutes and 7seconds, placing the Apollo8 spacecraft in orbit around the Moon. The crew described the burn as being the longest four minutes of their lives. If the burn had not lasted exactly the correct amount of time, the spacecraft could have ended up in a highly elliptical lunar orbit or even been flung off into space. If it had lasted too long, they could have struck the Moon. After making sure the spacecraft was working, they finally had a chance to look at the Moon, which they would orbit for the next 20 hours.
On Earth, Mission Control continued to wait. If the crew had not burned the engine, or the burn had not lasted the planned length of time, the crew would have appeared early from behind the Moon. Exactly at the calculated moment the signal was received from the spacecraft, indicating it was in a orbit around the Moon.
After reporting on the status of the spacecraft, Lovell gave the first description of what the lunar surface looked like:
Lovell continued to describe the terrain they were passing over. One of the crew's major tasks was reconnaissance of planned future landing sites on the Moon, especially one in Mare Tranquillitatis that was planned as the Apollo11 landing site. The launch time of Apollo8 had been chosen to give the best lighting conditions for examining the site. A film camera had been set up in one of the spacecraft windows to record one frame per second of the Moon below. Bill Anders spent much of the next 20 hours taking as many photographs as possible of targets of interest. By the end of the mission, the crew had taken over eight hundred 70 mm still photographs and of 16 mm movie film.
Throughout the hour that the spacecraft was in contact with Earth, Borman kept asking how the data for the SPS looked. He wanted to make sure that the engine was working and could be used to return early to the Earth if necessary. He also asked that they receive a "go/no go" decision before they passed behind the Moon on each orbit.
As they reappeared for their second pass in front of the Moon, the crew set up equipment to broadcast a view of the lunar surface. Anders described the craters that they were passing over. At the end of this second orbit, they performed an 11-second LOI-2 burn of the SPS to circularize the orbit to .
Throughout the next two orbits, the crew continued to check the spacecraft and to observe and photograph the Moon. During the third pass, Borman read a small prayer for his church. He had been scheduled to participate in a service at St. Christopher's Episcopal Church near Seabrook, Texas, but due to the Apollo8 flight, he was unable to attend. A fellow parishioner and engineer at Mission Control, Rod Rose, suggested that Borman read the prayer, which could be recorded and then replayed during the service.
"Earthrise" and Genesis broadcast.
When the spacecraft came out from behind the Moon for its fourth pass across the front, the crew witnessed an "Earthrise" in person for the first time in human history. NASA's Lunar Orbiter 1 had taken the first picture of an Earthrise from the vicinity of the Moon, on August 23, 1966. Anders saw the Earth emerging from behind the lunar horizon and called in excitement to the others, taking a black-and-white photograph as he did so. Anders asked Lovell for color film and then took "Earthrise", a now famous color photo, later picked by "Life" magazine as one of its hundred photos of the century.
Due to the synchronous rotation of the Moon about the Earth, Earthrise is not generally visible from the lunar surface. This is because, as seen from any one place on the Moon's surface, Earth remains in approximately the same position in the lunar sky, either above or below the horizon. Earthrise is generally visible only while orbiting the Moon, and at selected surface locations near the Moon's limb, where libration carries the Earth slightly above and below the lunar horizon.
Anders continued to take photographs while Lovell assumed control of the spacecraft so that Borman could rest. Despite the difficulty resting in the cramped and noisy spacecraft, Borman was able to sleep for two orbits, awakening periodically to ask questions about their status. Borman awoke fully when he started to hear his fellow crew members make mistakes. They were beginning to not understand questions and had to ask for the answers to be repeated. Borman realized that everyone was extremely tired from not having a good night's sleep in over three days. He ordered Anders and Lovell to get some sleep and that the rest of the flight plan regarding observing the Moon be scrubbed. Anders initially protested, saying that he was fine, but Borman would not be swayed. Anders finally agreed under the condition that Borman would set up the camera to continue to take automatic pictures of the Moon. Borman also remembered that there was a second television broadcast planned, and with so many people expected to be watching, he wanted the crew to be alert. For the next two orbits, Anders and Lovell slept while Borman sat at the helm.
As they rounded the Moon for the ninth time, the astronauts began the second television transmission. Borman introduced the crew, followed by each man giving his impression of the lunar surface and what it was like to be orbiting the Moon. Borman described it as being "a vast, lonely, forbidding expanse of nothing". Then, after talking about what they were flying over, Anders said that the crew had a message for all those on Earth. Each man on board read a section from the Biblical creation story from the Book of Genesis. Borman finished the broadcast by wishing a Merry Christmas to everyone on Earth. His message appeared to sum up the feelings that all three crewmen had from their vantage point in lunar orbit. Borman said, "And from the crew of Apollo8, we close with good night, good luck, a Merry Christmas and God bless all of you—all of you on the good Earth."
The only task left for the crew at this point was to perform the trans-Earth injection (TEI), which was scheduled for hours after the end of the television transmission. The TEI was the most critical burn of the flight, as any failure of the SPS to ignite would strand the crew in lunar orbit, with little hope of escape. As with the previous burn, the crew had to perform the maneuver above the far side of the Moon, out of contact with Earth. The burn occurred exactly on time. The spacecraft telemetry was reacquired as it re-emerged from behind the Moon at 89 hours, 28 minutes, and 39 seconds, the exact time calculated. When voice contact was regained, Lovell announced, "Please be informed, there is a Santa Claus", to which Ken Mattingly, the current CAPCOM, replied, "That's affirmative, you are the best ones to know." The spacecraft began its journey back to Earth on December 25, Christmas Day.
Unplanned manual realignment.
Later, Lovell used some otherwise idle time to do some navigational sightings, maneuvering the module to view various stars by using the computer keyboard. He accidentally erased some of the computer's memory, which caused the inertial measurement unit (IMU) to contain data indicating that the module was in the same relative orientation it had been in before lift-off; the IMU then fired the thrusters to "correct" the module's attitude.
Once the crew realized why the computer had changed the module's attitude, they realized that they would have to reenter data to tell the computer the module's actual orientation. It took Lovell ten minutes to figure out the right numbers, using the thrusters to get the stars Rigel and Sirius aligned, and another 15 minutes to enter the corrected data into the computer. Sixteen months later, during the Apollo13 mission, Lovell would have to perform a similar manual realignment under more critical conditions after the module's IMU had to be turned off to conserve energy.
Cruise back to Earth and reentry.
The cruise back to Earth was mostly a time for the crew to relax and monitor the spacecraft. As long as the trajectory specialists had calculated everything correctly, the spacecraft would reenter Earth's atmosphere two-and-a-half days after TEI and splash down in the Pacific.
On Christmas afternoon, the crew made their fifth television broadcast. This time, they gave a tour of the spacecraft, showing how an astronaut lived in space. When they finished broadcasting, they found a small present from Slayton in the food locker: a real turkey dinner with stuffing, in the same kind of pack given to the troops in Vietnam.
Another Slayton surprise was a gift of three miniature bottles of brandy, which Borman ordered the crew to leave alone until after they landed. They remained unopened, even years after the flight. There were also small presents to the crew from their wives. The next day, at about 124 hours into the mission, the sixth and final TV transmission showed the mission's best video images of the Earth, during a four-minute broadcast. After two uneventful days, the crew prepared for reentry. The computer would control the reentry, and all the crew had to do was put the spacecraft in the correct attitude, with the blunt end forward. In the event of computer failure, Borman was ready to take over.
Separation from the service module prepared the command module for reentry by exposing the heat shield and shedding unneeded mass. The service module would burn up in the atmosphere as planned. Six minutes before they hit the top of the atmosphere, the crew saw the Moon rising above the Earth's horizon, just as had been calculated by the trajectory specialists. As the module hit the thin outer atmosphere, the crew noticed that it was becoming hazy outside as glowing plasma formed around the spacecraft. The spacecraft started slowing down, and the deceleration peaked at . With the computer controlling the descent by changing the attitude of the spacecraft, Apollo8 rose briefly like a skipping stone before descending to the ocean. At , the drogue parachute deployed, stabilizing the spacecraft, followed at by the three main parachutes. The spacecraft splashdown position was officially reported as in the North Pacific Ocean, southwest of Hawaii at 15:51:42 UTC on December 27, 1968.
When the spacecraft hit the water, the parachutes dragged it over and left it upside down, in what was termed Stable2 position. As they were buffeted by a swell, Borman vomited, waiting for the three flotation balloons to right the spacecraft. About six minutes after splashdown, the command module was righted into a normal apex-up (Stable 1) orientation by its inflatable bag uprighting system. The first frogman from aircraft carrier arrived 43 minutes after splashdown. Forty-five minutes later, the crew was safe on the flight deck of the "Yorktown".
Legacy.
Historical importance.
Apollo 8 came at the end of 1968, a year that had seen much upheaval in the United States and most of the world. Even though the year saw political assassinations, political unrest in the streets of Europe and America, and the Prague Spring, "Time" magazine chose the crew of Apollo8 as its Men of the Year for 1968, recognizing them as the people who most influenced events in the preceding year. They had been the first people ever to leave the gravitational influence of the Earth and orbit another celestial body. They had survived a mission that even the crew themselves had rated as having only a fifty-fifty chance of fully succeeding. The effect of Apollo8 was summed up in a telegram from a stranger, received by Borman after the mission, that stated simply, "Thank you Apollo8. You saved 1968."
One of the most famous aspects of the flight was the "Earthrise" picture that the crew took as they came around for their fourth orbit of the Moon. This was the first time that humans had taken such a picture while actually behind the camera, and it has been credited as one of the inspirations of the first Earth Day in 1970. It was selected as the first of "Life" magazine's "100 Photographs That Changed the World".
Apollo 11 astronaut Michael Collins said, "Eight's momentous historic significance was foremost"; while space historian Robert K. Poole saw Apollo8 as the most historically significant of all the Apollo missions. The mission was the most widely covered by the media since the first American orbital flight, Mercury-Atlas 6 by John Glenn, in 1962. There were 1,200 journalists covering the mission, with the BBC's coverage broadcast in 54 countries in 15 different languages. The Soviet newspaper "Pravda" featured a quote from , Chairman of the Soviet Interkosmos program, who described the flight as an "outstanding achievement of American space sciences and technology". It is estimated that a quarter of the people alive at the time saw—either live or delayed—the Christmas Eve transmission during the ninth orbit of the Moon. The Apollo8 broadcasts won an Emmy Award, the highest honor given by the Academy of Television Arts &amp; Sciences.
Madalyn Murray O'Hair, an atheist, later caused controversy by bringing a lawsuit against NASA over the reading from Genesis. O'Hair wanted the courts to ban American astronauts—who were all government employees—from public prayer in space. Though the case was rejected by the Supreme Court of the United States, apparently for lack of jurisdiction in outer space, it caused NASA to be skittish about the issue of religion throughout the rest of the Apollo program. Buzz Aldrin, on Apollo11, self-communicated Presbyterian Communion on the surface of the Moon after landing; he refrained from mentioning this publicly for several years and referred to it only obliquely at the time.
In 1969, the United States Post Office Department issued a postage stamp (Scott catalogue #1371) commemorating the Apollo8 flight around the Moon. The stamp featured a detail of the famous photograph of the Earthrise over the Moon taken by Anders on Christmas Eve, and the words, "In the beginning God...", the first words of the book of Genesis. In January 1969, just 18 days after the crew's return to Earth, they appeared in the Super Bowl III pre-game show, reciting the Pledge of Allegiance, before the national anthem was performed by trumpeter Lloyd Geisler of the Washington National Symphony Orchestra.
Spacecraft location.
In January 1970, the spacecraft was delivered to Osaka, Japan, for display in the U.S. pavilion at Expo '70. It is now displayed at the Chicago Museum of Science and Industry, along with a collection of personal items from the flight donated by Lovell and the space suit worn by Frank Borman. Jim Lovell's Apollo8 space suit is on public display in the Visitor Center at NASA's Glenn Research Center. Bill Anders's space suit is on display at the Science Museum in London, United Kingdom.
In popular culture.
Apollo 8's historic mission has been depicted and referred to in several forms, both documentary and fiction. The various television transmissions and 16 mm footage shot by the crew of Apollo8 were compiled and released by NASA in the 1969 documentary "Debrief: Apollo8", hosted by Burgess Meredith. In addition, Spacecraft Films released, in 2003, a three-disc DVD set containing all of NASA's TV and 16 mm film footage related to the mission, including all TV transmissions from space, training and launch footage, and motion pictures taken in flight. Other documentaries include "Race to the Moon" (2005) as part of season 18 of "American Experience" and "In the Shadow of the Moon" (2007). Apollo's Daring Mission aired on PBS' "" in December 2018, marking the flight's 50th anniversary.
The 1994 album "The Songs of Distant Earth" by Mike Oldfield uses the Anders' reading for the cut "In The Beginning".
Apollo 8 serves as character development in the 1995 film "Apollo 13", in which Jim Lovell is motivated to walk on the Moon by his Apollo 8 experience and later disappointed to be so near the surface twice without walking on it.
Parts of the mission are dramatized in the 1998 miniseries "From the Earth to the Moon" episode "1968". The S-IVB stage of Apollo8 was also portrayed as the location of an alien device in the 1970 "UFO" episode "Conflict". Apollo8's lunar orbit insertion was chronicled with actual recordings in the song "The Other Side", on the 2015 album "The Race for Space", by the band Public Service Broadcasting.
In the credits of the animated film "Free Birds" (2013) a newspaper front page about the Apollo 8 mission is doctored to read: "As one of the most turbulent, tragic years in American history drew to a close, millions around the world were watching and listening as the Apollo 8 astronauts – Frank Gobbler, Jim Snood, and Bill Wattles – became the first turkeys to orbit another world."
A documentary film, "" was released in 2018.
The choral music piece "Earthrise" by Luke Byrne commemorates the mission. The piece was premièred on January 19, 2020, by Sydney Philharmonia Choirs at the Sydney Opera House.

</doc>
<doc id="664" url="?curid=664" title="Astronaut">
Astronaut

An astronaut (from the Ancient Greek (), meaning 'star', and (), meaning 'sailor') is a person trained, equipped, and deployed by a human spaceflight program to serve as a commander or crew member aboard a spacecraft. Although generally reserved for professional space travelers, the term is sometimes applied to anyone who travels into space, including scientists, politicians, journalists, and tourists.
"Astronaut" technically applies to all human space travelers regardless of nationality. However, astronauts fielded by Russia or the Soviet Union are typically known instead as cosmonauts (from the Russian "kosmos" (космос), meaning "space", also borrowed from Greek ). Comparatively recent developments in crewed spaceflight made by China have led to the rise of the term taikonaut (from the Mandarin "tàikōng" (), meaning "space"), although its use is somewhat informal and its origin is unclear. In China, the People's Liberation Army Astronaut Corps astronauts and their foreign counterparts are all officially called "hángtiānyuán" (, meaning "heaven navigator" or literally "heaven-sailing staff").
Since 1961, 600 astronauts have flown in space. Until 2002, astronauts were sponsored and trained exclusively by governments, either by the military or by civilian space agencies. With the suborbital flight of the privately funded SpaceShipOne in 2004, a new category of astronaut was created: the commercial astronaut.
Definition.
The criteria for what constitutes human spaceflight vary, with some focus on the point where the atmosphere becomes so thin that centrifugal force, rather than aerodynamic force, carries a significant portion of the weight of the flight object. The (FAI) Sporting Code for astronautics recognizes only flights that exceed the Kármán line, at an altitude of . In the United States, professional, military, and commercial astronauts who travel above an altitude of are awarded astronaut wings.
, 552 people from 36 countries have reached or more in altitude, of whom 549 reached low Earth orbit or beyond.
Of these, 24 people have traveled beyond low Earth orbit, either to lunar orbit, the lunar surface, or, in one case, a loop around the Moon. Three of the 24—Jim Lovell, John Young and Eugene Cernan—did so twice.
, under the U.S. definition, 558 people qualify as having reached space, above altitude. Of eight X-15 pilots who exceeded in altitude, only one, Joseph A. Walker, exceeded 100 kilometers (about 62.1 miles) and he did it two times, becoming the first person in space twice. Space travelers have spent over 41,790 man-days (114.5-man-years) in space, including over 100 astronaut-days of spacewalks. , the man with the longest cumulative time in space is Gennady Padalka, who has spent 879 days in space. Peggy A. Whitson holds the record for the most time in space by a woman, at 675 days.
Terminology.
In 1959, when both the United States and Soviet Union were planning, but had yet to launch humans into space, NASA Administrator T. Keith Glennan and his Deputy Administrator, Hugh Dryden, discussed whether spacecraft crew members should be called "astronauts" or "cosmonauts". Dryden preferred "cosmonaut", on the grounds that flights would occur in and to the broader "cosmos", while the "astro" prefix suggested flight specifically to the stars. Most NASA Space Task Group members preferred "astronaut", which survived by common usage as the preferred American term. When the Soviet Union launched the first man into space, Yuri Gagarin in 1961, they chose a term which anglicizes to "cosmonaut".
Astronaut.
A professional space traveler is called an "astronaut". The first known use of the term "astronaut" in the modern sense was by Neil R. Jones in his 1930 short story "The Death's Head Meteor". The word itself had been known earlier; for example, in Percy Greg's 1880 book "Across the Zodiac", "astronaut" referred to a spacecraft. In "Les Navigateurs de l'infini" (1925) by J.-H. Rosny aîné, the word "astronautique" (astronautics) was used. The word may have been inspired by "aeronaut", an older term for an air traveler first applied in 1784 to balloonists. An early use of "astronaut" in a non-fiction publication is Eric Frank Russell's poem "The Astronaut", appearing in the November 1934 "Bulletin of the British Interplanetary Society".
The first known formal use of the term astronautics in the scientific community was the establishment of the annual International Astronautical Congress in 1950, and the subsequent founding of the International Astronautical Federation the following year.
NASA applies the term astronaut to any crew member aboard NASA spacecraft bound for Earth orbit or beyond. NASA also uses the term as a title for those selected to join its Astronaut Corps. The European Space Agency similarly uses the term astronaut for members of its Astronaut Corps.
Cosmonaut.
By convention, an astronaut employed by the Russian Federal Space Agency (or its predecessor, the Soviet space program) is called a "cosmonaut" in English texts. The word is an Anglicization of "kosmonavt" ( ). Other countries of the former Eastern Bloc use variations of the Russian kosmonavt, such as the (although Poles also used , and the two words are considered synonyms).
Coinage of the term has been credited to Soviet aeronautics (or "cosmonautics") pioneer Mikhail Tikhonravov (1900–1974). The first cosmonaut was Soviet Air Force pilot Yuri Gagarin, also the first person in space. He was part of the first six Soviet citizens, with German Titov, Yevgeny Khrunov, Andriyan Nikolayev, Pavel Popovich, and Grigoriy Nelyubov, who were given the title of pilot-cosmonaut in January 1961. Valentina Tereshkova was the first female cosmonaut and the first and youngest woman to have flown in space with a solo mission on the Vostok 6 in 1963. On 14 March 1995, Norman Thagard became the first American to ride to space on board a Russian launch vehicle, and thus became the first "American cosmonaut".
Taikonaut.
In Chinese, the term (, "cosmos navigating personnel") is used for astronauts and cosmonauts in general, while (, "navigating celestial-heaven personnel") is used for Chinese astronauts. Here, (, literally "heaven-navigating", or spaceflight) is strictly defined as the navigation of outer space within the local star system, i.e. Solar System. The phrase (, "spaceman") is often used in Hong Kong and Taiwan.
The term "taikonaut" is used by some English-language news media organizations for professional space travelers from China. The word has featured in the Longman and Oxford English dictionaries, and the term became more common in 2003 when China sent its first astronaut Yang Liwei into space aboard the "Shenzhou 5" spacecraft. This is the term used by Xinhua News Agency in the English version of the Chinese "People's Daily" since the advent of the Chinese space program. The origin of the term is unclear; as early as May 1998, Chiew Lee Yih () from Malaysia used it in newsgroups.
Parastronaut.
For its 2022 Astronaut Group, the European Space Agency envisioned recruiting an astronaut with a physical disability, a category they called "parastronauts", with the intention but not guarantee of spaceflight. The categories of disability considered for the program were individuals with lower limb deficiency (either through amputation or congenital), leg length difference, or a short stature (less than ). On 23 November 2022, John McFall was selected to be the first ESA parastronaut.
Other terms.
With the rise of space tourism, NASA and the Russian Federal Space Agency agreed to use the term "spaceflight participant" to distinguish those space travelers from professional astronauts on missions coordinated by those two agencies.
While no nation other than Russia (and previously the Soviet Union), the United States, and China have launched a crewed spacecraft, several other nations have sent people into space in cooperation with one of these countries, e.g. the Soviet-led Interkosmos program. Inspired partly by these missions, other synonyms for astronaut have entered occasional English usage. For example, the term "spationaut" () is sometimes used to describe French space travelers, from the Latin word for "space"; the Malay term (deriving from "angkasa" meaning 'space') was used to describe participants in the Angkasawan program (note its similarity with the Indonesian term "antariksawan"). Plans of the Indian Space Research Organisation to launch its crewed Gaganyaan spacecraft have spurred at times public discussion if another term than "astronaut" should be used for the crew members, suggesting "vyomanaut" (from the Sanskrit word meaning 'sky' or 'space') or "gagannaut" (from the Sanskrit word for 'sky'). In Finland, the NASA astronaut Timothy Kopra, a Finnish American, has sometimes been referred to as , from the Finnish word . Across Germanic languages, the word for "astronaut" typically translates to "space traveler", as it does with German's "Raumfahrer", Dutch's "ruimtevaarder", Swedish's "rymdfarare", and Norwegian's "romfarer".
As of 2021 in the United States, astronaut status is conferred on a person depending on the authorizing agency:
On July 20, 2021, the FAA issued an order redefining the eligibility criteria to be an astronaut in response to the private suborbital spaceflights of Jeff Bezos and Richard Branson. The new criteria states that one must have "[d]emonstrated activities during flight that were essential to public safety, or contributed to
human space flight safety" to qualify as an astronaut. This new definition excludes Bezos and Branson.
Space travel milestones.
The first human in space was Soviet Yuri Gagarin, who was launched on 12 April 1961, aboard Vostok 1 and orbited around the Earth for 108 minutes. The first woman in space was Soviet Valentina Tereshkova, who launched on 16 June 1963, aboard Vostok 6 and orbited Earth for almost three days.
Alan Shepard became the first American and second person in space on 5 May 1961, on a 15-minute sub-orbital flight aboard "Freedom 7". The first American to orbit the Earth was John Glenn, aboard "Friendship 7" on 20 February 1962. The first American woman in space was Sally Ride, during Space Shuttle "Challenger"'s mission STS-7, on 18 June 1983. In 1992, Mae Jemison became the first African American woman to travel in space aboard STS-47.
Cosmonaut Alexei Leonov was the first person to conduct an extravehicular activity (EVA), (commonly called a "spacewalk"), on 18 March 1965, on the Soviet Union's Voskhod 2 mission. This was followed two and a half months later by astronaut Ed White who made the first American EVA on NASA's Gemini 4 mission.
The first crewed mission to orbit the Moon, Apollo 8, included American William Anders who was born in Hong Kong, making him the first Asian-born astronaut in 1968.
The Soviet Union, through its Intercosmos program, allowed people from other "socialist" (i.e. Warsaw Pact and other Soviet-allied) countries to fly on its missions, with the notable exceptions of France and Austria participating in Soyuz TM-7 and Soyuz TM-13, respectively. An example is Czechoslovak Vladimír Remek, the first cosmonaut from a country other than the Soviet Union or the United States, who flew to space in 1978 on a Soyuz-U rocket. Rakesh Sharma became the first Indian citizen to travel to space. He was launched aboard Soyuz T-11, on 2 April 1984.
On 23 July 1980, Pham Tuan of Vietnam became the first Asian in space when he flew aboard Soyuz 37. Also in 1980, Cuban Arnaldo Tamayo Méndez became the first person of Hispanic and black African descent to fly in space, and in 1983, Guion Bluford became the first African American to fly into space. In April 1985, Taylor Wang became the first ethnic Chinese person in space. The first person born in Africa to fly in space was Patrick Baudry (France), in 1985. In 1985, Saudi Arabian Prince Sultan Bin Salman Bin AbdulAziz Al-Saud became the first Arab Muslim astronaut in space. In 1988, Abdul Ahad Mohmand became the first Afghan to reach space, spending nine days aboard the "Mir" space station.
With the increase of seats on the Space Shuttle, the U.S. began taking international astronauts. In 1983, Ulf Merbold of West Germany became the first non-US citizen to fly in a US spacecraft. In 1984, Marc Garneau became the first of eight Canadian astronauts to fly in space (through 2010).
In 1985, Rodolfo Neri Vela became the first Mexican-born person in space. In 1991, Helen Sharman became the first Briton to fly in space.
In 2002, Mark Shuttleworth became the first citizen of an African country to fly in space, as a paying spaceflight participant. In 2003, Ilan Ramon became the first Israeli to fly in space, although he died during a re-entry accident.
On 15 October 2003, Yang Liwei became China's first astronaut on the Shenzhou 5 spacecraft.
On 30 May 2020, Doug Hurley and Bob Behnken became the first astronauts to launch on a private crewed spacecraft, Crew Dragon.
Age milestones.
The youngest person to reach space is Oliver Daemen, who was 18 years and 11 months old when he made a suborbital spaceflight on Blue Origin NS-16. Daemen, who was a commercial passenger aboard the New Shepard, broke the record of Soviet cosmonaut Gherman Titov, who was 25 years old when he flew Vostok 2. Titov remains the youngest human to reach orbit; he rounded the planet 17 times. Titov was also the first person to suffer space sickness and the first person to sleep in space, twice. The oldest person to reach space is William Shatner, who was 90 years old when he made a suborbital spaceflight on Blue Origin NS-18. The oldest person to reach orbit is John Glenn, one of the Mercury 7, who was 77 when he flew on STS-95.
Duration and distance milestones.
The longest time spent in space was by Russian Valeri Polyakov, who spent 438 days there.
As of 2006, the most spaceflights by an individual astronaut is seven, a record held by both Jerry L. Ross and Franklin Chang-Diaz. The farthest distance from Earth an astronaut has traveled was , when Jim Lovell, Jack Swigert, and Fred Haise went around the Moon during the Apollo 13 emergency.
Civilian and non-government milestones.
The first civilian in space was Valentina Tereshkova aboard Vostok 6 (she also became the first woman in space on that mission).
Tereshkova was only honorarily inducted into the USSR's Air Force, which did not accept female pilots at that time. A month later, Joseph Albert Walker became the first American civilian in space when his X-15 Flight 90 crossed the line, qualifying him by the international definition of spaceflight. Walker had joined the US Army Air Force but was not a member during his flight.
The first people in space who had never been a member of any country's armed forces were both Konstantin Feoktistov and Boris Yegorov aboard Voskhod 1.
The first non-governmental space traveler was Byron K. Lichtenberg, a researcher from the Massachusetts Institute of Technology who flew on STS-9 in 1983. In December 1990, Toyohiro Akiyama became the first paying space traveler and the first journalist in space for Tokyo Broadcasting System, a visit to Mir as part of an estimated $12 million (USD) deal with a Japanese TV station, although at the time, the term used to refer to Akiyama was "Research Cosmonaut". Akiyama suffered severe space sickness during his mission, which affected his productivity.
The first self-funded space tourist was Dennis Tito on board the Russian spacecraft Soyuz TM-3 on 28 April 2001.
Self-funded travelers.
The first person to fly on an entirely privately funded mission was Mike Melvill, piloting SpaceShipOne flight 15P on a suborbital journey, although he was a test pilot employed by Scaled Composites and not an actual paying space tourist. Jared Isaacman was the first person to self-fund a mission to orbit, commanding Inspiration4 in 2021. Nine others have paid Space Adventures to fly to the International Space Station:
Training.
The first NASA astronauts were selected for training in 1959. Early in the space program, military jet test piloting and engineering training were often cited as prerequisites for selection as an astronaut at NASA, although neither John Glenn nor Scott Carpenter (of the Mercury Seven) had any university degree, in engineering or any other discipline at the time of their selection. Selection was initially limited to military pilots. The earliest astronauts for both the US and the USSR tended to be jet fighter pilots, and were often test pilots.
Once selected, NASA astronauts go through twenty months of training in a variety of areas, including training for extravehicular activity in a facility such as NASA's Neutral Buoyancy Laboratory. Astronauts-in-training (astronaut candidates) may also experience short periods of weightlessness (microgravity) in an aircraft called the "Vomit Comet," the nickname given to a pair of modified KC-135s (retired in 2000 and 2004, respectively, and replaced in 2005 with a C-9) which perform parabolic flights. Astronauts are also required to accumulate a number of flight hours in high-performance jet aircraft. This is mostly done in T-38 jet aircraft out of Ellington Field, due to its proximity to the Johnson Space Center. Ellington Field is also where the Shuttle Training Aircraft is maintained and developed, although most flights of the aircraft are conducted from Edwards Air Force Base.
Astronauts in training must learn how to control and fly the Space Shuttle; further, it is vital that they are familiar with the International Space Station so they know what they must do when they get there.
NASA candidacy requirements.
The master's degree requirement can also be met by:
Mission Specialist Educator.
Mission Specialist Educators, or "Educator Astronauts", were first selected in 2004; as of 2007, there are three NASA Educator astronauts: Joseph M. Acaba, Richard R. Arnold, and Dorothy Metcalf-Lindenburger.
Barbara Morgan, selected as back-up teacher to Christa McAuliffe in 1985, is considered to be the first Educator astronaut by the media, but she trained as a mission specialist.
The Educator Astronaut program is a successor to the Teacher in Space program from the 1980s.
Health risks of space travel.
Astronauts are susceptible to a variety of health risks including decompression sickness, barotrauma, immunodeficiencies, loss of bone and muscle, loss of eyesight, orthostatic intolerance, sleep disturbances, and radiation injury. A variety of large scale medical studies are being conducted in space via the National Space Biomedical Research Institute (NSBRI) to address these issues. Prominent among these is the Advanced Diagnostic Ultrasound in Microgravity Study in which astronauts (including former ISS commanders Leroy Chiao and Gennady Padalka) perform ultrasound scans under the guidance of remote experts to diagnose and potentially treat hundreds of medical conditions in space. This study's techniques are now being applied to cover professional and Olympic sports injuries as well as ultrasound performed by non-expert operators in medical and high school students. It is anticipated that remote guided ultrasound will have application on Earth in emergency and rural care situations, where access to a trained physician is often rare.
A 2006 Space Shuttle experiment found that "Salmonella typhimurium", a bacterium that can cause food poisoning, became more virulent when cultivated in space. More recently, in 2017, bacteria were found to be more resistant to antibiotics and to thrive in the near-weightlessness of space. Microorganisms have been observed to survive the vacuum of outer space.
On 31 December 2012, a NASA-supported study reported that human spaceflight may harm the brain and accelerate the onset of Alzheimer's disease.
In October 2015, the NASA Office of Inspector General issued a health hazards report related to space exploration, including a human mission to Mars.
Over the last decade, flight surgeons and scientists at NASA have seen a pattern of vision problems in astronauts on long-duration space missions. The syndrome, known as visual impairment intracranial pressure (VIIP), has been reported in nearly two-thirds of space explorers after long periods spent aboard the International Space Station (ISS).
On 2 November 2017, scientists reported that significant changes in the position and structure of the brain have been found in astronauts who have taken trips in space, based on MRI studies. Astronauts who took longer space trips were associated with greater brain changes.
Being in space can be physiologically deconditioning on the body. It can affect the otolith organs and adaptive capabilities of the central nervous system. Zero gravity and cosmic rays can cause many implications for astronauts.
In October 2018, NASA-funded researchers found that lengthy journeys into outer space, including travel to the planet Mars, may substantially damage the gastrointestinal tissues of astronauts. The studies support earlier work that found such journeys could significantly damage the brains of astronauts, and age them prematurely.
Researchers in 2018 reported, after detecting the presence on the International Space Station (ISS) of five "Enterobacter bugandensis" bacterial strains, none pathogenic to humans, that microorganisms on ISS should be carefully monitored to continue assuring a medically healthy environment for astronauts.
A study by Russian scientists published in April 2019 stated that astronauts facing space radiation could face temporary hindrance of their memory centers. While this does not affect their intellectual capabilities, it temporarily hinders formation of new cells in brain's memory centers. The study conducted by Moscow Institute of Physics and Technology (MIPT) concluded this after they observed that mice exposed to neutron and gamma radiation did not impact the rodents' intellectual capabilities.
A 2020 study conducted on the brains of eight male Russian cosmonauts after they returned from long stays aboard the International Space Station showed that long-duration spaceflight causes many physiological adaptions, including macro- and microstructural changes. While scientists still know little about the effects of spaceflight on brain structure, this study showed that space travel can lead to new motor skills (dexterity), but also slightly weaker vision, both of which could possibly be long lasting. It was the first study to provide clear evidence of sensorimotor neuroplasticity, which is the brain's ability to change through growth and reorganization.
Food and drink.
An astronaut on the International Space Station requires about mass of food per meal each day (inclusive of about packaging mass per meal).
Space Shuttle astronauts worked with nutritionists to select menus that appealed to their individual tastes. Five months before flight, menus were selected and analyzed for nutritional content by the shuttle dietician. Foods are tested to see how they will react in a reduced gravity environment. Caloric requirements are determined using a basal energy expenditure (BEE) formula. On Earth, the average American uses about of water every day. On board the ISS astronauts limit water use to only about per day.
Insignia.
In Russia, cosmonauts are awarded Pilot-Cosmonaut of the Russian Federation upon completion of their missions, often accompanied with the award of Hero of the Russian Federation. This follows the practice established in the USSR where cosmonauts were usually awarded the title Hero of the Soviet Union.
At NASA, those who complete astronaut candidate training receive a silver lapel pin. Once they have flown in space, they receive a gold pin. U.S. astronauts who also have active-duty military status receive a special qualification badge, known as the Astronaut Badge, after participation on a spaceflight. The United States Air Force also presents an Astronaut Badge to its pilots who exceed in altitude.
Deaths.
, eighteen astronauts (fourteen men and four women) have died during four space flights. By nationality, thirteen were American, four were Russian (Soviet Union), and one was Israeli.
, eleven people (all men) have died training for spaceflight: eight Americans and three Russians. Six of these were in crashes of training jet aircraft, one drowned during water recovery training, and four were due to fires in pure oxygen environments.
Astronaut David Scott left a memorial consisting of a statuette titled "Fallen Astronaut" on the surface of the Moon during his 1971 Apollo 15 mission, along with a list of the names of eight of the astronauts and six cosmonauts known at the time to have died in service.
The Space Mirror Memorial, which stands on the grounds of the Kennedy Space Center Visitor Complex, is maintained by the Astronauts Memorial Foundation and commemorates the lives of the men and women who have died during spaceflight and during training in the space programs of the United States. In addition to twenty NASA career astronauts, the memorial includes the names of an X-15 test pilot, a U.S. Air Force officer who died while training for a then-classified military space program, and a civilian spaceflight participant.

</doc>
<doc id="665" url="?curid=665" title="A Modest Proposal">
A Modest Proposal

A Modest Proposal for Preventing the Children of Poor People from Being a Burthen to Their Parents or Country, and for Making Them Beneficial to the Publick, commonly referred to as A Modest Proposal, is a Juvenalian satirical essay written and published anonymously by Anglo-Irish writer and clergyman Jonathan Swift in 1729. The essay suggests that poor people in Ireland could ease their economic troubles by selling their children as food to the elite. Swift's use of satirical hyperbole was intended to mock hostile attitudes towards the poor and anti-Catholicism among the Protestant Ascendancy as well as the Dublin Castle administration's policies in general. In English writing, the phrase "a modest proposal" is now conventionally an allusion to this style of straight-faced satire.
Synopsis.
Swift's essay is widely held to be one of the greatest examples of sustained irony in the history of English literature. Much of its shock value derives from the fact that the first portion of the essay describes the plight of starving beggars in Ireland, so that the reader is unprepared for the surprise of Swift's solution when he states: "A young healthy child well nursed, is, at a year old, a most delicious nourishing and wholesome food, whether stewed, roasted, baked, or boiled; and I make no doubt that it will equally serve in a fricassee, or a ragout."
Swift goes to great lengths to support his argument, including a list of possible preparation styles for the children, and calculations showing the financial benefits of his suggestion. He uses methods of argument throughout his essay which lampoon the then-influential William Petty and the social engineering popular among followers of Francis Bacon. These lampoons include appealing to the authority of "a very knowing American of my acquaintance in London" and "the famous Psalmanazar, a native of the island Formosa" (who had already confessed to "not" being from Formosa in 1706).
In the tradition of Roman satire, Swift introduces the reforms he is actually suggesting by paralipsis:
Population solutions.
George Wittkowsky argued that Swift's main target in "A Modest Proposal" was not the conditions in Ireland, but rather the can-do spirit of the times that led people to devise a number of illogical schemes that would purportedly solve social and economic ills. Swift was especially attacking projects that tried to fix population and labour issues with a simple cure-all solution. A memorable example of these sorts of schemes "involved the idea of running the poor through a joint-stock company". In response, Swift's "Modest Proposal" was "a burlesque of projects concerning the poor" that were in vogue during the early 18th century.
Ian McBride argues that the point of "A Modest Proposal" was to "find a suitably decisive means of dehumanizing the settlers who had failed so comprehensively to meet their social responsibilities." 
"A Modest Proposal" also targets the calculating way people perceived the poor in designing their projects. The pamphlet targets reformers who "regard people as commodities". In the piece, Swift adopts the "technique of a political arithmetician" to show the utter ridiculousness of trying to prove any proposal with dispassionate statistics.
Critics differ about Swift's intentions in using this faux-mathematical philosophy. Edmund Wilson argues that statistically "the logic of the 'Modest proposal' can be compared with defence of crime (arrogated to Marx) in which he argues that crime takes care of the superfluous population". Wittkowsky counters that Swift's satiric use of statistical analysis is an effort to enhance his satire that "springs from a spirit of bitter mockery, not from the delight in calculations for their own sake".
Rhetoric.
Author Charles K. Smith argues that Swift's rhetorical style persuades the reader to detest the speaker and pity the Irish. Swift's specific strategy is twofold, using a "trap" to create sympathy for the Irish and a dislike of the narrator who, in the span of one sentence, "details vividly and with rhetorical emphasis the grinding poverty" but feels emotion solely for members of his own class. Swift's use of gripping details of poverty and his narrator's cool approach towards them create "two opposing points of view" that "alienate the reader, perhaps unconsciously, from a narrator who can view with 'melancholy' detachment a subject that Swift has directed us, rhetorically, to see in a much less detached way."
Swift has his proposer further degrade the Irish by using language ordinarily reserved for animals. Lewis argues that the speaker uses "the vocabulary of animal husbandry" to describe the Irish. Once the children have been commodified, Swift's rhetoric can easily turn "people into animals, then meat, and from meat, logically, into tonnage worth a price per pound".
Swift uses the proposer's serious tone to highlight the absurdity of his proposal. In making his argument, the speaker uses the conventional, textbook-approved order of argument from Swift's time (which was derived from the Latin rhetorician Quintilian). The contrast between the "careful control against the almost inconceivable perversion of his scheme" and "the ridiculousness of the proposal" create a situation in which the reader has "to consider just what perverted values and assumptions would allow such a diligent, thoughtful, and conventional man to propose so perverse a plan".
Influences.
Scholars have speculated about which earlier works Swift may have had in mind when he wrote "A Modest Proposal".
Tertullian's "Apology".
James William Johnson argues that "A Modest Proposal" was largely influenced and inspired by Tertullian's "Apology": a satirical attack against early Roman persecution of Christianity. Johnson believes that Swift saw major similarities between the two situations. Johnson notes Swift's obvious affinity for Tertullian and the bold stylistic and structural similarities between the works "A Modest Proposal" and "Apology". In structure, Johnson points out the same central theme, that of cannibalism and the eating of babies as well as the same final argument, that "human depravity is such that men will attempt to justify their own cruelty by accusing their victims of being lower than human". Stylistically, Swift and Tertullian share the same command of sarcasm and language. In agreement with Johnson, Donald C. Baker points out the similarity between both authors' tones and use of irony. Baker notes the uncanny way that both authors imply an ironic "justification by ownership" over the subject of sacrificing children—Tertullian while attacking pagan parents, and Swift while attacking the mistreatment of the poor in Ireland.
Defoe's "The Generous Projector".
It has also been argued that "A Modest Proposal" was, at least in part, a response to the 1728 essay "The Generous Projector or, A Friendly Proposal to Prevent Murder and Other Enormous Abuses, By Erecting an Hospital for Foundlings and Bastard Children" by Swift's rival Daniel Defoe.
Mandeville's "Modest Defence of Publick Stews".
Bernard Mandeville's "Modest Defence of Publick Stews" asked to introduce public and state-controlled bordellos. The 1726 paper acknowledges women's interests and—while not being a completely satirical text—has also been discussed as an inspiration for Jonathan Swift's title. Mandeville had by 1705 already become famous for "The Fable of the Bees" and deliberations on private vices and public benefits.
John Locke's "First Treatise of Government".
John Locke commented: "Be it then as Sir Robert says, that Anciently, it was usual for Men to sell and Castrate their Children. Let it be, that they exposed them; Add to it, if you please, for this is still greater Power, "that they begat them for their Tables to fat and eat them": If this proves a right to do so, we may, by the same Argument, justifie Adultery, Incest and Sodomy, for there are examples of these too, both Ancient and Modern; Sins, which I suppose, have the Principle Aggravation from this, that they cross the main intention of Nature, which willeth the increase of Mankind, and the continuation of the Species in the highest perfection, and the distinction of Families, with the Security of the Marriage Bed, as necessary thereunto". (First Treatise, sec. 59).
Economic themes.
Robert Phiddian's article "Have you eaten yet? The Reader in A Modest Proposal" focuses on two aspects of "A Modest Proposal": the voice of Swift and the voice of the Proposer. Phiddian stresses that a reader of the pamphlet must learn to distinguish between the satirical voice of Jonathan Swift and the apparent economic projections of the Proposer. He reminds readers that "there is a gap between the narrator's meaning and the text's, and that a moral-political argument is being carried out by means of parody".
While Swift's proposal is obviously not a serious economic proposal, George Wittkowsky, author of "Swift's Modest Proposal: The Biography of an Early Georgian Pamphlet", argues that to understand the piece fully it is important to understand the economics of Swift's time. Wittowsky argued that an insufficient number of critics have taken the time to focus directly on mercantilism and theories of labour in Georgian era Britain. "If one regards the "Modest Proposal" simply as a criticism of condition, about all one can say is that conditions were bad and that Swift's irony brilliantly underscored this fact".
"People are the riches of a nation".
At the start of a new industrial age in the 18th century, it was believed that "people are the riches of the nation", and there was a general faith in an economy that paid its workers low wages because high wages meant workers would work less. Furthermore, "in the mercantilist view no child was too young to go into industry". In those times, the "somewhat more humane attitudes of an earlier day had all but disappeared and the laborer had come to be regarded as a commodity".
Louis A. Landa composed a conducive analysis when he noted that it would have been healthier for the Irish economy to more appropriately utilize their human assets by giving the people an opportunity to "become a source of wealth to the nation" or else they "must turn to begging and thievery". This opportunity may have included giving the farmers more coin to work for, diversifying their professions, or even consider enslaving their people to lower coin usage and build up financial stock in Ireland. Landa wrote that, "Swift is maintaining that the maxim—people are the riches of a nation—applies to Ireland only if Ireland is permitted slavery or cannibalism"
Landa presents Swift's "A Modest Proposal" as a critique of the popular and unjustified maxim of mercantilism in the 18th century that "people are the riches of a nation". Swift presents the dire state of Ireland and shows that mere population itself, in Ireland's case, did not always mean greater wealth and economy. The uncontrolled maxim fails to take into account that a person who does not produce in an economic or political way makes a country poorer, not richer. Swift also recognises the implications of this fact in making mercantilist philosophy a paradox: the wealth of a country is based on the poverty of the majority of its citizens. Landa argued that Swift was putting the onus "on England of vitiating the working of natural economic law in Ireland" by denying Irishmen "the same natural rights common to the rest of mankind."
Public reaction.
Swift's essay created a backlash within Georgian society after its publication. The work was aimed at the elite, and they responded in turn. Several prominent members of society wrote to Swift regarding the work. Lord Bathurst's letter (12 February 1729–30) intimated that he certainly understood the message, and interpreted it as a work of comedy:
Modern usage.
"A Modest Proposal" is included in many literature courses as an example of early modern western satire. It also serves as an introduction to the concept and use of argumentative language, lending itself to secondary and post-secondary essay courses. Outside of the realm of English studies, "A Modest Proposal" is included in many comparative and global literature and history courses, as well as those of numerous other disciplines in the arts, humanities, and even the social sciences.
"A Modest Video Game Proposal" is the title of an open letter sent by activist/former attorney Jack Thompson on 10 October 2005.
The 2012 horror film "Butcher Boys", written by the original "The Texas Chain Saw Massacre" scribe Kim Henkel, is said to be an updating of Jonathan Swift's "A Modest Proposal". Henkel imagined the descendants of folks who actually took Swift up on his proposal. The film opens with a quote from J. Swift.
The 2023 song "Eat Your Young" written by Irish musician Hozier might be a reference to "A Modest Proposal". It combines themes regarding the anti-war and anti-income-inequality movement, and uses Swift's essay as a framework to compare those modern problems to those same problems during Swift's time.
The July 2023 Channel 4 mockumentary "", written by British comedy writer Matt Edmonds, updates "A Modest Proposal" and presents it in a similar format to Wallace's "Inside the Factory", with human meat given as a potential solution to the UK's cost of living crisis. The words "a modest proposal" are used in Wallace's summing up at the end of the programme, and Swift is credited.

</doc>
<doc id="666" url="?curid=666" title="Alkali metal">
Alkali metal

! colspan=2 style="text-align:left;" | ↓ 
! 2
! 3
! 4
! 5
! 6
! 7
"Legend"
The alkali metals consist of the chemical elements lithium (Li), sodium (Na), potassium (K), rubidium (Rb), caesium (Cs), and francium (Fr). Together with hydrogen they constitute group 1, which lies in the s-block of the periodic table. All alkali metals have their outermost electron in an s-orbital: this shared electron configuration results in their having very similar characteristic properties. Indeed, the alkali metals provide the best example of group trends in properties in the periodic table, with elements exhibiting well-characterised homologous behaviour. This family of elements is also known as the lithium family after its leading element.
The alkali metals are all shiny, soft, highly reactive metals at standard temperature and pressure and readily lose their outermost electron to form cations with charge +1. They can all be cut easily with a knife due to their softness, exposing a shiny surface that tarnishes rapidly in air due to oxidation by atmospheric moisture and oxygen (and in the case of lithium, nitrogen). Because of their high reactivity, they must be stored under oil to prevent reaction with air, and are found naturally only in salts and never as the free elements. Caesium, the fifth alkali metal, is the most reactive of all the metals. All the alkali metals react with water, with the heavier alkali metals reacting more vigorously than the lighter ones.
All of the discovered alkali metals occur in nature as their compounds: in order of abundance, sodium is the most abundant, followed by potassium, lithium, rubidium, caesium, and finally francium, which is very rare due to its extremely high radioactivity; francium occurs only in minute traces in nature as an intermediate step in some obscure side branches of the natural decay chains. Experiments have been conducted to attempt the synthesis of element 119, which is likely to be the next member of the group; none were successful. However, ununennium may not be an alkali metal due to relativistic effects, which are predicted to have a large influence on the chemical properties of superheavy elements; even if it does turn out to be an alkali metal, it is predicted to have some differences in physical and chemical properties from its lighter homologues.
Most alkali metals have many different applications. One of the best-known applications of the pure elements is the use of rubidium and caesium in atomic clocks, of which caesium atomic clocks form the basis of the second. A common application of the compounds of sodium is the sodium-vapour lamp, which emits light very efficiently. Table salt, or sodium chloride, has been used since antiquity. Lithium finds use as a psychiatric medication and as an anode in lithium batteries. Sodium, potassium and possibly lithium are essential elements, having major biological roles as electrolytes, and although the other alkali metals are not essential, they also have various effects on the body, both beneficial and harmful.
History.
Sodium compounds have been known since ancient times; salt (sodium chloride) has been an important commodity in human activities, as testified by the English word "salary", referring to "salarium", money paid to Roman soldiers for the purchase of salt. While potash has been used since ancient times, it was not understood for most of its history to be a fundamentally different substance from sodium mineral salts. Georg Ernst Stahl obtained experimental evidence which led him to suggest the fundamental difference of sodium and potassium salts in 1702, and Henri-Louis Duhamel du Monceau was able to prove this difference in 1736. The exact chemical composition of potassium and sodium compounds, and the status as chemical element of potassium and sodium, was not known then, and thus Antoine Lavoisier did not include either alkali in his list of chemical elements in 1789.
Pure potassium was first isolated in 1807 in England by Humphry Davy, who derived it from caustic potash (KOH, potassium hydroxide) by the use of electrolysis of the molten salt with the newly invented voltaic pile. Previous attempts at electrolysis of the aqueous salt were unsuccessful due to potassium's extreme reactivity. Potassium was the first metal that was isolated by electrolysis. Later that same year, Davy reported extraction of sodium from the similar substance caustic soda (NaOH, lye) by a similar technique, demonstrating the elements, and thus the salts, to be different.
Petalite () was discovered in 1800 by the Brazilian chemist José Bonifácio de Andrada in a mine on the island of Utö, Sweden. However, it was not until 1817 that Johan August Arfwedson, then working in the laboratory of the chemist Jöns Jacob Berzelius, detected the presence of a new element while analysing petalite ore. This new element was noted by him to form compounds similar to those of sodium and potassium, though its carbonate and hydroxide were less soluble in water and more alkaline than the other alkali metals. Berzelius gave the unknown material the name "lithion"/"lithina", from the Greek word "λιθoς" (transliterated as "lithos", meaning "stone"), to reflect its discovery in a solid mineral, as opposed to potassium, which had been discovered in plant ashes, and sodium, which was known partly for its high abundance in animal blood. He named the metal inside the material "lithium". Lithium, sodium, and potassium were part of the discovery of periodicity, as they are among a series of triads of elements in the same group that were noted by Johann Wolfgang Döbereiner in 1850 as having similar properties.
Rubidium and caesium were the first elements to be discovered using the spectroscope, invented in 1859 by Robert Bunsen and Gustav Kirchhoff. The next year, they discovered caesium in the mineral water from Bad Dürkheim, Germany. Their discovery of rubidium came the following year in Heidelberg, Germany, finding it in the mineral lepidolite. The names of rubidium and caesium come from the most prominent lines in their emission spectra: a bright red line for rubidium (from the Latin word "rubidus", meaning dark red or bright red), and a sky-blue line for caesium (derived from the Latin word "caesius", meaning sky-blue).
Around 1865 John Newlands produced a series of papers where he listed the elements in order of increasing atomic weight and similar physical and chemical properties that recurred at intervals of eight; he likened such periodicity to the octaves of music, where notes an octave apart have similar musical functions. His version put all the alkali metals then known (lithium to caesium), as well as copper, silver, and thallium (which show the +1 oxidation state characteristic of the alkali metals), together into a group. His table placed hydrogen with the halogens.
After 1869, Dmitri Mendeleev proposed his periodic table placing lithium at the top of a group with sodium, potassium, rubidium, caesium, and thallium. Two years later, Mendeleev revised his table, placing hydrogen in group 1 above lithium, and also moving thallium to the boron group. In this 1871 version, copper, silver, and gold were placed twice, once as part of group IB, and once as part of a "group VIII" encompassing today's groups 8 to 11. After the introduction of the 18-column table, the group IB elements were moved to their current position in the d-block, while alkali metals were left in "group IA". Later the group's name was changed to "group 1" in 1988. The trivial name "alkali metals" comes from the fact that the hydroxides of the group 1 elements are all strong alkalis when dissolved in water.
There were at least four erroneous and incomplete discoveries before Marguerite Perey of the Curie Institute in Paris, France discovered francium in 1939 by purifying a sample of actinium-227, which had been reported to have a decay energy of 220 keV. However, Perey noticed decay particles with an energy level below 80 keV. Perey thought this decay activity might have been caused by a previously unidentified decay product, one that was separated during purification, but emerged again out of the pure actinium-227. Various tests eliminated the possibility of the unknown element being thorium, radium, lead, bismuth, or thallium. The new product exhibited chemical properties of an alkali metal (such as coprecipitating with caesium salts), which led Perey to believe that it was element 87, caused by the alpha decay of actinium-227. Perey then attempted to determine the proportion of beta decay to alpha decay in actinium-227. Her first test put the alpha branching at 0.6%, a figure that she later revised to 1%.
The next element below francium (eka-francium) in the periodic table would be ununennium (Uue), element 119. The synthesis of ununennium was first attempted in 1985 by bombarding a target of einsteinium-254 with calcium-48 ions at the superHILAC accelerator at the Lawrence Berkeley National Laboratory in Berkeley, California. No atoms were identified, leading to a limiting yield of 300 nb.
It is highly unlikely that this reaction will be able to create any atoms of ununennium in the near future, given the extremely difficult task of making sufficient amounts of einsteinium-254, which is favoured for production of ultraheavy elements because of its large mass, relatively long half-life of 270 days, and availability in significant amounts of several micrograms, to make a large enough target to increase the sensitivity of the experiment to the required level; einsteinium has not been found in nature and has only been produced in laboratories, and in quantities smaller than those needed for effective synthesis of superheavy elements. However, given that ununennium is only the first period 8 element on the extended periodic table, it may well be discovered in the near future through other reactions, and indeed an attempt to synthesise it is currently ongoing in Japan. Currently, none of the period 8 elements has been discovered yet, and it is also possible, due to drip instabilities, that only the lower period 8 elements, up to around element 128, are physically possible. No attempts at synthesis have been made for any heavier alkali metals: due to their extremely high atomic number, they would require new, more powerful methods and technology to make.
Occurrence.
In the Solar System.
The Oddo–Harkins rule holds that elements with even atomic numbers are more common that those with odd atomic numbers, with the exception of hydrogen. This rule argues that elements with odd atomic numbers have one unpaired proton and are more likely to capture another, thus increasing their atomic number. In elements with even atomic numbers, protons are paired, with each member of the pair offsetting the spin of the other, enhancing stability. All the alkali metals have odd atomic numbers and they are not as common as the elements with even atomic numbers adjacent to them (the noble gases and the alkaline earth metals) in the Solar System. The heavier alkali metals are also less abundant than the lighter ones as the alkali metals from rubidium onward can only be synthesised in supernovae and not in stellar nucleosynthesis. Lithium is also much less abundant than sodium and potassium as it is poorly synthesised in both Big Bang nucleosynthesis and in stars: the Big Bang could only produce trace quantities of lithium, beryllium and boron due to the absence of a stable nucleus with 5 or 8 nucleons, and stellar nucleosynthesis could only pass this bottleneck by the triple-alpha process, fusing three helium nuclei to form carbon, and skipping over those three elements.
On Earth.
The Earth formed from the same cloud of matter that formed the Sun, but the planets acquired different compositions during the formation and evolution of the solar system. In turn, the natural history of the Earth caused parts of this planet to have differing concentrations of the elements. The mass of the Earth is approximately 5.98 kg. It is composed mostly of iron (32.1%), oxygen (30.1%), silicon (15.1%), magnesium (13.9%), sulfur (2.9%), nickel (1.8%), calcium (1.5%), and aluminium (1.4%); with the remaining 1.2% consisting of trace amounts of other elements. Due to planetary differentiation, the core region is believed to be primarily composed of iron (88.8%), with smaller amounts of nickel (5.8%), sulfur (4.5%), and less than 1% trace elements.
The alkali metals, due to their high reactivity, do not occur naturally in pure form in nature. They are lithophiles and therefore remain close to the Earth's surface because they combine readily with oxygen and so associate strongly with silica, forming relatively low-density minerals that do not sink down into the Earth's core. Potassium, rubidium and caesium are also incompatible elements due to their large ionic radii.
Sodium and potassium are very abundant on Earth, both being among the ten most common elements in Earth's crust; sodium makes up approximately 2.6% of the Earth's crust measured by weight, making it the sixth most abundant element overall and the most abundant alkali metal. Potassium makes up approximately 1.5% of the Earth's crust and is the seventh most abundant element. Sodium is found in many different minerals, of which the most common is ordinary salt (sodium chloride), which occurs in vast quantities dissolved in seawater. Other solid deposits include halite, amphibole, cryolite, nitratine, and zeolite. Many of these solid deposits occur as a result of ancient seas evaporating, which still occurs now in places such as Utah's Great Salt Lake and the Dead Sea. Despite their near-equal abundance in Earth's crust, sodium is far more common than potassium in the ocean, both because potassium's larger size makes its salts less soluble, and because potassium is bound by silicates in soil and what potassium leaches is absorbed far more readily by plant life than sodium.
Despite its chemical similarity, lithium typically does not occur together with sodium or potassium due to its smaller size. Due to its relatively low reactivity, it can be found in seawater in large amounts; it is estimated that lithium concentration in seawater is approximately 0.14 to 0.25 parts per million (ppm) or 25 micromolar. Its diagonal relationship with magnesium often allows it to replace magnesium in ferromagnesium minerals, where its crustal concentration is about 18 ppm, comparable to that of gallium and niobium. Commercially, the most important lithium mineral is spodumene, which occurs in large deposits worldwide.
Rubidium is approximately as abundant as zinc and more abundant than copper. It occurs naturally in the minerals leucite, pollucite, carnallite, zinnwaldite, and lepidolite, although none of these contain only rubidium and no other alkali metals. Caesium is more abundant than some commonly known elements, such as antimony, cadmium, tin, and tungsten, but is much less abundant than rubidium.
Francium-223, the only naturally occurring isotope of francium, is the product of the alpha decay of actinium-227 and can be found in trace amounts in uranium minerals. In a given sample of uranium, there is estimated to be only one francium atom for every 1018 uranium atoms. It has been calculated that there are at most 30 grams of francium in the earth's crust at any time, due to its extremely short half-life of 22 minutes.
Properties.
Physical and chemical.
The physical and chemical properties of the alkali metals can be readily explained by their having an ns1 valence electron configuration, which results in weak metallic bonding. Hence, all the alkali metals are soft and have low densities, melting and boiling points, as well as heats of sublimation, vaporisation, and dissociation. They all crystallise in the body-centered cubic crystal structure, and have distinctive flame colours because their outer s electron is very easily excited. Indeed, these flame test colours are the most common way of identifying them since all their salts with common ions are soluble. The ns1 configuration also results in the alkali metals having very large atomic and ionic radii, as well as very high thermal and electrical conductivity. Their chemistry is dominated by the loss of their lone valence electron in the outermost s-orbital to form the +1 oxidation state, due to the ease of ionising this electron and the very high second ionisation energy. Most of the chemistry has been observed only for the first five members of the group. The chemistry of francium is not well established due to its extreme radioactivity; thus, the presentation of its properties here is limited. What little is known about francium shows that it is very close in behaviour to caesium, as expected. The physical properties of francium are even sketchier because the bulk element has never been observed; hence any data that may be found in the literature are certainly speculative extrapolations.
The alkali metals are more similar to each other than the elements in any other group are to each other. Indeed, the similarity is so great that it is quite difficult to separate potassium, rubidium, and caesium, due to their similar ionic radii; lithium and sodium are more distinct. For instance, when moving down the table, all known alkali metals show increasing atomic radius, decreasing electronegativity, increasing reactivity, and decreasing melting and boiling points as well as heats of fusion and vaporisation. In general, their densities increase when moving down the table, with the exception that potassium is less dense than sodium. One of the very few properties of the alkali metals that does not display a very smooth trend is their reduction potentials: lithium's value is anomalous, being more negative than the others. This is because the Li+ ion has a very high hydration energy in the gas phase: though the lithium ion disrupts the structure of water significantly, causing a higher change in entropy, this high hydration energy is enough to make the reduction potentials indicate it as being the most electropositive alkali metal, despite the difficulty of ionising it in the gas phase.
The stable alkali metals are all silver-coloured metals except for caesium, which has a pale golden tint: it is one of only three metals that are clearly coloured (the other two being copper and gold). Additionally, the heavy alkaline earth metals calcium, strontium, and barium, as well as the divalent lanthanides europium and ytterbium, are pale yellow, though the colour is much less prominent than it is for caesium. Their lustre tarnishes rapidly in air due to oxidation.
All the alkali metals are highly reactive and are never found in elemental forms in nature. Because of this, they are usually stored in mineral oil or kerosene (paraffin oil). They react aggressively with the halogens to form the alkali metal halides, which are white ionic crystalline compounds that are all soluble in water except lithium fluoride (LiF). The alkali metals also react with water to form strongly alkaline hydroxides and thus should be handled with great care. The heavier alkali metals react more vigorously than the lighter ones; for example, when dropped into water, caesium produces a larger explosion than potassium if the same number of moles of each metal is used. The alkali metals have the lowest first ionisation energies in their respective periods of the periodic table because of their low effective nuclear charge and the ability to attain a noble gas configuration by losing just one electron. Not only do the alkali metals react with water, but also with proton donors like alcohols and phenols, gaseous ammonia, and alkynes, the last demonstrating the phenomenal degree of their reactivity. Their great power as reducing agents makes them very useful in liberating other metals from their oxides or halides.
The second ionisation energy of all of the alkali metals is very high as it is in a full shell that is also closer to the nucleus; thus, they almost always lose a single electron, forming cations. The alkalides are an exception: they are unstable compounds which contain alkali metals in a −1 oxidation state, which is very unusual as before the discovery of the alkalides, the alkali metals were not expected to be able to form anions and were thought to be able to appear in salts only as cations. The alkalide anions have filled s-subshells, which gives them enough stability to exist. All the stable alkali metals except lithium are known to be able to form alkalides, and the alkalides have much theoretical interest due to their unusual stoichiometry and low ionisation potentials. Alkalides are chemically similar to the electrides, which are salts with trapped electrons acting as anions. A particularly striking example of an alkalide is "inverse sodium hydride", H+Na− (both ions being complexed), as opposed to the usual sodium hydride, Na+H−: it is unstable in isolation, due to its high energy resulting from the displacement of two electrons from hydrogen to sodium, although several derivatives are predicted to be metastable or stable.
In aqueous solution, the alkali metal ions form aqua ions of the formula [M(H2O)"n"]+, where "n" is the solvation number. Their coordination numbers and shapes agree well with those expected from their ionic radii. In aqueous solution the water molecules directly attached to the metal ion are said to belong to the first coordination sphere, also known as the first, or primary, solvation shell. The bond between a water molecule and the metal ion is a dative covalent bond, with the oxygen atom donating both electrons to the bond. Each coordinated water molecule may be attached by hydrogen bonds to other water molecules. The latter are said to reside in the second coordination sphere. However, for the alkali metal cations, the second coordination sphere is not well-defined as the +1 charge on the cation is not high enough to polarise the water molecules in the primary solvation shell enough for them to form strong hydrogen bonds with those in the second coordination sphere, producing a more stable entity. The solvation number for Li+ has been experimentally determined to be 4, forming the tetrahedral [Li(H2O)4]+: while solvation numbers of 3 to 6 have been found for lithium aqua ions, solvation numbers less than 4 may be the result of the formation of contact ion pairs, and the higher solvation numbers may be interpreted in terms of water molecules that approach [Li(H2O)4]+ through a face of the tetrahedron, though molecular dynamic simulations may indicate the existence of an octahedral hexaaqua ion. There are also probably six water molecules in the primary solvation sphere of the sodium ion, forming the octahedral [Na(H2O)6]+ ion. While it was previously thought that the heavier alkali metals also formed octahedral hexaaqua ions, it has since been found that potassium and rubidium probably form the [K(H2O)8]+ and [Rb(H2O)8]+ ions, which have the square antiprismatic structure, and that caesium forms the 12-coordinate [Cs(H2O)12]+ ion.
Lithium.
The chemistry of lithium shows several differences from that of the rest of the group as the small Li+ cation polarises anions and gives its compounds a more covalent character. Lithium and magnesium have a diagonal relationship due to their similar atomic radii, so that they show some similarities. For example, lithium forms a stable nitride, a property common among all the alkaline earth metals (magnesium's group) but unique among the alkali metals. In addition, among their respective groups, only lithium and magnesium form organometallic compounds with significant covalent character (e.g. LiMe and MgMe2).
Lithium fluoride is the only alkali metal halide that is poorly soluble in water, and lithium hydroxide is the only alkali metal hydroxide that is not deliquescent. Conversely, lithium perchlorate and other lithium salts with large anions that cannot be polarised are much more stable than the analogous compounds of the other alkali metals, probably because Li+ has a high solvation energy. This effect also means that most simple lithium salts are commonly encountered in hydrated form, because the anhydrous forms are extremely hygroscopic: this allows salts like lithium chloride and lithium bromide to be used in dehumidifiers and air-conditioners.
Francium.
Francium is also predicted to show some differences due to its high atomic weight, causing its electrons to travel at considerable fractions of the speed of light and thus making relativistic effects more prominent. In contrast to the trend of decreasing electronegativities and ionisation energies of the alkali metals, francium's electronegativity and ionisation energy are predicted to be higher than caesium's due to the relativistic stabilisation of the 7s electrons; also, its atomic radius is expected to be abnormally low. Thus, contrary to expectation, caesium is the most reactive of the alkali metals, not francium. All known physical properties of francium also deviate from the clear trends going from lithium to caesium, such as the first ionisation energy, electron affinity, and anion polarisability, though due to the paucity of known data about francium many sources give extrapolated values, ignoring that relativistic effects make the trend from lithium to caesium become inapplicable at francium. Some of the few properties of francium that have been predicted taking relativity into account are the electron affinity (47.2 kJ/mol) and the enthalpy of dissociation of the Fr2 molecule (42.1 kJ/mol). The CsFr molecule is polarised as Cs+Fr−, showing that the 7s subshell of francium is much more strongly affected by relativistic effects than the 6s subshell of caesium. Additionally, francium superoxide (FrO2) is expected to have significant covalent character, unlike the other alkali metal superoxides, because of bonding contributions from the 6p electrons of francium.
Nuclear.
All the alkali metals have odd atomic numbers; hence, their isotopes must be either odd–odd (both proton and neutron number are odd) or odd–even (proton number is odd, but neutron number is even). Odd–odd nuclei have even mass numbers, whereas odd–even nuclei have odd mass numbers. Odd–odd primordial nuclides are rare because most odd–odd nuclei are highly unstable with respect to beta decay, because the decay products are even–even, and are therefore more strongly bound, due to nuclear pairing effects.
Due to the great rarity of odd–odd nuclei, almost all the primordial isotopes of the alkali metals are odd–even (the exceptions being the light stable isotope lithium-6 and the long-lived radioisotope potassium-40). For a given odd mass number, there can be only a single beta-stable nuclide, since there is not a difference in binding energy between even–odd and odd–even comparable to that between even–even and odd–odd, leaving other nuclides of the same mass number (isobars) free to beta decay toward the lowest-mass nuclide. An effect of the instability of an odd number of either type of nucleons is that odd-numbered elements, such as the alkali metals, tend to have fewer stable isotopes than even-numbered elements. Of the 26 monoisotopic elements that have only a single stable isotope, all but one have an odd atomic number and all but one also have an even number of neutrons. Beryllium is the single exception to both rules, due to its low atomic number.
All of the alkali metals except lithium and caesium have at least one naturally occurring radioisotope: sodium-22 and sodium-24 are trace radioisotopes produced cosmogenically, potassium-40 and rubidium-87 have very long half-lives and thus occur naturally, and all isotopes of francium are radioactive. Caesium was also thought to be radioactive in the early 20th century, although it has no naturally occurring radioisotopes. (Francium had not been discovered yet at that time.) The natural long-lived radioisotope of potassium, potassium-40, makes up about 0.012% of natural potassium, and thus natural potassium is weakly radioactive. This natural radioactivity became a basis for a mistaken claim of the discovery for element 87 (the next alkali metal after caesium) in 1925. Natural rubidium is similarly slightly radioactive, with 27.83% being the long-lived radioisotope rubidium-87.
Caesium-137, with a half-life of 30.17 years, is one of the two principal medium-lived fission products, along with strontium-90, which are responsible for most of the radioactivity of spent nuclear fuel after several years of cooling, up to several hundred years after use. It constitutes most of the radioactivity still left from the Chernobyl accident. Caesium-137 undergoes high-energy beta decay and eventually becomes stable barium-137. It is a strong emitter of gamma radiation. Caesium-137 has a very low rate of neutron capture and cannot be feasibly disposed of in this way, but must be allowed to decay. Caesium-137 has been used as a tracer in hydrologic studies, analogous to the use of tritium. Small amounts of caesium-134 and caesium-137 were released into the environment during nearly all nuclear weapon tests and some nuclear accidents, most notably the Goiânia accident and the Chernobyl disaster. As of 2005, caesium-137 is the principal source of radiation in the zone of alienation around the Chernobyl nuclear power plant. Its chemical properties as one of the alkali metals make it one of the most problematic of the short-to-medium-lifetime fission products because it easily moves and spreads in nature due to the high water solubility of its salts, and is taken up by the body, which mistakes it for its essential congeners sodium and potassium.
Periodic trends.
The alkali metals are more similar to each other than the elements in any other group are to each other. For instance, when moving down the table, all known alkali metals show increasing atomic radius, decreasing electronegativity, increasing reactivity, and decreasing melting and boiling points as well as heats of fusion and vaporisation. In general, their densities increase when moving down the table, with the exception that potassium is less dense than sodium.
Atomic and ionic radii.
The atomic radii of the alkali metals increase going down the group. Because of the shielding effect, when an atom has more than one electron shell, each electron feels electric repulsion from the other electrons as well as electric attraction from the nucleus. In the alkali metals, the outermost electron only feels a net charge of +1, as some of the nuclear charge (which is equal to the atomic number) is cancelled by the inner electrons; the number of inner electrons of an alkali metal is always one less than the nuclear charge. Therefore, the only factor which affects the atomic radius of the alkali metals is the number of electron shells. Since this number increases down the group, the atomic radius must also increase down the group.
The ionic radii of the alkali metals are much smaller than their atomic radii. This is because the outermost electron of the alkali metals is in a different electron shell than the inner electrons, and thus when it is removed the resulting atom has one fewer electron shell and is smaller. Additionally, the effective nuclear charge has increased, and thus the electrons are attracted more strongly towards the nucleus and the ionic radius decreases.
First ionisation energy.
The first ionisation energy of an element or molecule is the energy required to move the most loosely held electron from one mole of gaseous atoms of the element or molecules to form one mole of gaseous ions with electric charge +1. The factors affecting the first ionisation energy are the nuclear charge, the amount of shielding by the inner electrons and the distance from the most loosely held electron from the nucleus, which is always an outer electron in main group elements. The first two factors change the effective nuclear charge the most loosely held electron feels. Since the outermost electron of alkali metals always feels the same effective nuclear charge (+1), the only factor which affects the first ionisation energy is the distance from the outermost electron to the nucleus. Since this distance increases down the group, the outermost electron feels less attraction from the nucleus and thus the first ionisation energy decreases. This trend is broken in francium due to the relativistic stabilisation and contraction of the 7s orbital, bringing francium's valence electron closer to the nucleus than would be expected from non-relativistic calculations. This makes francium's outermost electron feel more attraction from the nucleus, increasing its first ionisation energy slightly beyond that of caesium.
The second ionisation energy of the alkali metals is much higher than the first as the second-most loosely held electron is part of a fully filled electron shell and is thus difficult to remove.
Reactivity.
The reactivities of the alkali metals increase going down the group. This is the result of a combination of two factors: the first ionisation energies and atomisation energies of the alkali metals. Because the first ionisation energy of the alkali metals decreases down the group, it is easier for the outermost electron to be removed from the atom and participate in chemical reactions, thus increasing reactivity down the group. The atomisation energy measures the strength of the metallic bond of an element, which falls down the group as the atoms increase in radius and thus the metallic bond must increase in length, making the delocalised electrons further away from the attraction of the nuclei of the heavier alkali metals. Adding the atomisation and first ionisation energies gives a quantity closely related to (but not equal to) the activation energy of the reaction of an alkali metal with another substance. This quantity decreases going down the group, and so does the activation energy; thus, chemical reactions can occur faster and the reactivity increases down the group.
Electronegativity.
Electronegativity is a chemical property that describes the tendency of an atom or a functional group to attract electrons (or electron density) towards itself. If the bond between sodium and chlorine in sodium chloride were covalent, the pair of shared electrons would be attracted to the chlorine because the effective nuclear charge on the outer electrons is +7 in chlorine but is only +1 in sodium. The electron pair is attracted so close to the chlorine atom that they are practically transferred to the chlorine atom (an ionic bond). However, if the sodium atom was replaced by a lithium atom, the electrons will not be attracted as close to the chlorine atom as before because the lithium atom is smaller, making the electron pair more strongly attracted to the closer effective nuclear charge from lithium. Hence, the larger alkali metal atoms (further down the group) will be less electronegative as the bonding pair is less strongly attracted towards them. As mentioned previously, francium is expected to be an exception.
Because of the higher electronegativity of lithium, some of its compounds have a more covalent character. For example, lithium iodide (LiI) will dissolve in organic solvents, a property of most covalent compounds. Lithium fluoride (LiF) is the only alkali halide that is not soluble in water, and lithium hydroxide (LiOH) is the only alkali metal hydroxide that is not deliquescent.
Melting and boiling points.
The melting point of a substance is the point where it changes state from solid to liquid while the boiling point of a substance (in liquid state) is the point where the vapour pressure of the liquid equals the environmental pressure surrounding the liquid and all the liquid changes state to gas. As a metal is heated to its melting point, the metallic bonds keeping the atoms in place weaken so that the atoms can move around, and the metallic bonds eventually break completely at the metal's boiling point. Therefore, the falling melting and boiling points of the alkali metals indicate that the strength of the metallic bonds of the alkali metals decreases down the group. This is because metal atoms are held together by the electromagnetic attraction from the positive ions to the delocalised electrons. As the atoms increase in size going down the group (because their atomic radius increases), the nuclei of the ions move further away from the delocalised electrons and hence the metallic bond becomes weaker so that the metal can more easily melt and boil, thus lowering the melting and boiling points. The increased nuclear charge is not a relevant factor due to the shielding effect.
Density.
The alkali metals all have the same crystal structure (body-centred cubic) and thus the only relevant factors are the number of atoms that can fit into a certain volume and the mass of one of the atoms, since density is defined as mass per unit volume. The first factor depends on the volume of the atom and thus the atomic radius, which increases going down the group; thus, the volume of an alkali metal atom increases going down the group. The mass of an alkali metal atom also increases going down the group. Thus, the trend for the densities of the alkali metals depends on their atomic weights and atomic radii; if figures for these two factors are known, the ratios between the densities of the alkali metals can then be calculated. The resultant trend is that the densities of the alkali metals increase down the table, with an exception at potassium. Due to having the lowest atomic weight and the largest atomic radius of all the elements in their periods, the alkali metals are the least dense metals in the periodic table. Lithium, sodium, and potassium are the only three metals in the periodic table that are less dense than water: in fact, lithium is the least dense known solid at room temperature.
Compounds.
The alkali metals form complete series of compounds with all usually encountered anions, which well illustrate group trends. These compounds can be described as involving the alkali metals losing electrons to acceptor species and forming monopositive ions. This description is most accurate for alkali halides and becomes less and less accurate as cationic and anionic charge increase, and as the anion becomes larger and more polarisable. For instance, ionic bonding gives way to metallic bonding along the series NaCl, Na2O, Na2S, Na3P, Na3As, Na3Sb, Na3Bi, Na.
Hydroxides.
All the alkali metals react vigorously or explosively with cold water, producing an aqueous solution of a strongly basic alkali metal hydroxide and releasing hydrogen gas. This reaction becomes more vigorous going down the group: lithium reacts steadily with effervescence, but sodium and potassium can ignite, and rubidium and caesium sink in water and generate hydrogen gas so rapidly that shock waves form in the water that may shatter glass containers. When an alkali metal is dropped into water, it produces an explosion, of which there are two separate stages. The metal reacts with the water first, breaking the hydrogen bonds in the water and producing hydrogen gas; this takes place faster for the more reactive heavier alkali metals. Second, the heat generated by the first part of the reaction often ignites the hydrogen gas, causing it to burn explosively into the surrounding air. This secondary hydrogen gas explosion produces the visible flame above the bowl of water, lake or other body of water, not the initial reaction of the metal with water (which tends to happen mostly under water). The alkali metal hydroxides are the most basic known hydroxides.
Recent research has suggested that the explosive behavior of alkali metals in water is driven by a Coulomb explosion rather than solely by rapid generation of hydrogen itself. All alkali metals melt as a part of the reaction with water. Water molecules ionise the bare metallic surface of the liquid metal, leaving a positively charged metal surface and negatively charged water ions. The attraction between the charged metal and water ions will rapidly increase the surface area, causing an exponential increase of ionisation. When the repulsive forces within the liquid metal surface exceeds the forces of the surface tension, it vigorously explodes.
The hydroxides themselves are the most basic hydroxides known, reacting with acids to give salts and with alcohols to give oligomeric alkoxides. They easily react with carbon dioxide to form carbonates or bicarbonates, or with hydrogen sulfide to form sulfides or bisulfides, and may be used to separate thiols from petroleum. They react with amphoteric oxides: for example, the oxides of aluminium, zinc, tin, and lead react with the alkali metal hydroxides to give aluminates, zincates, stannates, and plumbates. Silicon dioxide is acidic, and thus the alkali metal hydroxides can also attack silicate glass.
Intermetallic compounds.
The alkali metals form many intermetallic compounds with each other and the elements from groups 2 to 13 in the periodic table of varying stoichiometries, such as the sodium amalgams with mercury, including Na5Hg8 and Na3Hg. Some of these have ionic characteristics: taking the alloys with gold, the most electronegative of metals, as an example, NaAu and KAu are metallic, but RbAu and CsAu are semiconductors. NaK is an alloy of sodium and potassium that is very useful because it is liquid at room temperature, although precautions must be taken due to its extreme reactivity towards water and air. The eutectic mixture melts at −12.6 °C. An alloy of 41% caesium, 47% sodium, and 12% potassium has the lowest known melting point of any metal or alloy, −78 °C.
Compounds with the group 13 elements.
The intermetallic compounds of the alkali metals with the heavier group 13 elements (aluminium, gallium, indium, and thallium), such as NaTl, are poor conductors or semiconductors, unlike the normal alloys with the preceding elements, implying that the alkali metal involved has lost an electron to the Zintl anions involved. Nevertheless, while the elements in group 14 and beyond tend to form discrete anionic clusters, group 13 elements tend to form polymeric ions with the alkali metal cations located between the giant ionic lattice. For example, NaTl consists of a polymeric anion (—Tl−—)n with a covalent diamond cubic structure with Na+ ions located between the anionic lattice. The larger alkali metals cannot fit similarly into an anionic lattice and tend to force the heavier group 13 elements to form anionic clusters.
Boron is a special case, being the only nonmetal in group 13. The alkali metal borides tend to be boron-rich, involving appreciable boron–boron bonding involving deltahedral structures, and are thermally unstable due to the alkali metals having a very high vapour pressure at elevated temperatures. This makes direct synthesis problematic because the alkali metals do not react with boron below 700 °C, and thus this must be accomplished in sealed containers with the alkali metal in excess. Furthermore, exceptionally in this group, reactivity with boron decreases down the group: lithium reacts completely at 700 °C, but sodium at 900 °C and potassium not until 1200 °C, and the reaction is instantaneous for lithium but takes hours for potassium. Rubidium and caesium borides have not even been characterised. Various phases are known, such as LiB10, NaB6, NaB15, and KB6. Under high pressure the boron–boron bonding in the lithium borides changes from following Wade's rules to forming Zintl anions like the rest of group 13.
Compounds with the group 14 elements.
Lithium and sodium react with carbon to form acetylides, Li2C2 and Na2C2, which can also be obtained by reaction of the metal with acetylene. Potassium, rubidium, and caesium react with graphite; their atoms are intercalated between the hexagonal graphite layers, forming graphite intercalation compounds of formulae MC60 (dark grey, almost black), MC48 (dark grey, almost black), MC36 (blue), MC24 (steel blue), and MC8 (bronze) (M = K, Rb, or Cs). These compounds are over 200 times more electrically conductive than pure graphite, suggesting that the valence electron of the alkali metal is transferred to the graphite layers (e.g. ). Upon heating of KC8, the elimination of potassium atoms results in the conversion in sequence to KC24, KC36, KC48 and finally KC60. KC8 is a very strong reducing agent and is pyrophoric and explodes on contact with water. While the larger alkali metals (K, Rb, and Cs) initially form MC8, the smaller ones initially form MC6, and indeed they require reaction of the metals with graphite at high temperatures around 500 °C to form. Apart from this, the alkali metals are such strong reducing agents that they can even reduce buckminsterfullerene to produce solid fullerides M"n"C60; sodium, potassium, rubidium, and caesium can form fullerides where "n" = 2, 3, 4, or 6, and rubidium and caesium additionally can achieve "n" = 1.
When the alkali metals react with the heavier elements in the carbon group (silicon, germanium, tin, and lead), ionic substances with cage-like structures are formed, such as the silicides M4Si4 (M = K, Rb, or Cs), which contains M+ and tetrahedral ions. The chemistry of alkali metal germanides, involving the germanide ion Ge4− and other cluster (Zintl) ions such as , , , and [(Ge9)2]6−, is largely analogous to that of the corresponding silicides. Alkali metal stannides are mostly ionic, sometimes with the stannide ion (Sn4−), and sometimes with more complex Zintl ions such as , which appears in tetrapotassium nonastannide (K4Sn9). The monatomic plumbide ion (Pb4−) is unknown, and indeed its formation is predicted to be energetically unfavourable; alkali metal plumbides have complex Zintl ions, such as . These alkali metal germanides, stannides, and plumbides may be produced by reducing germanium, tin, and lead with sodium metal in liquid ammonia.
Nitrides and pnictides.
Lithium, the lightest of the alkali metals, is the only alkali metal which reacts with nitrogen at standard conditions, and its nitride is the only stable alkali metal nitride. Nitrogen is an unreactive gas because breaking the strong triple bond in the dinitrogen molecule (N2) requires a lot of energy. The formation of an alkali metal nitride would consume the ionisation energy of the alkali metal (forming M+ ions), the energy required to break the triple bond in N2 and the formation of N3− ions, and all the energy released from the formation of an alkali metal nitride is from the lattice energy of the alkali metal nitride. The lattice energy is maximised with small, highly charged ions; the alkali metals do not form highly charged ions, only forming ions with a charge of +1, so only lithium, the smallest alkali metal, can release enough lattice energy to make the reaction with nitrogen exothermic, forming lithium nitride. The reactions of the other alkali metals with nitrogen would not release enough lattice energy and would thus be endothermic, so they do not form nitrides at standard conditions. Sodium nitride (Na3N) and potassium nitride (K3N), while existing, are extremely unstable, being prone to decomposing back into their constituent elements, and cannot be produced by reacting the elements with each other at standard conditions. Steric hindrance forbids the existence of rubidium or caesium nitride. However, sodium and potassium form colourless azide salts involving the linear anion; due to the large size of the alkali metal cations, they are thermally stable enough to be able to melt before decomposing.
All the alkali metals react readily with phosphorus and arsenic to form phosphides and arsenides with the formula M3Pn (where M represents an alkali metal and Pn represents a pnictogen – phosphorus, arsenic, antimony, or bismuth). This is due to the greater size of the P3− and As3− ions, so that less lattice energy needs to be released for the salts to form. These are not the only phosphides and arsenides of the alkali metals: for example, potassium has nine different known phosphides, with formulae K3P, K4P3, K5P4, KP, K4P6, K3P7, K3P11, KP10.3, and KP15. While most metals form arsenides, only the alkali and alkaline earth metals form mostly ionic arsenides. The structure of Na3As is complex with unusually short Na–Na distances of 328–330 pm which are shorter than in sodium metal, and this indicates that even with these electropositive metals the bonding cannot be straightforwardly ionic. Other alkali metal arsenides not conforming to the formula M3As are known, such as LiAs, which has a metallic lustre and electrical conductivity indicating the presence of some metallic bonding. The antimonides are unstable and reactive as the Sb3− ion is a strong reducing agent; reaction of them with acids form the toxic and unstable gas stibine (SbH3). Indeed, they have some metallic properties, and the alkali metal antimonides of stoichiometry MSb involve antimony atoms bonded in a spiral Zintl structure. Bismuthides are not even wholly ionic; they are intermetallic compounds containing partially metallic and partially ionic bonds.
Oxides and chalcogenides.
All the alkali metals react vigorously with oxygen at standard conditions. They form various types of oxides, such as simple oxides (containing the O2− ion), peroxides (containing the ion, where there is a single bond between the two oxygen atoms), superoxides (containing the ion), and many others. Lithium burns in air to form lithium oxide, but sodium reacts with oxygen to form a mixture of sodium oxide and sodium peroxide. Potassium forms a mixture of potassium peroxide and potassium superoxide, while rubidium and caesium form the superoxide exclusively. Their reactivity increases going down the group: while lithium, sodium and potassium merely burn in air, rubidium and caesium are pyrophoric (spontaneously catch fire in air).
The smaller alkali metals tend to polarise the larger anions (the peroxide and superoxide) due to their small size. This attracts the electrons in the more complex anions towards one of its constituent oxygen atoms, forming an oxide ion and an oxygen atom. This causes lithium to form the oxide exclusively on reaction with oxygen at room temperature. This effect becomes drastically weaker for the larger sodium and potassium, allowing them to form the less stable peroxides. Rubidium and caesium, at the bottom of the group, are so large that even the least stable superoxides can form. Because the superoxide releases the most energy when formed, the superoxide is preferentially formed for the larger alkali metals where the more complex anions are not polarised. The oxides and peroxides for these alkali metals do exist, but do not form upon direct reaction of the metal with oxygen at standard conditions. In addition, the small size of the Li+ and O2− ions contributes to their forming a stable ionic lattice structure. Under controlled conditions, however, all the alkali metals, with the exception of francium, are known to form their oxides, peroxides, and superoxides. The alkali metal peroxides and superoxides are powerful oxidising agents. Sodium peroxide and potassium superoxide react with carbon dioxide to form the alkali metal carbonate and oxygen gas, which allows them to be used in submarine air purifiers; the presence of water vapour, naturally present in breath, makes the removal of carbon dioxide by potassium superoxide even more efficient. All the stable alkali metals except lithium can form red ozonides (MO3) through low-temperature reaction of the powdered anhydrous hydroxide with ozone: the ozonides may be then extracted using liquid ammonia. They slowly decompose at standard conditions to the superoxides and oxygen, and hydrolyse immediately to the hydroxides when in contact with water. Potassium, rubidium, and caesium also form sesquioxides M2O3, which may be better considered peroxide disuperoxides, .
Rubidium and caesium can form a great variety of suboxides with the metals in formal oxidation states below +1. Rubidium can form Rb6O and Rb9O2 (copper-coloured) upon oxidation in air, while caesium forms an immense variety of oxides, such as the ozonide CsO3 and several brightly coloured suboxides, such as Cs7O (bronze), Cs4O (red-violet), Cs11O3 (violet), Cs3O (dark green), CsO, Cs3O2, as well as Cs7O2. The last of these may be heated under vacuum to generate Cs2O.
The alkali metals can also react analogously with the heavier chalcogens (sulfur, selenium, tellurium, and polonium), and all the alkali metal chalcogenides are known (with the exception of francium's). Reaction with an excess of the chalcogen can similarly result in lower chalcogenides, with chalcogen ions containing chains of the chalcogen atoms in question. For example, sodium can react with sulfur to form the sulfide (Na2S) and various polysulfides with the formula Na2S"x" ("x" from 2 to 6), containing the ions. Due to the basicity of the Se2− and Te2− ions, the alkali metal selenides and tellurides are alkaline in solution; when reacted directly with selenium and tellurium, alkali metal polyselenides and polytellurides are formed along with the selenides and tellurides with the and ions. They may be obtained directly from the elements in liquid ammonia or when air is not present, and are colourless, water-soluble compounds that air oxidises quickly back to selenium or tellurium. The alkali metal polonides are all ionic compounds containing the Po2− ion; they are very chemically stable and can be produced by direct reaction of the elements at around 300–400 °C.
Halides, hydrides, and pseudohalides.
The alkali metals are among the most electropositive elements on the periodic table and thus tend to bond ionically to the most electronegative elements on the periodic table, the halogens (fluorine, chlorine, bromine, iodine, and astatine), forming salts known as the alkali metal halides. The reaction is very vigorous and can sometimes result in explosions. All twenty stable alkali metal halides are known; the unstable ones are not known, with the exception of sodium astatide, because of the great instability and rarity of astatine and francium. The most well-known of the twenty is certainly sodium chloride, otherwise known as common salt. All of the stable alkali metal halides have the formula MX where M is an alkali metal and X is a halogen. They are all white ionic crystalline solids that have high melting points. All the alkali metal halides are soluble in water except for lithium fluoride (LiF), which is insoluble in water due to its very high lattice enthalpy. The high lattice enthalpy of lithium fluoride is due to the small sizes of the Li+ and F− ions, causing the electrostatic interactions between them to be strong: a similar effect occurs for magnesium fluoride, consistent with the diagonal relationship between lithium and magnesium.
The alkali metals also react similarly with hydrogen to form ionic alkali metal hydrides, where the hydride anion acts as a pseudohalide: these are often used as reducing agents, producing hydrides, complex metal hydrides, or hydrogen gas. Other pseudohalides are also known, notably the cyanides. These are isostructural to the respective halides except for lithium cyanide, indicating that the cyanide ions may rotate freely. Ternary alkali metal halide oxides, such as Na3ClO, K3BrO (yellow), Na4Br2O, Na4I2O, and K4Br2O, are also known. The polyhalides are rather unstable, although those of rubidium and caesium are greatly stabilised by the feeble polarising power of these extremely large cations.
Coordination complexes.
Alkali metal cations do not usually form coordination complexes with simple Lewis bases due to their low charge of just +1 and their relatively large size; thus the Li+ ion forms most complexes and the heavier alkali metal ions form less and less (though exceptions occur for weak complexes). Lithium in particular has a very rich coordination chemistry in which it exhibits coordination numbers from 1 to 12, although octahedral hexacoordination is its preferred mode. In aqueous solution, the alkali metal ions exist as octahedral hexahydrate complexes [M(H2O)6]+, with the exception of the lithium ion, which due to its small size forms tetrahedral tetrahydrate complexes [Li(H2O)4]+; the alkali metals form these complexes because their ions are attracted by electrostatic forces of attraction to the polar water molecules. Because of this, anhydrous salts containing alkali metal cations are often used as desiccants. Alkali metals also readily form complexes with crown ethers (e.g. 12-crown-4 for Li+, 15-crown-5 for Na+, 18-crown-6 for K+, and 21-crown-7 for Rb+) and cryptands due to electrostatic attraction.
Ammonia solutions.
The alkali metals dissolve slowly in liquid ammonia, forming ammoniacal solutions of solvated metal cation M+ and solvated electron e−, which react to form hydrogen gas and the alkali metal amide (MNH2, where M represents an alkali metal): this was first noted by Humphry Davy in 1809 and rediscovered by W. Weyl in 1864. The process may be speeded up by a catalyst. Similar solutions are formed by the heavy divalent alkaline earth metals calcium, strontium, barium, as well as the divalent lanthanides, europium and ytterbium. The amide salt is quite insoluble and readily precipitates out of solution, leaving intensely coloured ammonia solutions of the alkali metals. In 1907, Charles A. Kraus identified the colour as being due to the presence of solvated electrons, which contribute to the high electrical conductivity of these solutions. At low concentrations (below 3 M), the solution is dark blue and has ten times the conductivity of aqueous sodium chloride; at higher concentrations (above 3 M), the solution is copper-coloured and has approximately the conductivity of liquid metals like mercury. In addition to the alkali metal amide salt and solvated electrons, such ammonia solutions also contain the alkali metal cation (M+), the neutral alkali metal atom (M), diatomic alkali metal molecules (M2) and alkali metal anions (M−). These are unstable and eventually become the more thermodynamically stable alkali metal amide and hydrogen gas. Solvated electrons are powerful reducing agents and are often used in chemical synthesis.
Organometallic.
Organolithium.
Being the smallest alkali metal, lithium forms the widest variety of and most stable organometallic compounds, which are bonded covalently. Organolithium compounds are electrically non-conducting volatile solids or liquids that melt at low temperatures, and tend to form oligomers with the structure (RLi)"x" where R is the organic group. As the electropositive nature of lithium puts most of the charge density of the bond on the carbon atom, effectively creating a carbanion, organolithium compounds are extremely powerful bases and nucleophiles. For use as bases, butyllithiums are often used and are commercially available. An example of an organolithium compound is methyllithium ((CH3Li)"x"), which exists in tetrameric ("x" = 4, tetrahedral) and hexameric ("x" = 6, octahedral) forms. Organolithium compounds, especially "n"-butyllithium, are useful reagents in organic synthesis, as might be expected given lithium's diagonal relationship with magnesium, which plays an important role in the Grignard reaction. For example, alkyllithiums and aryllithiums may be used to synthesise aldehydes and ketones by reaction with metal carbonyls. The reaction with nickel tetracarbonyl, for example, proceeds through an unstable acyl nickel carbonyl complex which then undergoes electrophilic substitution to give the desired aldehyde (using H+ as the electrophile) or ketone (using an alkyl halide) product.
Alkyllithiums and aryllithiums may also react with "N","N"-disubstituted amides to give aldehydes and ketones, and symmetrical ketones by reacting with carbon monoxide. They thermally decompose to eliminate a β-hydrogen, producing alkenes and lithium hydride: another route is the reaction of ethers with alkyl- and aryllithiums that act as strong bases. In non-polar solvents, aryllithiums react as the carbanions they effectively are, turning carbon dioxide to aromatic carboxylic acids (ArCO2H) and aryl ketones to tertiary carbinols (Ar'2C(Ar)OH). Finally, they may be used to synthesise other organometallic compounds through metal-halogen exchange.
Heavier alkali metals.
Unlike the organolithium compounds, the organometallic compounds of the heavier alkali metals are predominantly ionic. The application of organosodium compounds in chemistry is limited in part due to competition from organolithium compounds, which are commercially available and exhibit more convenient reactivity. The principal organosodium compound of commercial importance is sodium cyclopentadienide. Sodium tetraphenylborate can also be classified as an organosodium compound since in the solid state sodium is bound to the aryl groups. Organometallic compounds of the higher alkali metals are even more reactive than organosodium compounds and of limited utility. A notable reagent is Schlosser's base, a mixture of "n"-butyllithium and potassium "tert"-butoxide. This reagent reacts with propene to form the compound allylpotassium (KCH2CHCH2). "cis"-2-Butene and "trans"-2-butene equilibrate when in contact with alkali metals. Whereas isomerisation is fast with lithium and sodium, it is slow with the heavier alkali metals. The heavier alkali metals also favour the sterically congested conformation. Several crystal structures of organopotassium compounds have been reported, establishing that they, like the sodium compounds, are polymeric. Organosodium, organopotassium, organorubidium and organocaesium compounds are all mostly ionic and are insoluble (or nearly so) in nonpolar solvents.
Alkyl and aryl derivatives of sodium and potassium tend to react with air. They cause the cleavage of ethers, generating alkoxides. Unlike alkyllithium compounds, alkylsodiums and alkylpotassiums cannot be made by reacting the metals with alkyl halides because Wurtz coupling occurs:
As such, they have to be made by reacting alkylmercury compounds with sodium or potassium metal in inert hydrocarbon solvents. While methylsodium forms tetramers like methyllithium, methylpotassium is more ionic and has the nickel arsenide structure with discrete methyl anions and potassium cations.
The alkali metals and their hydrides react with acidic hydrocarbons, for example cyclopentadienes and terminal alkynes, to give salts. Liquid ammonia, ether, or hydrocarbon solvents are used, the most common of which being tetrahydrofuran. The most important of these compounds is sodium cyclopentadienide, NaC5H5, an important precursor to many transition metal cyclopentadienyl derivatives. Similarly, the alkali metals react with cyclooctatetraene in tetrahydrofuran to give alkali metal cyclooctatetraenides; for example, dipotassium cyclooctatetraenide (K2C8H8) is an important precursor to many metal cyclooctatetraenyl derivatives, such as uranocene. The large and very weakly polarising alkali metal cations can stabilise large, aromatic, polarisable radical anions, such as the dark-green sodium naphthalenide, Na+[C10H8•]−, a strong reducing agent.
Representative reactions of alkali metals.
Reaction with oxygen.
Upon reacting with oxygen, alkali metals form oxides, peroxides, superoxides and suboxides. However, the first three are more common. The table below shows the types of compounds formed in reaction with oxygen. The compound in brackets represents the minor product of combustion.
The alkali metal peroxides are ionic compounds that are unstable in water. The peroxide anion is weakly bound to the cation, and it is hydrolysed, forming stronger covalent bonds.
The other oxygen compounds are also unstable in water.
Reaction with sulfur.
With sulfur, they form sulfides and polysulfides.
Because alkali metal sulfides are essentially salts of a weak acid and a strong base, they form basic solutions.
Reaction with nitrogen.
Lithium is the only metal that combines directly with nitrogen at room temperature.
Li3N can react with water to liberate ammonia.
Reaction with hydrogen.
With hydrogen, alkali metals form saline hydrides that hydrolyse in water. 
Reaction with carbon.
Lithium is the only metal that reacts directly with carbon to give dilithium acetylide. Na and K can react with acetylene to give acetylides.
Reaction with water.
On reaction with water, they generate hydroxide ions and hydrogen gas. This reaction is vigorous and highly exothermic and the hydrogen resulted may ignite in air or even explode in the case of Rb and Cs.
Reaction with other salts.
The alkali metals are very good reducing agents. They can reduce metal cations that are less electropositive. Titanium is produced industrially by the reduction of titanium tetrachloride with Na at 400 °C (van Arkel–de Boer process).
Reaction with organohalide compounds.
Alkali metals react with halogen derivatives to generate hydrocarbon via the Wurtz reaction.
Alkali metals in liquid ammonia.
Alkali metals dissolve in liquid ammonia or other donor solvents like aliphatic amines or hexamethylphosphoramide to give blue solutions. These solutions are believed to contain free electrons.
Due to the presence of solvated electrons, these solutions are very powerful reducing agents used in organic synthesis.
Reaction 1) is known as Birch reduction.
Other reductions that can be carried by these solutions are:
Extensions.
Although francium is the heaviest alkali metal that has been discovered, there has been some theoretical work predicting the physical and chemical characteristics of hypothetical heavier alkali metals. Being the first period 8 element, the undiscovered element ununennium (element 119) is predicted to be the next alkali metal after francium and behave much like their lighter congeners; however, it is also predicted to differ from the lighter alkali metals in some properties. Its chemistry is predicted to be closer to that of potassium or rubidium instead of caesium or francium. This is unusual as periodic trends, ignoring relativistic effects would predict ununennium to be even more reactive than caesium and francium. This lowered reactivity is due to the relativistic stabilisation of ununennium's valence electron, increasing ununennium's first ionisation energy and decreasing the metallic and ionic radii; this effect is already seen for francium. This assumes that ununennium will behave chemically as an alkali metal, which, although likely, may not be true due to relativistic effects. The relativistic stabilisation of the 8s orbital also increases ununennium's electron affinity far beyond that of caesium and francium; indeed, ununennium is expected to have an electron affinity higher than all the alkali metals lighter than it. Relativistic effects also cause a very large drop in the polarisability of ununennium. On the other hand, ununennium is predicted to continue the trend of melting points decreasing going down the group, being expected to have a melting point between 0 °C and 30 °C.
The stabilisation of ununennium's valence electron and thus the contraction of the 8s orbital cause its atomic radius to be lowered to 240 pm, very close to that of rubidium (247 pm), so that the chemistry of ununennium in the +1 oxidation state should be more similar to the chemistry of rubidium than to that of francium. On the other hand, the ionic radius of the Uue+ ion is predicted to be larger than that of Rb+, because the 7p orbitals are destabilised and are thus larger than the p-orbitals of the lower shells. Ununennium may also show the +3 and +5 oxidation states, which are not seen in any other alkali metal, in addition to the +1 oxidation state that is characteristic of the other alkali metals and is also the main oxidation state of all the known alkali metals: this is because of the destabilisation and expansion of the 7p3/2 spinor, causing its outermost electrons to have a lower ionisation energy than what would otherwise be expected. Indeed, many ununennium compounds are expected to have a large covalent character, due to the involvement of the 7p3/2 electrons in the bonding.
Not as much work has been done predicting the properties of the alkali metals beyond ununennium. Although a simple extrapolation of the periodic table (by the Aufbau principle) would put element 169, unhexennium, under ununennium, Dirac-Fock calculations predict that the next element after ununennium with alkali-metal-like properties may be element 165, unhexpentium, which is predicted to have the electron configuration [Og] 5g18 6f14 7d10 8s2 8p1/22 9s1. This element would be intermediate in properties between an alkali metal and a group 11 element, and while its physical and atomic properties would be closer to the former, its chemistry may be closer to that of the latter. Further calculations show that unhexpentium would follow the trend of increasing ionisation energy beyond caesium, having an ionisation energy comparable to that of sodium, and that it should also continue the trend of decreasing atomic radii beyond caesium, having an atomic radius comparable to that of potassium. However, the 7d electrons of unhexpentium may also be able to participate in chemical reactions along with the 9s electron, possibly allowing oxidation states beyond +1, whence the likely transition metal behaviour of unhexpentium. Due to the alkali and alkaline earth metals both being s-block elements, these predictions for the trends and properties of ununennium and unhexpentium also mostly hold quite similarly for the corresponding alkaline earth metals unbinilium (Ubn) and unhexhexium (Uhh). Unsepttrium, element 173, may be an even better heavier homologue of ununennium; with a predicted electron configuration of [Usb] 6g1, it returns to the alkali-metal-like situation of having one easily removed electron far above a closed p-shell in energy, and is expected to be even more reactive than caesium.
The probable properties of further alkali metals beyond unsepttrium have not been explored yet as of 2019, and they may or may not be able to exist. In periods 8 and above of the periodic table, relativistic and shell-structure effects become so strong that extrapolations from lighter congeners become completely inaccurate. In addition, the relativistic and shell-structure effects (which stabilise the s-orbitals and destabilise and expand the d-, f-, and g-orbitals of higher shells) have opposite effects, causing even larger difference between relativistic and non-relativistic calculations of the properties of elements with such high atomic numbers. Interest in the chemical properties of ununennium, unhexpentium, and unsepttrium stems from the fact that they are located close to the expected locations of islands of stability, centered at elements 122 (306Ubb) and 164 (482Uhq).
Pseudo-alkali metals.
Many other substances are similar to the alkali metals in their tendency to form monopositive cations. Analogously to the pseudohalogens, they have sometimes been called "pseudo-alkali metals". These substances include some elements and many more polyatomic ions; the polyatomic ions are especially similar to the alkali metals in their large size and weak polarising power.
Hydrogen.
The element hydrogen, with one electron per neutral atom, is usually placed at the top of Group 1 of the periodic table because of its electron configuration. But hydrogen is not normally considered to be an alkali metal. Metallic hydrogen, which only exists at very high pressures, is known for its electrical and magnetic properties, not its chemical properties. Under typical conditions, pure hydrogen exists as a diatomic gas consisting of two atoms per molecule (H2); however, the alkali metals form diatomic molecules (such as dilithium, Li2) only at high temperatures, when they are in the gaseous state.
Hydrogen, like the alkali metals, has one valence electron and reacts easily with the halogens, but the similarities mostly end there because of the small size of a bare proton H+ compared to the alkali metal cations. Its placement above lithium is primarily due to its electron configuration. It is sometimes placed above fluorine due to their similar chemical properties, though the resemblance is likewise not absolute.
The first ionisation energy of hydrogen (1312.0 kJ/mol) is much higher than that of the alkali metals. As only one additional electron is required to fill in the outermost shell of the hydrogen atom, hydrogen often behaves like a halogen, forming the negative hydride ion, and is very occasionally considered to be a halogen on that basis. (The alkali metals can also form negative ions, known as alkalides, but these are little more than laboratory curiosities, being unstable.) An argument against this placement is that formation of hydride from hydrogen is endothermic, unlike the exothermic formation of halides from halogens. The radius of the H− anion also does not fit the trend of increasing size going down the halogens: indeed, H− is very diffuse because its single proton cannot easily control both electrons. It was expected for some time that liquid hydrogen would show metallic properties; while this has been shown to not be the case, under extremely high pressures, such as those found at the cores of Jupiter and Saturn, hydrogen does become metallic and behaves like an alkali metal; in this phase, it is known as metallic hydrogen. The electrical resistivity of liquid metallic hydrogen at 3000 K is approximately equal to that of liquid rubidium and caesium at 2000 K at the respective pressures when they undergo a nonmetal-to-metal transition.
The 1s1 electron configuration of hydrogen, while analogous to that of the alkali metals (ns1), is unique because there is no 1p subshell. Hence it can lose an electron to form the hydron H+, or gain one to form the hydride ion H−. In the former case it resembles superficially the alkali metals; in the latter case, the halogens, but the differences due to the lack of a 1p subshell are important enough that neither group fits the properties of hydrogen well. Group 14 is also a good fit in terms of thermodynamic properties such as ionisation energy and electron affinity, but hydrogen cannot be tetravalent. Thus none of the three placements are entirely satisfactory, although group 1 is the most common placement (if one is chosen) because of the electron configuration and the fact that the hydron is by far the most important of all monatomic hydrogen species, being the foundation of acid-base chemistry. As an example of hydrogen's unorthodox properties stemming from its unusual electron configuration and small size, the hydrogen ion is very small (radius around 150 fm compared to the 50–220 pm size of most other atoms and ions) and so is nonexistent in condensed systems other than in association with other atoms or molecules. Indeed, transferring of protons between chemicals is the basis of acid-base chemistry. Also unique is hydrogen's ability to form hydrogen bonds, which are an effect of charge-transfer, electrostatic, and electron correlative contributing phenomena. While analogous lithium bonds are also known, they are mostly electrostatic. Nevertheless, hydrogen can take on the same structural role as the alkali metals in some molecular crystals, and has a close relationship with the lightest alkali metals (especially lithium).
Ammonium and derivatives.
The ammonium ion () has very similar properties to the heavier alkali metals, acting as an alkali metal intermediate between potassium and rubidium, and is often considered a close relative. For example, most alkali metal salts are soluble in water, a property which ammonium salts share. Ammonium is expected to behave stably as a metal ( ions in a sea of delocalised electrons) at very high pressures (though less than the typical pressure where transitions from insulating to metallic behaviour occur around, 100 GPa), and could possibly occur inside the ice giants Uranus and Neptune, which may have significant impacts on their interior magnetic fields. It has been estimated that the transition from a mixture of ammonia and dihydrogen molecules to metallic ammonium may occur at pressures just below 25 GPa. Under standard conditions, ammonium can form a metallic amalgam with mercury.
Other "pseudo-alkali metals" include the alkylammonium cations, in which some of the hydrogen atoms in the ammonium cation are replaced by alkyl or aryl groups. In particular, the quaternary ammonium cations () are very useful since they are permanently charged, and they are often used as an alternative to the expensive Cs+ to stabilise very large and very easily polarisable anions such as . Tetraalkylammonium hydroxides, like alkali metal hydroxides, are very strong bases that react with atmospheric carbon dioxide to form carbonates. Furthermore, the nitrogen atom may be replaced by a phosphorus, arsenic, or antimony atom (the heavier nonmetallic pnictogens), creating a phosphonium () or arsonium () cation that can itself be substituted similarly; while stibonium () itself is not known, some of its organic derivatives are characterised.
Cobaltocene and derivatives.
Cobaltocene, Co(C5H5)2, is a metallocene, the cobalt analogue of ferrocene. It is a dark purple solid. Cobaltocene has 19 valence electrons, one more than usually found in organotransition metal complexes, such as its very stable relative, ferrocene, in accordance with the 18-electron rule. This additional electron occupies an orbital that is antibonding with respect to the Co–C bonds. Consequently, many chemical reactions of Co(C5H5)2 are characterized by its tendency to lose this "extra" electron, yielding a very stable 18-electron cation known as cobaltocenium. Many cobaltocenium salts coprecipitate with caesium salts, and cobaltocenium hydroxide is a strong base that absorbs atmospheric carbon dioxide to form cobaltocenium carbonate. Like the alkali metals, cobaltocene is a strong reducing agent, and decamethylcobaltocene is stronger still due to the combined inductive effect of the ten methyl groups. Cobalt may be substituted by its heavier congener rhodium to give rhodocene, an even stronger reducing agent. Iridocene (involving iridium) would presumably be still more potent, but is not very well-studied due to its instability.
Thallium.
Thallium is the heaviest stable element in group 13 of the periodic table. At the bottom of the periodic table, the inert-pair effect is quite strong, because of the relativistic stabilisation of the 6s orbital and the decreasing bond energy as the atoms increase in size so that the amount of energy released in forming two more bonds is not worth the high ionisation energies of the 6s electrons. It displays the +1 oxidation state that all the known alkali metals display, and thallium compounds with thallium in its +1 oxidation state closely resemble the corresponding potassium or silver compounds stoichiometrically due to the similar ionic radii of the Tl+ (164 pm), K+ (152 pm) and Ag+ (129 pm) ions. It was sometimes considered an alkali metal in continental Europe (but not in England) in the years immediately following its discovery, and was placed just after caesium as the sixth alkali metal in Dmitri Mendeleev's 1869 periodic table and Julius Lothar Meyer's 1868 periodic table. Mendeleev's 1871 periodic table and Meyer's 1870 periodic table put thallium in its current position in the boron group and left the space below caesium blank. However, thallium also displays the oxidation state +3, which no known alkali metal displays (although ununennium, the undiscovered seventh alkali metal, is predicted to possibly display the +3 oxidation state). The sixth alkali metal is now considered to be francium. While Tl+ is stabilised by the inert-pair effect, this inert pair of 6s electrons is still able to participate chemically, so that these electrons are stereochemically active in aqueous solution. Additionally, the thallium halides (except TlF) are quite insoluble in water, and TlI has an unusual structure because of the presence of the stereochemically active inert pair in thallium.
Copper, silver, and gold.
The group 11 metals (or coinage metals), copper, silver, and gold, are typically categorised as transition metals given they can form ions with incomplete d-shells. Physically, they have the relatively low melting points and high electronegativity values associated with post-transition metals. "The filled "d" subshell and free "s" electron of Cu, Ag, and Au contribute to their high electrical and thermal conductivity. Transition metals to the left of group 11 experience interactions between "s" electrons and the partially filled "d" subshell that lower electron mobility." Chemically, the group 11 metals behave like main-group metals in their +1 valence states, and are hence somewhat related to the alkali metals: this is one reason for their previously being labelled as "group IB", paralleling the alkali metals' "group IA". They are occasionally classified as post-transition metals. Their spectra are analogous to those of the alkali metals. Their monopositive ions are paramagnetic and contribute no colour to their salts, like those of the alkali metals.
In Mendeleev's 1871 periodic table, copper, silver, and gold are listed twice, once under group VIII (with the iron triad and platinum group metals), and once under group IB. Group IB was nonetheless parenthesised to note that it was tentative. Mendeleev's main criterion for group assignment was the maximum oxidation state of an element: on that basis, the group 11 elements could not be classified in group IB, due to the existence of copper(II) and gold(III) compounds being known at that time. However, eliminating group IB would make group I the only main group (group VIII was labelled a transition group) to lack an A–B bifurcation. Soon afterward, a majority of chemists chose to classify these elements in group IB and remove them from group VIII for the resulting symmetry: this was the predominant classification until the rise of the modern medium-long 18-column periodic table, which separated the alkali metals and group 11 metals.
The coinage metals were traditionally regarded as a subdivision of the alkali metal group, due to them sharing the characteristic s1 electron configuration of the alkali metals (group 1: p6s1; group 11: d10s1). However, the similarities are largely confined to the stoichiometries of the +1 compounds of both groups, and not their chemical properties. This stems from the filled d subshell providing a much weaker shielding effect on the outermost s electron than the filled p subshell, so that the coinage metals have much higher first ionisation energies and smaller ionic radii than do the corresponding alkali metals. Furthermore, they have higher melting points, hardnesses, and densities, and lower reactivities and solubilities in liquid ammonia, as well as having more covalent character in their compounds. Finally, the alkali metals are at the top of the electrochemical series, whereas the coinage metals are almost at the very bottom. The coinage metals' filled d shell is much more easily disrupted than the alkali metals' filled p shell, so that the second and third ionisation energies are lower, enabling higher oxidation states than +1 and a richer coordination chemistry, thus giving the group 11 metals clear transition metal character. Particularly noteworthy is gold forming ionic compounds with rubidium and caesium, in which it forms the auride ion (Au−) which also occurs in solvated form in liquid ammonia solution: here gold behaves as a pseudohalogen because its 5d106s1 configuration has one electron less than the quasi-closed shell 5d106s2 configuration of mercury.
Production and isolation.
The production of pure alkali metals is somewhat complicated due to their extreme reactivity with commonly used substances, such as water. From their silicate ores, all the stable alkali metals may be obtained the same way: sulfuric acid is first used to dissolve the desired alkali metal ion and aluminium(III) ions from the ore (leaching), whereupon basic precipitation removes aluminium ions from the mixture by precipitating it as the hydroxide. The remaining insoluble alkali metal carbonate is then precipitated selectively; the salt is then dissolved in hydrochloric acid to produce the chloride. The result is then left to evaporate and the alkali metal can then be isolated. Lithium and sodium are typically isolated through electrolysis from their liquid chlorides, with calcium chloride typically added to lower the melting point of the mixture. The heavier alkali metals, however, are more typically isolated in a different way, where a reducing agent (typically sodium for potassium and magnesium or calcium for the heaviest alkali metals) is used to reduce the alkali metal chloride. The liquid or gaseous product (the alkali metal) then undergoes fractional distillation for purification. Most routes to the pure alkali metals require the use of electrolysis due to their high reactivity; one of the few which does not is the pyrolysis of the corresponding alkali metal azide, which yields the metal for sodium, potassium, rubidium, and caesium and the nitride for lithium.
Lithium salts have to be extracted from the water of mineral springs, brine pools, and brine deposits. The metal is produced electrolytically from a mixture of fused lithium chloride and potassium chloride.
Sodium occurs mostly in seawater and dried seabed, but is now produced through electrolysis of sodium chloride by lowering the melting point of the substance to below 700 °C through the use of a Downs cell. Extremely pure sodium can be produced through the thermal decomposition of sodium azide. Potassium occurs in many minerals, such as sylvite (potassium chloride). Previously, potassium was generally made from the electrolysis of potassium chloride or potassium hydroxide, found extensively in places such as Canada, Russia, Belarus, Germany, Israel, United States, and Jordan, in a method similar to how sodium was produced in the late 1800s and early 1900s. It can also be produced from seawater. However, these methods are problematic because the potassium metal tends to dissolve in its molten chloride and vaporises significantly at the operating temperatures, potentially forming the explosive superoxide. As a result, pure potassium metal is now produced by reducing molten potassium chloride with sodium metal at 850 °C.
Although sodium is less reactive than potassium, this process works because at such high temperatures potassium is more volatile than sodium and can easily be distilled off, so that the equilibrium shifts towards the right to produce more potassium gas and proceeds almost to completion.
Metals like sodium are obtained by electrolysis of molten salts. Rb &amp; Cs obtained mainly as by products of Li processing. To make pure caesium, ores of caesium and rubidium are crushed and heated to 650 °C with sodium metal, generating an alloy that can then be separated via a fractional distillation technique. Because metallic caesium is too reactive to handle, it is normally offered as caesium azide (CsN3). Caesium hydroxide is formed when caesium interacts aggressively with water and ice (CsOH).
Rubidium is the 16th most abundant element in the earth's crust; however, it is quite rare. Some minerals found in North America, South Africa, Russia, and Canada contain rubidium. Some potassium minerals (lepidolites, biotites, feldspar, carnallite) contain it, together with caesium. Pollucite, carnallite, leucite, and lepidolite are all minerals that contain rubidium. As a by-product of lithium extraction, it is commercially obtained from lepidolite. Rubidium is also found in potassium rocks and brines, which is a commercial supply. The majority of rubidium is now obtained as a byproduct of refining lithium. Rubidium is used in vacuum tubes as a getter, a material that combines with and removes trace gases from vacuum tubes.
For several years in the 1950s and 1960s, a by-product of the potassium production called Alkarb was a main source for rubidium. Alkarb contained 21% rubidium while the rest was potassium and a small fraction of caesium. Today the largest producers of caesium, for example the Tanco Mine in Manitoba, Canada, produce rubidium as by-product from pollucite. Today, a common method for separating rubidium from potassium and caesium is the fractional crystallisation of a rubidium and caesium alum (Cs, Rb)Al(SO4)2·12H2O, which yields pure rubidium alum after approximately 30 recrystallisations. The limited applications and the lack of a mineral rich in rubidium limit the production of rubidium compounds to 2 to 4 tonnes per year. Caesium, however, is not produced from the above reaction. Instead, the mining of pollucite ore is the main method of obtaining pure caesium, extracted from the ore mainly by three methods: acid digestion, alkaline decomposition, and direct reduction. Both metals are produced as by-products of lithium production: after 1958, when interest in lithium's thermonuclear properties increased sharply, the production of rubidium and caesium also increased correspondingly. Pure rubidium and caesium metals are produced by reducing their chlorides with calcium metal at 750 °C and low pressure.
As a result of its extreme rarity in nature, most francium is synthesised in the nuclear reaction 197Au + 18O → 210Fr + 5 n, yielding francium-209, francium-210, and francium-211. The greatest quantity of francium ever assembled to date is about 300,000 neutral atoms, which were synthesised using the nuclear reaction given above. When the only natural isotope francium-223 is specifically required, it is produced as the alpha daughter of actinium-227, itself produced synthetically from the neutron irradiation of natural radium-226, one of the daughters of natural uranium-238.
Applications.
Lithium, sodium, and potassium have many useful applications, while rubidium and caesium are very notable in academic contexts but do not have many applications yet. Lithium is the key ingredient for a range of lithium-based batteries, and lithium oxide can help process silica. Lithium stearate is a thickener and can be used to make lubricating greases; it is produced from lithium hydroxide, which is also used to absorb carbon dioxide in space capsules and submarines. Lithium chloride is used as a brazing alloy for aluminium parts. In medicine, some lithium salts are used as mood-stabilising phamaceuticals. Metallic lithium is used in alloys with magnesium and aluminium to give very tough and light alloys.
Sodium compounds have many applications, the most well-known being sodium chloride as table salt. Sodium salts of fatty acids are used as soap. Pure sodium metal also has many applications, including use in sodium-vapour lamps, which produce very efficient light compared to other types of lighting, and can help smooth the surface of other metals. Being a strong reducing agent, it is often used to reduce many other metals, such as titanium and zirconium, from their chlorides. Furthermore, it is very useful as a heat-exchange liquid in fast breeder nuclear reactors due to its low melting point, viscosity, and cross-section towards neutron absorption. Sodium-ion batteries may provide cheaper alternatives to their equivalent lithium-based cells. Both sodium and potassium are commonly used as GRAS counterions to create more water soluble and hence more bioavailable salt forms of acidic pharmaceuticals.
Potassium compounds are often used as fertilisers as potassium is an important element for plant nutrition. Potassium hydroxide is a very strong base, and is used to control the pH of various substances. Potassium nitrate and potassium permanganate are often used as powerful oxidising agents. Potassium superoxide is used in breathing masks, as it reacts with carbon dioxide to give potassium carbonate and oxygen gas. Pure potassium metal is not often used, but its alloys with sodium may substitute for pure sodium in fast breeder nuclear reactors.
Rubidium and caesium are often used in atomic clocks. Caesium atomic clocks are extraordinarily accurate; if a clock had been made at the time of the dinosaurs, it would be off by less than four seconds (after 80 million years). For that reason, caesium atoms are used as the definition of the second. Rubidium ions are often used in purple fireworks, and caesium is often used in drilling fluids in the petroleum industry.
Francium has no commercial applications, but because of francium's relatively simple atomic structure, among other things, it has been used in spectroscopy experiments, leading to more information regarding energy levels and the coupling constants between subatomic particles. Studies on the light emitted by laser-trapped francium-210 ions have provided accurate data on transitions between atomic energy levels, similar to those predicted by quantum theory.
Biological role and precautions.
Metals.
Pure alkali metals are dangerously reactive with air and water and must be kept away from heat, fire, oxidising agents, acids, most organic compounds, halocarbons, plastics, and moisture. They also react with carbon dioxide and carbon tetrachloride, so that normal fire extinguishers are counterproductive when used on alkali metal fires. Some Class D dry powder extinguishers designed for metal fires are effective, depriving the fire of oxygen and cooling the alkali metal.
Experiments are usually conducted using only small quantities of a few grams in a fume hood. Small quantities of lithium may be disposed of by reaction with cool water, but the heavier alkali metals should be dissolved in the less reactive isopropanol. The alkali metals must be stored under mineral oil or an inert atmosphere. The inert atmosphere used may be argon or nitrogen gas, except for lithium, which reacts with nitrogen. Rubidium and caesium must be kept away from air, even under oil, because even a small amount of air diffused into the oil may trigger formation of the dangerously explosive peroxide; for the same reason, potassium should not be stored under oil in an oxygen-containing atmosphere for longer than 6 months.
Ions.
The bioinorganic chemistry of the alkali metal ions has been extensively reviewed.
Solid state crystal structures have been determined for many complexes of alkali metal ions in small peptides, nucleic acid constituents, carbohydrates and ionophore complexes.
Lithium naturally only occurs in traces in biological systems and has no known biological role, but does have effects on the body when ingested. Lithium carbonate is used as a mood stabiliser in psychiatry to treat bipolar disorder (manic-depression) in daily doses of about 0.5 to 2 grams, although there are side-effects. Excessive ingestion of lithium causes drowsiness, slurred speech and vomiting, among other symptoms, and poisons the central nervous system, which is dangerous as the required dosage of lithium to treat bipolar disorder is only slightly lower than the toxic dosage. Its biochemistry, the way it is handled by the human body and studies using rats and goats suggest that it is an essential trace element, although the natural biological function of lithium in humans has yet to be identified.
Sodium and potassium occur in all known biological systems, generally functioning as electrolytes inside and outside cells. Sodium is an essential nutrient that regulates blood volume, blood pressure, osmotic equilibrium and pH; the minimum physiological requirement for sodium is 500 milligrams per day. Sodium chloride (also known as common salt) is the principal source of sodium in the diet, and is used as seasoning and preservative, such as for pickling and jerky; most of it comes from processed foods. The Dietary Reference Intake for sodium is 1.5 grams per day, but most people in the United States consume more than 2.3 grams per day, the minimum amount that promotes hypertension; this in turn causes 7.6 million premature deaths worldwide.
Potassium is the major cation (positive ion) inside animal cells, while sodium is the major cation outside animal cells. The concentration differences of these charged particles causes a difference in electric potential between the inside and outside of cells, known as the membrane potential. The balance between potassium and sodium is maintained by ion transporter proteins in the cell membrane. The cell membrane potential created by potassium and sodium ions allows the cell to generate an action potential—a "spike" of electrical discharge. The ability of cells to produce electrical discharge is critical for body functions such as neurotransmission, muscle contraction, and heart function. Disruption of this balance may thus be fatal: for example, ingestion of large amounts of potassium compounds can lead to hyperkalemia strongly influencing the cardiovascular system. Potassium chloride is used in the United States for lethal injection executions.
Due to their similar atomic radii, rubidium and caesium in the body mimic potassium and are taken up similarly. Rubidium has no known biological role, but may help stimulate metabolism, and, similarly to caesium, replace potassium in the body causing potassium deficiency. Partial substitution is quite possible and rather non-toxic: a 70 kg person contains on average 0.36 g of rubidium, and an increase in this value by 50 to 100 times did not show negative effects in test persons. Rats can survive up to 50% substitution of potassium by rubidium. Rubidium (and to a much lesser extent caesium) can function as temporary cures for hypokalemia; while rubidium can adequately physiologically substitute potassium in some systems, caesium is never able to do so. There is only very limited evidence in the form of deficiency symptoms for rubidium being possibly essential in goats; even if this is true, the trace amounts usually present in food are more than enough.
Caesium compounds are rarely encountered by most people, but most caesium compounds are mildly toxic. Like rubidium, caesium tends to substitute potassium in the body, but is significantly larger and is therefore a poorer substitute. Excess caesium can lead to hypokalemia, arrythmia, and acute cardiac arrest, but such amounts would not ordinarily be encountered in natural sources. As such, caesium is not a major chemical environmental pollutant. The median lethal dose (LD50) value for caesium chloride in mice is 2.3 g per kilogram, which is comparable to the LD50 values of potassium chloride and sodium chloride. Caesium chloride has been promoted as an alternative cancer therapy, but has been linked to the deaths of over 50 patients, on whom it was used as part of a scientifically unvalidated cancer treatment.
Radioisotopes of caesium require special precautions: the improper handling of caesium-137 gamma ray sources can lead to release of this radioisotope and radiation injuries. Perhaps the best-known case is the Goiânia accident of 1987, in which an improperly-disposed-of radiation therapy system from an abandoned clinic in the city of Goiânia, Brazil, was scavenged from a junkyard, and the glowing caesium salt sold to curious, uneducated buyers. This led to four deaths and serious injuries from radiation exposure. Together with caesium-134, iodine-131, and strontium-90, caesium-137 was among the isotopes distributed by the Chernobyl disaster which constitute the greatest risk to health. Radioisotopes of francium would presumably be dangerous as well due to their high decay energy and short half-life, but none have been produced in large enough amounts to pose any serious risk.

</doc>
<doc id="668" url="?curid=668" title="Argument form">
Argument form


</doc>
<doc id="669" url="?curid=669" title="Allotrope">
Allotrope


</doc>
<doc id="670" url="?curid=670" title="Alphabet">
Alphabet

An alphabet is a standard set of letters written to represent particular sounds in a spoken language. Specifically, letters largely correspond to "phonemes" as the smallest sound segments that can distinguish one word from another in a given language. Not all writing systems represent language in this way: a syllabary assigns symbols to spoken syllables, while logographies assign symbols to words, morphemes, or other semantic units.
The first letters were invented in Ancient Egypt to serve as an aid in writing Egyptian hieroglyphs; these are referred to as Egyptian uniliteral signs by lexicographers. This system was used until the 5th century AD, and fundamentally differed by adding pronunciation hints to existing hieroglyphs that had previously carried no pronunciation information. Later on, these phonemic symbols also became used to transcribe foreign words. The first fully phonemic script was the Proto-Sinaitic script, also descending from Egyptian hieroglyphs, which was later modified to create the Phoenician alphabet. The Phoenician system is considered the first true alphabet and is the ultimate ancestor of many modern scripts, including Arabic, Cyrillic, Greek, Hebrew, Latin, and possibly Brahmic.
Peter T. Daniels distinguishes true alphabets—which use letters to represent both consonants and vowels—from both abugidas and abjads, which only need letters for consonants. Abjads generally lack vowel indicators altogether, while abugidas represent them with diacritics added to letters. In this narrower sense, the Greek alphabet was the first true alphabet; it was originally derived from the Phoenician alphabet, which was an abjad.
Alphabets usually have a standard ordering for their letters. This makes alphabets a useful tool in collation, as words can be listed in a well-defined order—commonly known as alphabetical order. This also means that letters may be used as a method of "numbering" ordered items. Letters also have names in some languages; this is known as acrophony, and it is present in scripts including Greek, Arabic, Hebrew, and Syriac. However, acrophony is not present in all languages, such as the Latin alphabet, which simply adds a vowel after the character representing each letter. Some systems also used to have acrophony but later abandoned it, such as Cyrillic.
Etymology.
The English word "alphabet" came into Middle English from the Late Latin word , which in turn originated in the Greek, ἀλφάβητος ("alphábētos"); it was made from the first two letters of the Greek alphabet, "alpha" (α) and "beta" (β). The names for the Greek letters, in turn, came from the first two letters of the Phoenician alphabet: "aleph", the word for "ox", and "bet", the word for "house".
History.
Alphabets related to Phoenician.
Ancient Near Eastern alphabets.
The Ancient Egyptian writing system had a set of some 24 hieroglyphs that are called uniliterals, which are glyphs that provide one sound. These glyphs were used as pronunciation guides for logograms, to write grammatical inflections, and, later, to transcribe loan words and foreign names. The script was used a fair amount in the 4th century CE. However, after pagan temples were closed down, it was forgotten in the 5th century until the discovery of the Rosetta Stone. There was also cuneiform, primarily used to write several ancient languages, including Sumerian. The last known use of the Cuneiform script was in 75 CE, after which the script fell out of use.
In the Middle Bronze Age, an apparently alphabetic system known as the Proto-Sinaitic script appeared in Egyptian turquoise mines in the Sinai peninsula around 1840 BCE, apparently left by Canaanite workers. Orly Goldwasser has connected the illiterate turquoise miner graffiti theory to the origin of the alphabet. In 1999, American Egyptologists John and Deborah Darnell discovered an earlier version of this first alphabet at the Wadi el-Hol valley. The script dated to and shows evidence of having been adapted from specific forms of Egyptian hieroglyphs that could be dated to , strongly suggesting that the first alphabet had developed about that time. The script was based on letter appearances and names, believed to be based on Egyptian hieroglyphs. This script had no characters representing vowels. Originally, it probably was a syllabary—a script where syllables are represented with characters—with symbols that were not needed being removed. The best-attested Bronze Age alphabet is Ugaritic, invented in Ugarit before the 15th century BCE. This was an alphabetic cuneiform script with 30 signs, including three that indicate the following vowel. This script was not used after the destruction of Ugarit in 1178 BCE.
The Proto-Sinaitic script eventually developed into the Phoenician alphabet, conventionally called Proto-Canaanite, before . The oldest text in Phoenician script is an inscription on the sarcophagus of King Ahiram . This script is the parent script of all western alphabets. By the 10th century BCE, two other forms distinguish themselves, Canaanite and Aramaic. The Aramaic gave rise to the Hebrew alphabet.
The South Arabian alphabet, a sister script to the Phoenician alphabet, is the script from which the Ge'ez abugida was descended. Abugidas are writing systems with characters comprising consonant–vowel sequences. Alphabets without obligatory vowels are called "abjads", with examples being Arabic, Hebrew, and Syriac. The omission of vowels was not always a satisfactory solution due to the need of preserving sacred texts. "Weak" consonants are used to indicate vowels. These letters have a dual function since they can also be used as pure consonants.
The Proto-Sinaitic script and the Ugaritic script were the first scripts with a limited number of signs instead of using many different signs for words, in contrast to cuneiform, Egyptian hieroglyphs, and Linear B. The Phoenician script was probably the first phonemic script, and it contained only about two dozen distinct letters, making it a script simple enough for traders to learn. Another advantage of the Phoenician alphabet was that it could write different languages since it recorded words phonemically.
The Phoenician script was spread across the Mediterranean by the Phoenicians. The Greek Alphabet was the first alphabet in which vowels have independent letter forms separate from those of consonants. The Greeks chose letters representing sounds that did not exist in Phoenician to represent vowels. The Linear B syllabary, used by Mycenaean Greeks from the 16th century BCE, had 87 symbols, including five vowels. In its early years, there were many variants of the Greek alphabet, causing many different alphabets to evolve from it.
European alphabets.
The Greek alphabet, in Euboean form, was carried over by Greek colonists to the Italian peninsula -600 BCE giving rise to many different alphabets used to write the Italic languages, like the Etruscan alphabet. One of these became the Latin alphabet, which spread across Europe as the Romans expanded their republic. After the fall of the Western Roman Empire, the alphabet survived in intellectual and religious works. It came to be used for the descendant languages of Latin (the Romance languages) and most of the other languages of western and central Europe. Today, it is the most widely used script in the world.
The Etruscan alphabet remained nearly unchanged for several hundred years. Only evolving once the Etruscan language changed itself. The letters used for non-existent phonemes were dropped. Afterwards, however, the alphabet went through many different changes. The final classical form of Etruscan contained 20 letters. Four of them are vowels (a, e, i, and u) - six fewer letters than the earlier forms. The script in its classical form was used until the 1st century CE. The Etruscan language itself was not used in imperial Rome, but the script was used for religious texts.
Some adaptations of the Latin alphabet have ligatures, a combination of two letters make one, such as æ in Danish and Icelandic and Ȣ in Algonquian; borrowings from other alphabets, such as the thorn þ in Old English and Icelandic, which came from the Futhark runes; and modified existing letters, such as the eth ð of Old English and Icelandic, which is a modified "d". Other alphabets only use a subset of the Latin alphabet, such as Hawaiian and Italian, which uses the letters "j, k, x, y," and "w" only in foreign words.
Another notable script is Elder Futhark, believed to have evolved out of one of the Old Italic alphabets. Elder Futhark gave rise to other alphabets known collectively as the Runic alphabets. The Runic alphabets were used for Germanic languages from 100 CE to the late Middle Ages, being engraved on stone and jewelry, although inscriptions found on bone and wood occasionally appear. These alphabets have since been replaced with the Latin alphabet. The exception was for decorative use, where the runes remained in use until the 20th century.
The Old Hungarian script was the writing system of the Hungarians. It was in use during the entire history of Hungary, albeit not as an official writing system. From the 19th century, it once again became more and more popular.
The Glagolitic alphabet was the initial script of the liturgical language Old Church Slavonic and became, together with the Greek uncial script, the basis of the Cyrillic script. Cyrillic is one of the most widely used modern alphabetic scripts and is notable for its use in Slavic languages and also for other languages within the former Soviet Union. Cyrillic alphabets include Serbian, Macedonian, Bulgarian, Russian, Belarusian, and Ukrainian. The Glagolitic alphabet is believed to have been created by Saints Cyril and Methodius, while the Cyrillic alphabet was created by Clement of Ohrid, their disciple. They feature many letters that appear to have been borrowed from or influenced by Greek and Hebrew.
Asian alphabets.
Many phonetic scripts exist in Asia. The Arabic alphabet, Hebrew alphabet, Syriac alphabet, and other abjads of the Middle East are developments of the Aramaic alphabet.
Most alphabetic scripts of India and Eastern Asia descend from the Brahmi script, believed to be a descendant of Aramaic.
European alphabets, especially Latin and Cyrillic, have been adapted for many languages of Asia. Arabic is also widely used, sometimes as an abjad, as with Urdu and Persian, and sometimes as a complete alphabet, as with Kurdish and Uyghur.
Other alphabets.
Hangul.
In Korea, Sejong the Great created the Hangul alphabet in 1443 CE. Hangul is a unique alphabet: it is a featural alphabet, where the design of many of the letters comes from a sound's place of articulation, like P looking like the widened mouth and L looking like the tongue pulled in. The creation of Hangul was planned by the government of the day, and it places individual letters in syllable clusters with equal dimensions, in the same way as Chinese characters. This change allows for mixed-script writing, where one syllable always takes up one type space no matter how many letters get stacked into building that one sound-block.
Zhuyin.
Zhuyin, sometimes referred to as "Bopomofo," is a semi-syllabary. It transcribes Mandarin phonetically in the Republic of China. After the later establishment of the People's Republic of China and its adoption of Hanyu Pinyin, the use of Zhuyin today is limited. However, it is still widely used in Taiwan. Zhuyin developed from a form of Chinese shorthand based on Chinese characters in the early 1900s and has elements of both an alphabet and a syllabary. Like an alphabet, the phonemes of syllable initials are represented by individual symbols, but like a syllabary, the phonemes of the syllable finals are not; each possible final (excluding the medial glide) has its own character, an example being "luan" written as ㄌㄨㄢ ("l-u-an"). The last symbol ㄢ takes place as the entire final "-an". While Zhuyin is not a mainstream writing system, it is still often used in ways similar to a romanization system, for aiding pronunciation and as an input method for Chinese characters on computers and cellphones.
Types.
The term "alphabet" is used by linguists and paleographers in both a wide and a narrow sense. In a broader sense, an alphabet is a "segmental" script at the phoneme level—that is, it has separate glyphs for individual sounds and not for larger units such as syllables or words. In the narrower sense, some scholars distinguish "true" alphabets from two other types of segmental script, abjads, and abugidas. These three differ in how they treat vowels. Abjads have letters for consonants and leave most vowels unexpressed. Abugidas are also consonant-based but indicate vowels with diacritics, a systematic graphic modification of the consonants. The earliest known alphabet using this sense is the Wadi el-Hol script, believed to be an abjad. Its successor, Phoenician, is the ancestor of modern alphabets, including Arabic, Greek, Latin (via the Old Italic alphabet), Cyrillic (via the Greek alphabet), and Hebrew (via Aramaic).
Examples of present-day abjads are the Arabic and Hebrew scripts; true alphabets include Latin, Cyrillic, and Korean Hangul; and abugidas, used to write Tigrinya, Amharic, Hindi, and Thai. The Canadian Aboriginal syllabics are also an abugida, rather than a syllabary, as their name would imply, because each glyph stands for a consonant and is modified by rotation to represent the following vowel. In a true syllabary, each consonant-vowel combination gets represented by a separate glyph.
All three types may be augmented with syllabic glyphs. Ugaritic, for example, is essentially an abjad but has syllabic letters for These are the only times that vowels are indicated. Coptic has a letter for . Devanagari is typically an abugida augmented with dedicated letters for initial vowels, though some traditions use अ as a zero consonant as the graphic base for such vowels.
The boundaries between the three types of segmental scripts are not always clear-cut. For example, Sorani Kurdish is written in the Arabic script, which, when used for other languages, is an abjad. In Kurdish, writing the vowels is mandatory, and whole letters are used, so the script is a true alphabet. Other languages may use a Semitic abjad with forced vowel diacritics, effectively making them abugidas. On the other hand, the Phagspa script of the Mongol Empire was based closely on the Tibetan abugida, but vowel marks are written after the preceding consonant rather than as diacritic marks. Although short "a" is not written, as in the Indic abugidas, The source of the term "abugida", namely the Ge'ez abugida now used for Amharic and Tigrinya, has assimilated into their consonant modifications. It is no longer systematic and must be learned as a syllabary rather than as a segmental script. Even more extreme, the Pahlavi abjad eventually became logographic.
Thus the primary categorisation of alphabets reflects how they treat vowels. For tonal languages, further classification can be based on their treatment of tone. Though names do not yet exist to distinguish the various types. Some alphabets disregard tone entirely, especially when it does not carry a heavy functional load, as in Somali and many other languages of Africa and the Americas. Most commonly, tones are indicated by diacritics, which is how vowels are treated in abugidas, which is the case for Vietnamese (a true alphabet) and Thai (an abugida). In Thai, the tone is determined primarily by a consonant, with diacritics for disambiguation. In the Pollard script, an abugida, vowels are indicated by diacritics. The placing of the diacritic relative to the consonant is modified to indicate the tone. More rarely, a script may have separate letters for tones, as is the case for Hmong and Zhuang. For many, regardless of whether letters or diacritics get used, the most common tone is not marked, just as the most common vowel is not marked in Indic abugidas. In Zhuyin, not only is one of the tones unmarked; but there is a diacritic to indicate a lack of tone, like the virama of Indic.
Alphabetical order.
Alphabets often come to be associated with a standard ordering of their letters; this is for collation—namely, for listing words and other items in "alphabetical order".
Latin alphabets.
The basic ordering of the Latin alphabet (A B C D E F G H I J K L M N O P Q R S T U V W X Y Z), which derives from the Northwest Semitic "Abgad" order, is already well established. Although, languages using this alphabet have different conventions for their treatment of modified letters (such as the French "é", "à", and "ô") and certain combinations of letters (multigraphs). In French, these are not considered to be additional letters for collation. However, in Icelandic, the accented letters such as "á", "í", and "ö" are considered distinct letters representing different vowel sounds from sounds represented by their unaccented counterparts. In Spanish, "ñ" is considered a separate letter, but accented vowels such as "á" and "é" are not. The "ll" and "ch" were also formerly considered single letters and sorted separately after "l" and "c", but in 1994, the tenth congress of the Association of Spanish Language Academies changed the collating order so that "ll" came to be sorted between "lk" and "lm" in the dictionary and "ch" came to be sorted between "cg" and "ci"; those digraphs were still formally designated as letters, but in 2010 the changed it, so they are no longer considered letters at all.
In German, words starting with "sch-" (which spells the German phoneme ) are inserted between words with initial "sca-" and "sci-" (all incidentally loanwords) instead of appearing after the initial "sz", as though it were a single letter, which contrasts several languages such as Albanian, in which "dh-", "ë-", "gj-", "ll-", "rr-", "th-", "xh-," and "zh-," which all represent phonemes and considered separate single letters, would follow the letters "d", "e", "g", "l", "n", "r", "t", "x," and "z," respectively, as well as Hungarian and Welsh. Further, German words with an umlaut get collated ignoring the umlaut as—contrary to Turkish, which adopted the graphemes ö and ü, and where a word like "tüfek" would come after "tuz", in the dictionary. An exception is the German telephone directory, where umlauts are sorted like "ä"="ae" since names such as "Jäger" also appear with the spelling "Jaeger" and are not distinguished in the spoken language.
The Danish and Norwegian alphabets end with "æ"—"ø"—"å", whereas the Swedish conventionally put "å"—"ä"—"ö" at the end. However, æ phonetically corresponds with ä, as does ø and "ö."
Early alphabets.
It is unknown whether the earliest alphabets had a defined sequence. Some alphabets today, such as the Hanuno'o script, are learned one letter at a time, in no particular order, and are not used for collation where a definite order is required. However, a dozen Ugaritic tablets from the fourteenth century BCE preserve the alphabet in two sequences. One, the "ABCDE" order later used in Phoenician, has continued with minor changes in Hebrew, Greek, Armenian, Gothic, Cyrillic, and Latin; the other, "HMĦLQ," was used in southern Arabia and is preserved today in Ethiopic. Both orders have therefore been stable for at least 3000 years.
Runic used an unrelated Futhark sequence, which got simplified later on. Arabic usually uses its sequence, although Arabic retains the traditional abjadi order, which is used for numbers.
The Brahmic family of alphabets used in India uses a unique order based on phonology: The letters are arranged according to how and where the sounds get produced in the mouth. This organization is present in Southeast Asia, Tibet, Korean hangul, and even Japanese kana, which is not an alphabet.
Acrophony.
In Phoenician, each letter got associated with a word that begins with that sound. This is called acrophony and is continuously used to varying degrees in Samaritan, Aramaic, Syriac, Hebrew, Greek, and Arabic.
Acrophony got abandoned in Latin. It referred to the letters by adding a vowel (usually "e", sometimes "a", or "u") before or after the consonant. Two exceptions were Y and Z, which were borrowed from the Greek alphabet rather than Etruscan. They were known as "Y Graeca" "Greek Y" and "zeta" (from Greek)—this discrepancy was inherited by many European languages, as in the term "zed" for Z in all forms of English, other than American English. Over time names sometimes shifted or were added, as in "double U" for W, or "double V" in French, the English name for Y, and the American "zee" for Z. Comparing them in English and French gives a clear reflection of the Great Vowel Shift: A, B, C, and D are pronounced in today's English, but in contemporary French they are . The French names (from which the English names got derived) preserve the qualities of the English vowels before the Great Vowel Shift. By contrast, the names of F, L, M, N, and S () remain the same in both languages because "short" vowels were largely unaffected by the Shift.
In Cyrillic, originally, acrophony was present using Slavic words. The first three words going, azŭ, buky, vědě, with the Cyrillic collation order being, А, Б, В. However, this was later abandoned in favor of a system similar to Latin.
Orthography and pronunciation.
When an alphabet is adopted or developed to represent a given language, an orthography generally comes into being, providing rules for spelling words, following the principle on which alphabets get based. These rules will map letters of the alphabet to the phonemes of the spoken language. In a perfectly phonemic orthography, there would be a consistent one-to-one correspondence between the letters and the phonemes so that a writer could predict the spelling of a word given its pronunciation, and a speaker would always know the pronunciation of a word given its spelling, and vice versa. However, this ideal is usually never achieved in practice. Languages can come close to it, such as Spanish and Finnish. Others, such as English, deviate from it to a much larger degree.
The pronunciation of a language often evolves independently of its writing system. Writing systems have been borrowed for languages the orthography was not initially made to use. The degree to which letters of an alphabet correspond to phonemes of a language varies.
Languages may fail to achieve a one-to-one correspondence between letters and sounds in any of several ways:
National languages sometimes elect to address the problem of dialects by associating the alphabet with the national standard. Some national languages like Finnish, Armenian, Turkish, Russian, Serbo-Croatian (Serbian, Croatian, and Bosnian), and Bulgarian have a very regular spelling system with nearly one-to-one correspondence between letters and phonemes. Similarly, the Italian verb corresponding to 'spell (out),' "compitare", is unknown to many Italians because spelling is usually trivial, as Italian spelling is highly phonemic. In standard Spanish, one can tell the pronunciation of a word from its spelling, but not vice versa, as phonemes sometimes can be represented in more than one way, but a given letter is consistently pronounced. French using silent letters, nasal vowels, and elision, may seem to lack much correspondence between the spelling and pronunciation. However, its rules on pronunciation, though complex, are consistent and predictable with a fair degree of accuracy.
At the other extreme are languages such as English, where pronunciations mostly have to be memorized as they do not correspond to the spelling consistently. For English, this is because the Great Vowel Shift occurred after the orthography got established and because English has acquired a large number of loanwords at different times, retaining their original spelling at varying levels. However, even English has general, albeit complex, rules that predict pronunciation from spelling. Rules like this are usually successful. However, rules to predict spelling from pronunciation have a higher failure rate.
Sometimes, countries have the written language undergo a spelling reform to realign the writing with the contemporary spoken language. These can range from simple spelling changes and word forms to switching the entire writing system. For example, Turkey switched from the Arabic alphabet to a Latin-based Turkish alphabet, and Kazakh changed from an Arabic script to a Cyrillic script due to the Soviet Union's influence. In 2021, it made a transition to the Latin alphabet, similar to Turkish. The Cyrillic script used to be official in Uzbekistan and Turkmenistan before they switched to the Latin alphabet. Uzbekistan is reforming the alphabet to use diacritics on the letters that are marked by apostrophes and the letters that are digraphs.
The standard system of symbols used by linguists to represent sounds in any language, independently of orthography, is called the International Phonetic Alphabet.

</doc>
<doc id="673" url="?curid=673" title="Atomic number">
Atomic number

The atomic number or nuclear charge number (symbol Z) of a chemical element is the charge number of an atomic nucleus. For ordinary nuclei composed of protons and neutrons, this is equal to the proton number ("n"p) or the number of protons found in the nucleus of every atom of that element. The atomic number can be used to uniquely identify ordinary chemical elements. In an ordinary uncharged atom, the atomic number is also equal to the number of electrons.
For an ordinary atom which contains protons, neutrons and electrons, the sum of the atomic number "Z" and the neutron number "N" gives the atom's atomic mass number "A". Since protons and neutrons have approximately the same mass (and the mass of the electrons is negligible for many purposes) and the mass defect of the nucleon binding is always small compared to the nucleon mass, the atomic mass of any atom, when expressed in daltons (making a quantity called the "relative isotopic mass"), is within 1% of the whole number "A".
Atoms with the same atomic number but different neutron numbers, and hence different mass numbers, are known as isotopes. A little more than three-quarters of naturally occurring elements exist as a mixture of isotopes (see monoisotopic elements), and the average isotopic mass of an isotopic mixture for an element (called the relative atomic mass) in a defined environment on Earth determines the element's standard atomic weight. Historically, it was these atomic weights of elements (in comparison to hydrogen) that were the quantities measurable by chemists in the 19th century.
The conventional symbol "Z" comes from the German word "" 'number', which, before the modern synthesis of ideas from chemistry and physics, merely denoted an element's numerical place in the periodic table, whose order was then approximately, but not completely, consistent with the order of the elements by atomic weights. Only after 1915, with the suggestion and evidence that this "Z" number was also the nuclear charge and a physical characteristic of atoms, did the word (and its English equivalent "atomic number") come into common use in this context.
The rules above do not always apply to exotic atoms which contain short-lived elementary particles other than protons, neutrons and electrons.
History.
The periodic table and a natural number for each element.
Loosely speaking, the existence or construction of a periodic table of elements creates an ordering of the elements, and so they can be numbered in order.
Dmitri Mendeleev said that he arranged his first periodic tables (first published on March 6, 1869) in order of atomic weight ("Atomgewicht"). However, in consideration of the elements' observed chemical properties, he changed the order slightly and placed tellurium (atomic weight 127.6) ahead of iodine (atomic weight 126.9). This placement is consistent with the modern practice of ordering the elements by proton number, "Z", but that number was not known or suspected at the time.
A simple numbering based on periodic table position was never entirely satisfactory. In addition to the case of iodine and tellurium, several other pairs of elements (such as argon and potassium, cobalt and nickel) were later shown to have nearly identical or reversed atomic weights, thus requiring their placement in the periodic table to be determined by their chemical properties. However the gradual identification of more and more chemically similar lanthanide elements, whose atomic number was not obvious, led to inconsistency and uncertainty in the periodic numbering of elements at least from lutetium (element 71) onward (hafnium was not known at this time).
The Rutherford-Bohr model and van den Broek.
In 1911, Ernest Rutherford gave a model of the atom in which a central nucleus held most of the atom's mass and a positive charge which, in units of the electron's charge, was to be approximately equal to half of the atom's atomic weight, expressed in numbers of hydrogen atoms. This central charge would thus be approximately half the atomic weight (though it was almost 25% different from the atomic number of gold , ), the single element from which Rutherford made his guess). Nevertheless, in spite of Rutherford's estimation that gold had a central charge of about 100 (but was element on the periodic table), a month after Rutherford's paper appeared, Antonius van den Broek first formally suggested that the central charge and number of electrons in an atom were "exactly" equal to its place in the periodic table (also known as element number, atomic number, and symbolized "Z"). This eventually proved to be the case.
Moseley's 1913 experiment.
The experimental position improved dramatically after research by Henry Moseley in 1913. Moseley, after discussions with Bohr who was at the same lab (and who had used Van den Broek's hypothesis in his Bohr model of the atom), decided to test Van den Broek's and Bohr's hypothesis directly, by seeing if spectral lines emitted from excited atoms fitted the Bohr theory's postulation that the frequency of the spectral lines be proportional to the square of "Z".
To do this, Moseley measured the wavelengths of the innermost photon transitions (K and L lines) produced by the elements from aluminium ("Z" = 13) to gold ("Z" = 79) used as a series of movable anodic targets inside an x-ray tube. The square root of the frequency of these photons increased from one target to the next in an arithmetic progression. This led to the conclusion (Moseley's law) that the atomic number does closely correspond (with an offset of one unit for K-lines, in Moseley's work) to the calculated electric charge of the nucleus, i.e. the element number "Z". Among other things, Moseley demonstrated that the lanthanide series (from lanthanum to lutetium inclusive) must have 15 members—no fewer and no more—which was far from obvious from known chemistry at that time.
Missing elements.
After Moseley's death in 1915, the atomic numbers of all known elements from hydrogen to uranium ("Z" = 92) were examined by his method. There were seven elements (with "Z" &lt; 92) which were not found and therefore identified as still undiscovered, corresponding to atomic numbers 43, 61, 72, 75, 85, 87 and 91. From 1918 to 1947, all seven of these missing elements were discovered. By this time, the first four transuranium elements had also been discovered, so that the periodic table was complete with no gaps as far as curium ("Z" = 96).
The proton and the idea of nuclear electrons.
In 1915, the reason for nuclear charge being quantized in units of "Z", which were now recognized to be the same as the element number, was not understood. An old idea called Prout's hypothesis had postulated that the elements were all made of residues (or "protyles") of the lightest element hydrogen, which in the Bohr-Rutherford model had a single electron and a nuclear charge of one. However, as early as 1907, Rutherford and Thomas Royds had shown that alpha particles, which had a charge of +2, were the nuclei of helium atoms, which had a mass four times that of hydrogen, not two times. If Prout's hypothesis were true, something had to be neutralizing some of the charge of the hydrogen nuclei present in the nuclei of heavier atoms.
In 1917, Rutherford succeeded in generating hydrogen nuclei from a nuclear reaction between alpha particles and nitrogen gas, and believed he had proven Prout's law. He called the new heavy nuclear particles protons in 1920 (alternate names being proutons and protyles). It had been immediately apparent from the work of Moseley that the nuclei of heavy atoms have more than twice as much mass as would be expected from their being made of hydrogen nuclei, and thus there was required a hypothesis for the neutralization of the extra protons presumed present in all heavy nuclei. A helium nucleus was presumed to be composed of four protons plus two "nuclear electrons" (electrons bound inside the nucleus) to cancel two of the charges. At the other end of the periodic table, a nucleus of gold with a mass 197 times that of hydrogen was thought to contain 118 nuclear electrons in the nucleus to give it a residual charge of +79, consistent with its atomic number.
The discovery of the neutron makes "Z" the proton number.
All consideration of nuclear electrons ended with James Chadwick's discovery of the neutron in 1932. An atom of gold now was seen as containing 118 neutrons rather than 118 nuclear electrons, and its positive nuclear charge now was realized to come entirely from a content of 79 protons. Since Moseley had previously shown that the atomic number "Z" of an element equals this positive charge, it was now clear that "Z" is identical to the number of protons of its nuclei.
Chemical properties.
Each element has a specific set of chemical properties as a consequence of the number of electrons present in the neutral atom, which is "Z" (the atomic number). The configuration of these electrons follows from the principles of quantum mechanics. The number of electrons in each element's electron shells, particularly the outermost valence shell, is the primary factor in determining its chemical bonding behavior. Hence, it is the atomic number alone that determines the chemical properties of an element; and it is for this reason that an element can be defined as consisting of "any" mixture of atoms with a given atomic number. 
New elements.
The quest for new elements is usually described using atomic numbers. As of , all elements with atomic numbers 1 to 118 have been observed. Synthesis of new elements is accomplished by bombarding target atoms of heavy elements with ions, such that the sum of the atomic numbers of the target and ion elements equals the atomic number of the element being created. In general, the half-life of a nuclide becomes shorter as atomic number increases, though undiscovered nuclides with certain "magic" numbers of protons and neutrons may have relatively longer half-lives and comprise an island of stability. 
A hypothetical element composed only of neutrons has also been proposed and would have atomic number 0, but has never been observed.

</doc>
<doc id="674" url="?curid=674" title="Anatomy">
Anatomy

Anatomy () is the branch of morphology concerned with the study of the internal structure of organisms and their parts. Anatomy is a branch of natural science that deals with the structural organization of living things. It is an old science, having its beginnings in prehistoric times. Anatomy is inherently tied to developmental biology, embryology, comparative anatomy, evolutionary biology, and phylogeny, as these are the processes by which anatomy is generated, both over immediate and long-term timescales. Anatomy and physiology, which study the structure and function of organisms and their parts respectively, make a natural pair of related disciplines, and are often studied together. Human anatomy is one of the essential basic sciences that are applied in medicine, and is often studied alongside physiology.
Anatomy is a complex and dynamic field that is constantly evolving as new discoveries are made. In recent years, there has been a significant increase in the use of advanced imaging techniques, such as MRI and CT scans, which allow for more detailed and accurate visualizations of the body's structures.
The discipline of anatomy is divided into macroscopic and microscopic parts. Macroscopic anatomy, or gross anatomy, is the examination of an animal's body parts using unaided eyesight. Gross anatomy also includes the branch of superficial anatomy. Microscopic anatomy involves the use of optical instruments in the study of the tissues of various structures, known as histology, and also in the study of cells.
The history of anatomy is characterized by a progressive understanding of the functions of the organs and structures of the human body. Methods have also improved dramatically, advancing from the examination of animals by dissection of carcasses and cadavers (corpses) to 20th-century medical imaging techniques, including X-ray, ultrasound, and magnetic resonance imaging.
Etymology and definition.
Derived from the Greek "anatomē" "dissection" (from "anatémnō" "I cut up, cut open" from ἀνά "aná" "up", and τέμνω "témnō" "I cut"), anatomy is the scientific study of the structure of organisms including their systems, organs and tissues. It includes the appearance and position of the various parts, the materials from which they are composed, and their relationships with other parts. Anatomy is quite distinct from physiology and biochemistry, which deal respectively with the functions of those parts and the chemical processes involved. For example, an anatomist is concerned with the shape, size, position, structure, blood supply and innervation of an organ such as the liver; while a physiologist is interested in the production of bile, the role of the liver in nutrition and the regulation of bodily functions.
The discipline of anatomy can be subdivided into a number of branches, including gross or macroscopic anatomy and microscopic anatomy. Gross anatomy is the study of structures large enough to be seen with the naked eye, and also includes superficial anatomy or surface anatomy, the study by sight of the external body features. Microscopic anatomy is the study of structures on a microscopic scale, along with histology (the study of tissues), and embryology (the study of an organism in its immature condition). Regional anatomy is the study of the interrelationships of all of the structures in a specific body region, such as the abdomen. In contrast, systemic anatomy is the study of the structures that make up a discrete body system—that is, a group of structures that work together to perform a unique body function, such as the digestive system.
Anatomy can be studied using both invasive and non-invasive methods with the goal of obtaining information about the structure and organization of organs and systems. Methods used include dissection, in which a body is opened and its organs studied, and endoscopy, in which a video camera-equipped instrument is inserted through a small incision in the body wall and used to explore the internal organs and other structures. Angiography using X-rays or magnetic resonance angiography are methods to visualize blood vessels.
The term "anatomy" is commonly taken to refer to human anatomy. However, substantially similar structures and tissues are found throughout the rest of the animal kingdom, and the term also includes the anatomy of other animals. The term "zootomy" is also sometimes used to specifically refer to non-human animals. The structure and tissues of plants are of a dissimilar nature and they are studied in plant anatomy.
Animal tissues.
The kingdom Animalia contains multicellular organisms that are heterotrophic and motile (although some have secondarily adopted a sessile lifestyle). Most animals have bodies differentiated into separate tissues and these animals are also known as eumetazoans. They have an internal digestive chamber, with one or two openings; the gametes are produced in multicellular sex organs, and the zygotes include a blastula stage in their embryonic development. Metazoans do not include the sponges, which have undifferentiated cells.
Unlike plant cells, animal cells have neither a cell wall nor chloroplasts. Vacuoles, when present, are more in number and much smaller than those in the plant cell. The body tissues are composed of numerous types of cells, including those found in muscles, nerves and skin. Each typically has a cell membrane formed of phospholipids, cytoplasm and a nucleus. All of the different cells of an animal are derived from the embryonic germ layers. Those simpler invertebrates which are formed from two germ layers of ectoderm and endoderm are called diploblastic and the more developed animals whose structures and organs are formed from three germ layers are called triploblastic. All of a triploblastic animal's tissues and organs are derived from the three germ layers of the embryo, the ectoderm, mesoderm and endoderm.
Animal tissues can be grouped into four basic types: connective, epithelial, muscle and nervous tissue.
Connective tissue.
Connective tissues are fibrous and made up of cells scattered among inorganic material called the extracellular matrix. Often called fascia (from the Latin "fascia," meaning "band" or "bandage"), connective tissues give shape to organs and holds them in place. The main types are loose connective tissue, adipose tissue, fibrous connective tissue, cartilage and bone. The extracellular matrix contains proteins, the chief and most abundant of which is collagen. Collagen plays a major part in organizing and maintaining tissues. The matrix can be modified to form a skeleton to support or protect the body. An exoskeleton is a thickened, rigid cuticle which is stiffened by mineralization, as in crustaceans or by the cross-linking of its proteins as in insects. An endoskeleton is internal and present in all developed animals, as well as in many of those less developed.
Epithelium.
Epithelial tissue is composed of closely packed cells, bound to each other by cell adhesion molecules, with little intercellular space. Epithelial cells can be squamous (flat), cuboidal or columnar and rest on a basal lamina, the upper layer of the basement membrane, the lower layer is the reticular lamina lying next to the connective tissue in the extracellular matrix secreted by the epithelial cells. There are many different types of epithelium, modified to suit a particular function. In the respiratory tract there is a type of ciliated epithelial lining; in the small intestine there are microvilli on the epithelial lining and in the large intestine there are intestinal villi. Skin consists of an outer layer of keratinized stratified squamous epithelium that covers the exterior of the vertebrate body. Keratinocytes make up to 95% of the cells in the skin. The epithelial cells on the external surface of the body typically secrete an extracellular matrix in the form of a cuticle. In simple animals this may just be a coat of glycoproteins. In more advanced animals, many glands are formed of epithelial cells.
Muscle tissue.
Muscle cells (myocytes) form the active contractile tissue of the body. Muscle tissue functions to produce force and cause motion, either locomotion or movement within internal organs. Muscle is formed of contractile filaments and is separated into three main types; smooth muscle, skeletal muscle and cardiac muscle. Smooth muscle has no striations when examined microscopically. It contracts slowly but maintains contractibility over a wide range of stretch lengths. It is found in such organs as sea anemone tentacles and the body wall of sea cucumbers. Skeletal muscle contracts rapidly but has a limited range of extension. It is found in the movement of appendages and jaws. Obliquely striated muscle is intermediate between the other two. The filaments are staggered and this is the type of muscle found in earthworms that can extend slowly or make rapid contractions. In higher animals striated muscles occur in bundles attached to bone to provide movement and are often arranged in antagonistic sets. Smooth muscle is found in the walls of the uterus, bladder, intestines, stomach, oesophagus, respiratory airways, and blood vessels. Cardiac muscle is found only in the heart, allowing it to contract and pump blood round the body.
Nervous tissue.
Nervous tissue is composed of many nerve cells known as neurons which transmit information. In some slow-moving radially symmetrical marine animals such as ctenophores and cnidarians (including sea anemones and jellyfish), the nerves form a nerve net, but in most animals they are organized longitudinally into bundles. In simple animals, receptor neurons in the body wall cause a local reaction to a stimulus. In more complex animals, specialized receptor cells such as chemoreceptors and photoreceptors are found in groups and send messages along neural networks to other parts of the organism. Neurons can be connected together in ganglia. In higher animals, specialized receptors are the basis of sense organs and there is a central nervous system (brain and spinal cord) and a peripheral nervous system. The latter consists of sensory nerves that transmit information from sense organs and motor nerves that influence target organs. The peripheral nervous system is divided into the somatic nervous system which conveys sensation and controls voluntary muscle, and the autonomic nervous system which involuntarily controls smooth muscle, certain glands and internal organs, including the stomach.
Vertebrate anatomy.
All vertebrates have a similar basic body plan and at some point in their lives, mostly in the embryonic stage, share the major chordate characteristics: a stiffening rod, the notochord; a dorsal hollow tube of nervous material, the neural tube; pharyngeal arches; and a tail posterior to the anus. The spinal cord is protected by the vertebral column and is above the notochord, and the gastrointestinal tract is below it. Nervous tissue is derived from the ectoderm, connective tissues are derived from mesoderm, and gut is derived from the endoderm. At the posterior end is a tail which continues the spinal cord and vertebrae but not the gut. The mouth is found at the anterior end of the animal, and the anus at the base of the tail. The defining characteristic of a vertebrate is the vertebral column, formed in the development of the segmented series of vertebrae. In most vertebrates the notochord becomes the nucleus pulposus of the intervertebral discs. However, a few vertebrates, such as the sturgeon and the coelacanth, retain the notochord into adulthood. Jawed vertebrates are typified by paired appendages, fins or legs, which may be secondarily lost. The limbs of vertebrates are considered to be homologous because the same underlying skeletal structure was inherited from their last common ancestor. This is one of the arguments put forward by Charles Darwin to support his theory of evolution.
Fish anatomy.
The body of a fish is divided into a head, trunk and tail, although the divisions between the three are not always externally visible. The skeleton, which forms the support structure inside the fish, is either made of cartilage, in cartilaginous fish, or bone in bony fish. The main skeletal element is the vertebral column, composed of articulating vertebrae which are lightweight yet strong. The ribs attach to the spine and there are no limbs or limb girdles. The main external features of the fish, the fins, are composed of either bony or soft spines called rays, which with the exception of the caudal fins, have no direct connection with the spine. They are supported by the muscles which compose the main part of the trunk. The heart has two chambers and pumps the blood through the respiratory surfaces of the gills and on round the body in a single circulatory loop. The eyes are adapted for seeing underwater and have only local vision. There is an inner ear but no external or middle ear. Low frequency vibrations are detected by the lateral line system of sense organs that run along the length of the sides of fish, and these respond to nearby movements and to changes in water pressure.
Sharks and rays are basal fish with numerous primitive anatomical features similar to those of ancient fish, including skeletons composed of cartilage. Their bodies tend to be dorso-ventrally flattened, they usually have five pairs of gill slits and a large mouth set on the underside of the head. The dermis is covered with separate dermal placoid scales. They have a cloaca into which the urinary and genital passages open, but not a swim bladder. Cartilaginous fish produce a small number of large, yolky eggs. Some species are ovoviviparous and the young develop internally but others are oviparous and the larvae develop externally in egg cases.
The bony fish lineage shows more derived anatomical traits, often with major evolutionary changes from the features of ancient fish. They have a bony skeleton, are generally laterally flattened, have five pairs of gills protected by an operculum, and a mouth at or near the tip of the snout. The dermis is covered with overlapping scales. Bony fish have a swim bladder which helps them maintain a constant depth in the water column, but not a cloaca. They mostly spawn a large number of small eggs with little yolk which they broadcast into the water column.
Amphibian anatomy.
Amphibians are a class of animals comprising frogs, salamanders and caecilians. They are tetrapods, but the caecilians and a few species of salamander have either no limbs or their limbs are much reduced in size. Their main bones are hollow and lightweight and are fully ossified and the vertebrae interlock with each other and have articular processes. Their ribs are usually short and may be fused to the vertebrae. Their skulls are mostly broad and short, and are often incompletely ossified. Their skin contains little keratin and lacks scales, but contains many mucous glands and in some species, poison glands. The hearts of amphibians have three chambers, two atria and one ventricle. They have a urinary bladder and nitrogenous waste products are excreted primarily as urea. Amphibians breathe by means of buccal pumping, a pump action in which air is first drawn into the buccopharyngeal region through the nostrils. These are then closed and the air is forced into the lungs by contraction of the throat. They supplement this with gas exchange through the skin which needs to be kept moist.
In frogs the pelvic girdle is robust and the hind legs are much longer and stronger than the forelimbs. The feet have four or five digits and the toes are often webbed for swimming or have suction pads for climbing. Frogs have large eyes and no tail. Salamanders resemble lizards in appearance; their short legs project sideways, the belly is close to or in contact with the ground and they have a long tail. Caecilians superficially resemble earthworms and are limbless. They burrow by means of zones of muscle contractions which move along the body and they swim by undulating their body from side to side.
Reptile anatomy.
Reptiles are a class of animals comprising turtles, tuataras, lizards, snakes and crocodiles. They are tetrapods, but the snakes and a few species of lizard either have no limbs or their limbs are much reduced in size. Their bones are better ossified and their skeletons stronger than those of amphibians. The teeth are conical and mostly uniform in size. The surface cells of the epidermis are modified into horny scales which create a waterproof layer. Reptiles are unable to use their skin for respiration as do amphibians and have a more efficient respiratory system drawing air into their lungs by expanding their chest walls. The heart resembles that of the amphibian but there is a septum which more completely separates the oxygenated and deoxygenated bloodstreams. The reproductive system has evolved for internal fertilization, with a copulatory organ present in most species. The eggs are surrounded by amniotic membranes which prevents them from drying out and are laid on land, or develop internally in some species. The bladder is small as nitrogenous waste is excreted as uric acid.
Turtles are notable for their protective shells. They have an inflexible trunk encased in a horny carapace above and a plastron below. These are formed from bony plates embedded in the dermis which are overlain by horny ones and are partially fused with the ribs and spine. The neck is long and flexible and the head and the legs can be drawn back inside the shell. Turtles are vegetarians and the typical reptile teeth have been replaced by sharp, horny plates. In aquatic species, the front legs are modified into flippers.
Tuataras superficially resemble lizards but the lineages diverged in the Triassic period. There is one living species, "Sphenodon punctatus". The skull has two openings (fenestrae) on either side and the jaw is rigidly attached to the skull. There is one row of teeth in the lower jaw and this fits between the two rows in the upper jaw when the animal chews. The teeth are merely projections of bony material from the jaw and eventually wear down. The brain and heart are more primitive than those of other reptiles, and the lungs have a single chamber and lack bronchi. The tuatara has a well-developed parietal eye on its forehead.
Lizards have skulls with only one fenestra on each side, the lower bar of bone below the second fenestra having been lost. This results in the jaws being less rigidly attached which allows the mouth to open wider. Lizards are mostly quadrupeds, with the trunk held off the ground by short, sideways-facing legs, but a few species have no limbs and resemble snakes. Lizards have moveable eyelids, eardrums are present and some species have a central parietal eye.
Snakes are closely related to lizards, having branched off from a common ancestral lineage during the Cretaceous period, and they share many of the same features. The skeleton consists of a skull, a hyoid bone, spine and ribs though a few species retain a vestige of the pelvis and rear limbs in the form of pelvic spurs. The bar under the second fenestra has also been lost and the jaws have extreme flexibility allowing the snake to swallow its prey whole. Snakes lack moveable eyelids, the eyes being covered by transparent "spectacle" scales. They do not have eardrums but can detect ground vibrations through the bones of their skull. Their forked tongues are used as organs of taste and smell and some species have sensory pits on their heads enabling them to locate warm-blooded prey.
Crocodilians are large, low-slung aquatic reptiles with long snouts and large numbers of teeth. The head and trunk are dorso-ventrally flattened and the tail is laterally compressed. It undulates from side to side to force the animal through the water when swimming. The tough keratinized scales provide body armour and some are fused to the skull. The nostrils, eyes and ears are elevated above the top of the flat head enabling them to remain above the surface of the water when the animal is floating. Valves seal the nostrils and ears when it is submerged. Unlike other reptiles, crocodilians have hearts with four chambers allowing complete separation of oxygenated and deoxygenated blood.
Bird anatomy.
Birds are tetrapods but though their hind limbs are used for walking or hopping, their front limbs are wings covered with feathers and adapted for flight. Birds are endothermic, have a high metabolic rate, a light skeletal system and powerful muscles. The long bones are thin, hollow and very light. Air sac extensions from the lungs occupy the centre of some bones. The sternum is wide and usually has a keel and the caudal vertebrae are fused. There are no teeth and the narrow jaws are adapted into a horn-covered beak. The eyes are relatively large, particularly in nocturnal species such as owls. They face forwards in predators and sideways in ducks.
The feathers are outgrowths of the epidermis and are found in localized bands from where they fan out over the skin. Large flight feathers are found on the wings and tail, contour feathers cover the bird's surface and fine down occurs on young birds and under the contour feathers of water birds. The only cutaneous gland is the single uropygial gland near the base of the tail. This produces an oily secretion that waterproofs the feathers when the bird preens. There are scales on the legs, feet and claws on the tips of the toes.
Mammal anatomy.
Mammals are a diverse class of animals, mostly terrestrial but some are aquatic and others have evolved flapping or gliding flight. They mostly have four limbs, but some aquatic mammals have no limbs or limbs modified into fins, and the forelimbs of bats are modified into wings. The legs of most mammals are situated below the trunk, which is held well clear of the ground. The bones of mammals are well ossified and their teeth, which are usually differentiated, are coated in a layer of prismatic enamel. The teeth are shed once (milk teeth) during the animal's lifetime or not at all, as is the case in cetaceans. Mammals have three bones in the middle ear and a cochlea in the inner ear. They are clothed in hair and their skin contains glands which secrete sweat. Some of these glands are specialized as mammary glands, producing milk to feed the young. Mammals breathe with lungs and have a muscular diaphragm separating the thorax from the abdomen which helps them draw air into the lungs. The mammalian heart has four chambers, and oxygenated and deoxygenated blood are kept entirely separate. Nitrogenous waste is excreted primarily as urea.
Mammals are amniotes, and most are viviparous, giving birth to live young. Exceptions to this are the egg-laying monotremes, the platypus and the echidnas of Australia. Most other mammals have a placenta through which the developing foetus obtains nourishment, but in marsupials, the foetal stage is very short and the immature young is born and finds its way to its mother's pouch where it latches on to a teat and completes its development.
Human anatomy.
Humans have the overall body plan of a mammal. Humans have a head, neck, trunk (which includes the thorax and abdomen), two arms and hands, and two legs and feet.
Generally, students of certain biological sciences, paramedics, prosthetists and orthotists, physiotherapists, occupational therapists, nurses, podiatrists, and medical students learn gross anatomy and microscopic anatomy from anatomical models, skeletons, textbooks, diagrams, photographs, lectures and tutorials and in addition, medical students generally also learn gross anatomy through practical experience of dissection and inspection of cadavers. The study of microscopic anatomy (or histology) can be aided by practical experience examining histological preparations (or slides) under a microscope.
Human anatomy, physiology and biochemistry are complementary basic medical sciences, which are generally taught to medical students in their first year at medical school. Human anatomy can be taught regionally or systemically; that is, respectively, studying anatomy by bodily regions such as the head and chest, or studying by specific systems, such as the nervous or respiratory systems. The major anatomy textbook, Gray's Anatomy, has been reorganized from a systems format to a regional format, in line with modern teaching methods. A thorough working knowledge of anatomy is required by physicians, especially surgeons and doctors working in some diagnostic specialties, such as histopathology and radiology.
Academic anatomists are usually employed by universities, medical schools or teaching hospitals. They are often involved in teaching anatomy, and research into certain systems, organs, tissues or cells.
Invertebrate anatomy.
Invertebrates constitute a vast array of living organisms ranging from the simplest unicellular eukaryotes such as "Paramecium" to such complex multicellular animals as the octopus, lobster and dragonfly. They constitute about 95% of the animal species. By definition, none of these creatures has a backbone. The cells of single-cell protozoans have the same basic structure as those of multicellular animals but some parts are specialized into the equivalent of tissues and organs. Locomotion is often provided by cilia or flagella or may proceed via the advance of pseudopodia, food may be gathered by phagocytosis, energy needs may be supplied by photosynthesis and the cell may be supported by an endoskeleton or an exoskeleton. Some protozoans can form multicellular colonies.
Metazoans are a multicellular organism, with different groups of cells serving different functions. The most basic types of metazoan tissues are epithelium and connective tissue, both of which are present in nearly all invertebrates. The outer surface of the epidermis is normally formed of epithelial cells and secretes an extracellular matrix which provides support to the organism. An endoskeleton derived from the mesoderm is present in echinoderms, sponges and some cephalopods. Exoskeletons are derived from the epidermis and is composed of chitin in arthropods (insects, spiders, ticks, shrimps, crabs, lobsters). Calcium carbonate constitutes the shells of molluscs, brachiopods and some tube-building polychaete worms and silica forms the exoskeleton of the microscopic diatoms and radiolaria. Other invertebrates may have no rigid structures but the epidermis may secrete a variety of surface coatings such as the pinacoderm of sponges, the gelatinous cuticle of cnidarians (polyps, sea anemones, jellyfish) and the collagenous cuticle of annelids. The outer epithelial layer may include cells of several types including sensory cells, gland cells and stinging cells. There may also be protrusions such as microvilli, cilia, bristles, spines and tubercles.
Marcello Malpighi, the father of microscopical anatomy, discovered that plants had tubules similar to those he saw in insects like the silk worm. He observed that when a ring-like portion of bark was removed on a trunk a swelling occurred in the tissues above the ring, and he unmistakably interpreted this as growth stimulated by food coming down from the leaves, and being captured above the ring.
Arthropod anatomy.
Arthropods comprise the largest phylum of invertebrates in the animal kingdom with over a million known species.
Insects possess segmented bodies supported by a hard-jointed outer covering, the exoskeleton, made mostly of chitin. The segments of the body are organized into three distinct parts, a head, a thorax and an abdomen. The head typically bears a pair of sensory antennae, a pair of compound eyes, one to three simple eyes (ocelli) and three sets of modified appendages that form the mouthparts. The thorax has three pairs of segmented legs, one pair each for the three segments that compose the thorax and one or two pairs of wings. The abdomen is composed of eleven segments, some of which may be fused and houses the digestive, respiratory, excretory and reproductive systems. There is considerable variation between species and many adaptations to the body parts, especially wings, legs, antennae and mouthparts.
Spiders a class of arachnids have four pairs of legs; a body of two segments—a cephalothorax and an abdomen. Spiders have no wings and no antennae. They have mouthparts called chelicerae which are often connected to venom glands as most spiders are venomous. They have a second pair of appendages called pedipalps attached to the cephalothorax. These have similar segmentation to the legs and function as taste and smell organs. At the end of each male pedipalp is a spoon-shaped cymbium that acts to support the copulatory organ.
History.
Ancient.
In 1600 BCE, the Edwin Smith Papyrus, an Ancient Egyptian medical text, described the heart and its vessels, as well as the brain and its meninges and cerebrospinal fluid, and the liver, spleen, kidneys, uterus and bladder, and it showed the blood vessels diverging from the heart. The Ebers Papyrus () features a "treatise on the heart", with vessels carrying all the body's fluids to or from every member of the body.
Ancient Greek anatomy and physiology underwent great changes and advances throughout the early medieval world. Over time, this medical practice expanded by a continually developing understanding of the functions of organs and structures in the body. Phenomenal anatomical observations of the human body were made, which have contributed towards the understanding of the brain, eye, liver, reproductive organs and the nervous system.
The Hellenistic Egyptian city of Alexandria was the stepping-stone for Greek anatomy and physiology. Alexandria not only housed the biggest library for medical records and books of the liberal arts in the world during the time of the Greeks, but was also home to many medical practitioners and philosophers. Great patronage of the arts and sciences from the Ptolemaic dynasty of Egypt helped raise Alexandria up, further rivalling the cultural and scientific achievements of other Greek states.
Some of the most striking advances in early anatomy and physiology took place in Hellenistic Alexandria. Two of the most famous anatomists and physiologists of the third century were Herophilus and Erasistratus. These two physicians helped pioneer human dissection for medical research, using the cadavers of condemned criminals, which was considered taboo until the Renaissance—Herophilus was recognized as the first person to perform systematic dissections. Herophilus became known for his anatomical works making impressing contributions to many branches of anatomy and many other aspects of medicine. Some of the works included classifying the system of the pulse, the discovery that human arteries had thicker walls than veins, and that the atria were parts of the heart. Herophilus's knowledge of the human body has provided vital input towards understanding the brain, eye, liver, reproductive organs and nervous system, and characterizing the course of disease. Erasistratus accurately described the structure of the brain, including the cavities and membranes, and made a distinction between its cerebrum and cerebellum During his study in Alexandria, Erasistratus was particularly concerned with studies of the circulatory and nervous systems. He was able to distinguish the sensory and the motor nerves in the human body and believed that air entered the lungs and heart, which was then carried throughout the body. His distinction between the arteries and veins—the arteries carrying the air through the body, while the veins carried the blood from the heart was a great anatomical discovery. Erasistratus was also responsible for naming and describing the function of the epiglottis and the valves of the heart, including the tricuspid. During the third century, Greek physicians were able to differentiate nerves from blood vessels and tendons and to realize that the nerves convey neural impulses. It was Herophilus who made the point that damage to motor nerves induced paralysis. Herophilus named the meninges and ventricles in the brain, appreciated the division between cerebellum and cerebrum and recognized that the brain was the "seat of intellect" and not a "cooling chamber" as propounded by Aristotle Herophilus is also credited with describing the optic, oculomotor, motor division of the trigeminal, facial, vestibulocochlear and hypoglossal nerves.
Great feats were made during the third century BCE in both the digestive and reproductive systems. Herophilus was able to discover and describe not only the salivary glands, but the small intestine and liver. He showed that the uterus is a hollow organ and described the ovaries and uterine tubes. He recognized that spermatozoa were produced by the testes and was the first to identify the prostate gland.
The anatomy of the muscles and skeleton is described in the "Hippocratic Corpus", an Ancient Greek medical work written by unknown authors. Aristotle described vertebrate anatomy based on animal dissection. Praxagoras identified the difference between arteries and veins. Also in the 4th century BCE, Herophilos and Erasistratus produced more accurate anatomical descriptions based on vivisection of criminals in Alexandria during the Ptolemaic period.
In the 2nd century, Galen of Pergamum, an anatomist, clinician, writer and philosopher, wrote the final and highly influential anatomy treatise of ancient times. He compiled existing knowledge and studied anatomy through dissection of animals. He was one of the first experimental physiologists through his vivisection experiments on animals. Galen's drawings, based mostly on dog anatomy, became effectively the only anatomical textbook for the next thousand years. His work was known to Renaissance doctors only through Islamic Golden Age medicine until it was translated from the Greek some time in the 15th century.
Medieval to early modern.
Anatomy developed little from classical times until the sixteenth century; as the historian Marie Boas writes, "Progress in anatomy before the sixteenth century is as mysteriously slow as its development after 1500 is startlingly rapid". Between 1275 and 1326, the anatomists Mondino de Luzzi, Alessandro Achillini and Antonio Benivieni at Bologna carried out the first systematic human dissections since ancient times. Mondino's "Anatomy" of 1316 was the first textbook in the medieval rediscovery of human anatomy. It describes the body in the order followed in Mondino's dissections, starting with the abdomen, then the thorax, then the head and limbs. It was the standard anatomy textbook for the next century.
Leonardo da Vinci (1452–1519) was trained in anatomy by Andrea del Verrocchio. He made use of his anatomical knowledge in his artwork, making many sketches of skeletal structures, muscles and organs of humans and other vertebrates that he dissected.
Andreas Vesalius (1514–1564), professor of anatomy at the University of Padua, is considered the founder of modern human anatomy. Originally from Brabant, Vesalius published the influential book "De humani corporis fabrica" ("the structure of the human body"), a large format book in seven volumes, in 1543. The accurate and intricately detailed illustrations, often in allegorical poses against Italianate landscapes, are thought to have been made by the artist Jan van Calcar, a pupil of Titian.
In England, anatomy was the subject of the first public lectures given in any science; these were given by the Company of Barbers and Surgeons in the 16th century, joined in 1583 by the Lumleian lectures in surgery at the Royal College of Physicians.
Late modern.
In the United States, medical schools began to be set up towards the end of the 18th century. Classes in anatomy needed a continual stream of cadavers for dissection and these were difficult to obtain. Philadelphia, Baltimore and New York were all renowned for body snatching activity as criminals raided graveyards at night, removing newly buried corpses from their coffins. A similar problem existed in Britain where demand for bodies became so great that grave-raiding and even anatomy murder were practised to obtain cadavers. Some graveyards were in consequence protected with watchtowers. The practice was halted in Britain by the Anatomy Act of 1832, while in the United States, similar legislation was enacted after the physician William S. Forbes of Jefferson Medical College was found guilty in 1882 of "complicity with resurrectionists in the despoliation of graves in Lebanon Cemetery".
The teaching of anatomy in Britain was transformed by Sir John Struthers, Regius Professor of Anatomy at the University of Aberdeen from 1863 to 1889. He was responsible for setting up the system of three years of "pre-clinical" academic teaching in the sciences underlying medicine, including especially anatomy. This system lasted until the reform of medical training in 1993 and 2003. As well as teaching, he collected many vertebrate skeletons for his museum of comparative anatomy, published over 70 research papers, and became famous for his public dissection of the Tay Whale. From 1822 the Royal College of Surgeons regulated the teaching of anatomy in medical schools. Medical museums provided examples in comparative anatomy, and were often used in teaching. Ignaz Semmelweis investigated puerperal fever and he discovered how it was caused. He noticed that the frequently fatal fever occurred more often in mothers examined by medical students than by midwives. The students went from the dissecting room to the hospital ward and examined women in childbirth. Semmelweis showed that when the trainees washed their hands in chlorinated lime before each clinical examination, the incidence of puerperal fever among the mothers could be reduced dramatically.
Before the modern medical era, the main means for studying the internal structures of the body were dissection of the dead and inspection, palpation and auscultation of the living. It was the advent of microscopy that opened up an understanding of the building blocks that constituted living tissues. Technical advances in the development of achromatic lenses increased the resolving power of the microscope and around 1839, Matthias Jakob Schleiden and Theodor Schwann identified that cells were the fundamental unit of organization of all living things. Study of small structures involved passing light through them and the microtome was invented to provide sufficiently thin slices of tissue to examine. Staining techniques using artificial dyes were established to help distinguish between different types of tissue. Advances in the fields of histology and cytology began in the late 19th century along with advances in surgical techniques allowing for the painless and safe removal of biopsy specimens. The invention of the electron microscope brought a great advance in resolution power and allowed research into the ultrastructure of cells and the organelles and other structures within them. About the same time, in the 1950s, the use of X-ray diffraction for studying the crystal structures of proteins, nucleic acids and other biological molecules gave rise to a new field of molecular anatomy.
Equally important advances have occurred in "non-invasive" techniques for examining the interior structures of the body. X-rays can be passed through the body and used in medical radiography and fluoroscopy to differentiate interior structures that have varying degrees of opaqueness. Magnetic resonance imaging, computed tomography, and ultrasound imaging have all enabled examination of internal structures in unprecedented detail to a degree far beyond the imagination of earlier generations.

</doc>
<doc id="675" url="?curid=675" title="Affirming the consequent">
Affirming the consequent

In propositional logic, affirming the consequent, sometimes called converse error, fallacy of the converse, or confusion of necessity and sufficiency, is a formal fallacy of taking a true conditional statement (e.g., "if the lamp were broken, then the room would be dark") under certain assumptions (there are no other lights in the room, it is nighttime and the windows are closed), and invalidly inferring its converse ("the room is dark, so the lamp must be broken"), even though that statement may not be true under the same assumptions. This arises when the consequent ("the room would be dark") has other possible antecedents (for example, "the lamp is in working order, but is switched off" or "there is no lamp in the room").
Converse errors are common in everyday thinking and communication and can result from, among other causes, communication issues, misconceptions about logic, and failure to consider other causes.
The opposite statement, denying the consequent, is called modus tollens and "is" a valid form of argument.
Formal description.
Affirming the consequent is the action of taking a true statement formula_1 and invalidly concluding its converse formula_2. The name "affirming the consequent" derives from using the consequent, "Q", of formula_1, to conclude the antecedent "P". This fallacy can be summarized formally as formula_4 or, alternatively, formula_5.
The root cause of such a logical error is sometimes failure to realize that just because "P" is a "possible" condition for "Q", "P" may not be the "only" condition for "Q", i.e. "Q" may follow from another condition as well.
Affirming the consequent can also result from overgeneralizing the experience of many statements "having" true converses. If "P" and "Q" are "equivalent" statements, i.e. formula_6, it "is" possible to infer "P" under the condition "Q". For example, the statements "It is August 13, so it is my birthday" formula_1 and "It is my birthday, so it is August 13" formula_2 are equivalent and both true consequences of the statement "August 13 is my birthday" (an abbreviated form of formula_6).
Of the possible forms of "mixed hypothetical syllogisms," two are valid and two are invalid. Affirming the antecedent (modus ponens) and denying the consequent (modus tollens) are valid. Affirming the consequent and denying the antecedent are invalid.
Additional examples.
Example 1
One way to demonstrate the invalidity of this argument form is with a counterexample with true premises but an obviously false conclusion. For example:
There are many places to live in California other than San Diego. On the other hand, one can affirm with certainty that "if someone does not live in California" ("non-Q"), then "this person does not live in San Diego" ("non-P"). This is the contrapositive of the first statement, and it must be true if and only if the original statement is true.
Example 2
Here, it is immediately intuitive that any number of other antecedents ("If an animal is a deer...", "If an animal is an elephant...", "If an animal is a moose...", "etc.") can give rise to the consequent ("then it has four legs"), and that it is preposterous to suppose that having four legs "must" imply that the animal is a dog and nothing else. This is useful as a teaching example since most people can immediately recognize that the conclusion reached must be wrong (intuitively, a cat cannot be a dog), and that the method by which it was reached must therefore be fallacious.
Example 3
In "Catch-22", the chaplain is interrogated for supposedly being "Washington Irving"/"Irving Washington", who has been blocking out large portions of soldiers' letters home. The colonel has found such a letter, but with the Chaplain's name signed.
"P" in this case is 'The chaplain signs his own name', and "Q" 'The chaplain's name is written'. The chaplain's name may be written, but he did not necessarily write it, as the colonel falsely concludes."

</doc>
<doc id="676" url="?curid=676" title="Andrei Tarkovsky">
Andrei Tarkovsky

Andrei Arsenyevich Tarkovsky ( 4 April 1932 – 29 December 1986) was a Soviet film director and screenwriter of Russian origin. He has been widely considered one of the best directors in cinema history. His films explore spiritual and metaphysical themes and are known for their slow pacing and long takes, dreamlike visual imagery and preoccupation with nature and memory.
Tarkovsky studied film at the All-Union State Institute of Cinematography under filmmaker Mikhail Romm and subsequently directed his first five features in the Soviet Union: "Ivan's Childhood" (1962), "Andrei Rublev" (1966), "Solaris" (1972), "Mirror" (1975)—which has been ranked among the best films ever made—and "Stalker" (1979). After years of creative conflict with state film authorities, he left the country in 1979 and made his final two films—"Nostalghia" (1983) and "The Sacrifice" (1986)—abroad. In 1986, he published "Sculpting in Time", a book about cinema and art. He died later that year of cancer, a condition possibly caused by the toxic locations used in the filming of "Stalker".
Tarkovsky was the recipient of several awards at the Cannes Film Festival throughout his career, including the FIPRESCI prize, the Prize of the Ecumenical Jury and the Grand Prix Spécial du Jury. He was also awarded the Golden Lion at the Venice Film Festival for his debut film, "Ivan's Childhood". In 1990, he was posthumously awarded the Soviet Union's prestigious Lenin Prize. Three of his films—"Andrei Rublev", "Mirror", and "Stalker"—featured in "Sight &amp; Sound" 2012 poll of the 100 greatest films of all time.
Life and career.
Childhood and early life.
Andrei Tarkovsky was born in the village of Zavrazhye in the Yuryevetsky District of the Ivanovo Industrial Oblast (modern-day Kadyysky District of the Kostroma Oblast, Russia) to the poet and translator Arseny Aleksandrovich Tarkovsky, a native of Yelysavethrad (now Kropyvnytskyi, Ukraine), and Maria Ivanova Vishnyakova, a graduate of the Maxim Gorky Literature Institute who later worked as a proofreader; she was born in Moscow in the Dubasov family estate.
Andrei's paternal grandfather Aleksandr Karlovich Tarkovsky (in ) was a Polish nobleman who worked as a bank clerk. His wife Maria Danilovna Rachkovskaya was a Romanian language teacher who arrived from Iași. Andrei's maternal grandmother Vera Nikolayevna Vishnyakova (née Dubasova) belonged to an old Dubasov family of Russian nobility that traces its history back to the 17th century; among her relatives was Admiral Fyodor Dubasov, a fact she had to conceal during the Soviet days. She was married to Ivan Ivanovich Vishnyakov, a native of the Kaluga Governorate who studied law at the Moscow State University and served as a judge in Kozelsk.
According to the family legend, Tarkovsky's ancestors on his father's side were princes from the Shamkhalate of Tarki, Dagestan, although his sister, Marina Tarkovskaya, who conducted detailed research on their genealogy, called it "a myth, even a prank of sorts," stressing that no document confirms this narrative.
Tarkovsky spent his childhood in Yuryevets. He was described by childhood friends as active and popular, having many friends and being typically in the center of action. His father left the family in 1937, subsequently volunteering for the army in 1941. He returned home in 1943, having been awarded a Red Star after being shot in one of his legs (which he would eventually need to amputate due to gangrene). Tarkovsky stayed with his mother, moving with her and his sister Marina to Moscow, where she worked as a proofreader at a printing press.
In 1939, Tarkovsky enrolled at the Moscow School No. 554. During the war, the three evacuated to Yuryevets, living with his maternal grandmother. In 1943, the family returned to Moscow. Tarkovsky continued his studies at his old school, where the poet Andrei Voznesensky was one of his classmates. He studied piano at a music school and attended classes at an art school. The family lived on Shchipok Street in the Zamoskvorechye District in Moscow. From November 1947 to spring 1948 he was in the hospital with tuberculosis. Many themes of his childhood—the evacuation, his mother and her two children, the withdrawn father, the time in the hospital—feature prominently in his film "Mirror".
In his school years, Tarkovsky was a troublemaker and a poor student. He still managed to graduate, and from 1951 to 1952 studied Arabic at the Oriental Institute in Moscow, a branch of the Academy of Sciences of the Soviet Union. Although he already spoke some Arabic and was a successful student in his first semesters, he did not finish his studies and dropped out to work as a prospector for the Academy of Science Institute for Non-Ferrous Metals and Gold. He participated in a year-long research expedition to the river Kureyka near Turukhansk in the Krasnoyarsk Province. During this time in the taiga, Tarkovsky decided to study film.
Film school student.
Upon returning from the research expedition in 1954, Tarkovsky applied at the State Institute of Cinematography (VGIK) and was admitted to the film-directing program. He was in the same class as Irma Raush (Irina) whom he married in April 1957.
The early Khrushchev era offered good opportunities for young film directors. Before 1953, annual film production was low and most films were directed by veteran directors. After 1953, more films were produced, many of them by young directors. The Khrushchev Thaw relaxed Soviet social restrictions a bit and permitted a limited influx of European and North American literature, films and music. This allowed Tarkovsky to see films of the Italian neorealists, French New Wave, and of directors such as Kurosawa, Buñuel, Bergman, Bresson, Wajda (whose film "Ashes and Diamonds" influenced Tarkovsky) and Mizoguchi.
Tarkovsky's teacher and mentor was Mikhail Romm, who taught many film students who would later become influential film directors. In 1956, Tarkovsky directed his first student short film, "The Killers", from a short story of Ernest Hemingway. The longer television film "There Will Be No Leave Today" followed in 1959. Both films were a collaboration between the VGIK students. Classmate Aleksandr Gordon, who married Tarkovsky's sister, in particular directed, wrote, edited, and acted in the two films with Tarkovsky.
An important influence on Tarkovsky was the film director Grigory Chukhray, who was teaching at the VGIK. Impressed by the talent of his student, Chukhray offered Tarkovsky a position as assistant director for his film "Clear Skies". Tarkovsky initially showed interest but then decided to concentrate on his studies and his own projects.
During his third year at the VGIK, Tarkovsky met Andrei Konchalovsky. They found much in common as they liked the same film directors and shared ideas on cinema and films. In 1959, they wrote the script "Antarctica – Distant Country", which was later published in the "Moskovsky Komsomolets". Tarkovsky submitted the script to Lenfilm, but it was rejected. They were more successful with the script "The Steamroller and the Violin", which they sold to Mosfilm. This became Tarkovsky's graduation project, earning him his diploma in 1960 and winning First Prize at the New York Student Film Festival in 1961.
Film career in the Soviet Union.
Tarkovsky's first feature film was "Ivan's Childhood" in 1962. He had inherited the film from director Eduard Abalov, who had to abort the project. The film earned Tarkovsky international acclaim and won the Golden Lion award at the Venice Film Festival in the year 1962. In the same year, on 30 September, his first son Arseny (called Senka in Tarkovsky's diaries) Tarkovsky was born.
In 1965, he directed the film "Andrei Rublev" about the life of Andrei Rublev, the fifteenth-century Russian icon painter. "Andrei Rublev" was not, except for a single screening in Moscow in 1966, immediately released after completion due to problems with Soviet authorities. Tarkovsky had to cut the film several times, resulting in several different versions of varying lengths. The film was widely released in the Soviet Union in a cut version in 1971. Nevertheless, the film had a budget of more than 1 million rubles – a significant sum for that period. A version of the film was presented at the Cannes Film Festival in 1969 and won the FIPRESCI prize.
He divorced his wife, Irina, in June 1970. In the same year, he married Larisa Kizilova (née Egorkina), who had been a production assistant for the film "Andrei Rublev" (they had been living together since 1965). Their son, Andrei Andreyevich Tarkovsky, (nicknamed Andriosha, meaning "little Andre" or "Andre Junior") was born in the same year on 7 August.
In 1972, he completed "Solaris", an adaptation of the novel "Solaris" by Stanisław Lem. He had worked on this together with screenwriter Friedrich Gorenstein as early as 1968. The film was presented at the Cannes Film Festival, won the Grand Prix Spécial du Jury, and was nominated for the Palme d'Or.
From 1973 to 1974, he shot the film "Mirror", a highly autobiographical and unconventionally structured film drawing on his childhood and incorporating some of his father's poems. In this film Tarkovsky portrayed the plight of childhood affected by war. Tarkovsky had worked on the screenplay for this film since 1967, under the consecutive titles "Confession", "White day" and "A white, white day". From the beginning the film was not well received by Soviet authorities due to its content and its perceived elitist nature. Soviet authorities placed the film in the "third category", a severely limited distribution, and only allowed it to be shown in third-class cinemas and workers' clubs. Few prints were made and the film-makers received no returns. Third category films also placed the film-makers in danger of being accused of wasting public funds, which could have serious effects on their future productivity. These difficulties are presumed to have made Tarkovsky play with the idea of going abroad and producing a film outside the Soviet film industry.
During 1975, Tarkovsky also worked on the screenplay "Hoffmanniana", about the German writer and poet E. T. A. Hoffmann. In December 1976, he directed "Hamlet", his only stage play, at the Lenkom Theatre in Moscow. The main role was played by Anatoly Solonitsyn, who also acted in several of Tarkovsky's films. At the end of 1978, he also wrote the screenplay "Sardor" together with the writer Aleksandr Misharin.
The last film Tarkovsky completed in the Soviet Union was "Stalker", inspired by the novel "Roadside Picnic" by the brothers Arkady and Boris Strugatsky. Tarkovsky had met the brothers first in 1971 and was in contact with them until his death in 1986. Initially he wanted to shoot a film based on their novel "Dead Mountaineer's Hotel" and he developed a raw script. Influenced by a discussion with Arkady Strugatsky he changed his plan and began to work on the script based on "Roadside Picnic". Work on this film began in 1976. The production was mired in troubles; improper development of the negatives had ruined all the exterior shots. Tarkovsky's relationship with cinematographer Georgy Rerberg deteriorated to the point where he hired Alexander Knyazhinsky as a new first cinematographer. Furthermore, Tarkovsky had a heart attack in April 1978, resulting in further delay. The film was completed in 1979 and won the Prize of the Ecumenical Jury at the Cannes Film Festival. In a question and answer session at the Edinburgh Filmhouse on 11 February 1981, Tarkovsky trenchantly rejected suggestions that the film was either impenetrably mysterious or a political allegory.
In 1979, Tarkovsky began production of the film "The First Day" (Russian: Первый День "Pervyj Dyen"), based on a script by his friend and long-term collaborator Andrei Konchalovsky. The film was set in 18th-century Russia during the reign of Peter the Great and starred Natalya Bondarchuk and Anatoli Papanov. To get the project approved by Goskino, Tarkovsky submitted a script that was different from the original script, omitting several scenes that were critical of the official atheism in the Soviet Union. After shooting roughly half of the film the project was stopped by Goskino after it became apparent that the film differed from the script submitted to the censors. Tarkovsky was reportedly infuriated by this interruption and destroyed most of the film.
Film career outside the Soviet Union.
During the summer of 1979, Tarkovsky traveled to Italy, where he shot the documentary "Voyage in Time" together with his long-time friend Tonino Guerra. Tarkovsky returned to Italy in 1980 for an extended trip, during which he and Guerra completed the script for the film "Nostalghia". During this period, he took Polaroid photographs depicting his personal life.
Tarkovsky returned to Italy in 1982 to start shooting "Nostalghia", but Mosfilm then withdrew from the project, so he sought and received financial backing from the Italian RAI. Tarkovsky completed the film in 1983, and it was presented at the Cannes Film Festival where it won the FIPRESCI prize and the Prize of the Ecumenical Jury. Tarkovsky also shared a special prize called "Grand Prix du cinéma de création" with Robert Bresson. Soviet authorities lobbied to prevent the film from winning the Palme d'Or, a fact that hardened Tarkovsky's resolve to never work in the Soviet Union again. After Cannes he went to London to stage and choreograph the opera "Boris Godunov" at the Royal Opera House under the musical direction of Claudio Abbado.
At a press conference in Milan on 10 July 1984, he announced that he would never return to the Soviet Union and would remain in Western Europe. He stated, "I am not a Soviet dissident, I have no conflict with the Soviet Government," but if he returned home, he added, "I would be unemployed." At that time, his son Andriosha was still in the Soviet Union and not allowed to leave the country. On 28 August 1985, Tarkovsky was processed as a Soviet Defector at a refugee camp in Latina, Italy, registered with the serial number 13225/379, and officially welcomed to the West.
Tarkovsky spent most of 1984 preparing the film "The Sacrifice". It was finally shot in 1985 in Sweden, with many of the crew being alumni from Ingmar Bergman's films, including cinematographer Sven Nykvist. Tarkovsky's vision of his film was greatly influenced by Bergman's style.
While "The Sacrifice" is about an apocalypse and impending death, faith, and possible redemption, in the making-of documentary "Directed by Andrei Tarkovsky", in a particularly poignant scene, writer/director Michal Leszczylowski follows Tarkovsky on a walk as he expresses his sentiments on death—he claims himself to be immortal and has no fear of dying. Ironically, at the end of the year Tarkovsky was diagnosed with terminal lung cancer. In January 1986, he began treatment in Paris and was joined there by his son, Andre Jr, who was finally allowed to leave the Soviet Union. What would be Tarkovsky's final film was dedicated to him.
"The Sacrifice" was presented at the Cannes Film Festival and received the Grand Prix Spécial du Jury, the FIPRESCI prize and the Prize of the Ecumenical Jury. As Tarkovsky was unable to attend due to his illness, the prizes were collected by his son.
Death.
In Tarkovsky's last entry (15 December 1986), he wrote: "But now I have no strength left—that is the problem". The diaries are sometimes also known as "" and were published posthumously in 1989 and in English in 1991.
Tarkovsky died in Paris on 29 December 1986. His funeral ceremony was held at the Alexander Nevsky Cathedral. He was buried on 3 January 1987 in the Russian Cemetery in Sainte-Geneviève-des-Bois in France. The inscription on his gravestone, which was erected in 1994, was conceived by Tarkovsky's wife, Larisa, reads: "To the man who saw the Angel". Larisa died in 1998 and is buried beside her husband.
A conspiracy theory emerged in Russia in the early 1990s when it was alleged that Tarkovsky did not die of natural causes, but was assassinated by the KGB. Evidence for this hypothesis includes testimonies by former KGB agents who claim that Viktor Chebrikov gave the order to eradicate Tarkovsky to curtail what the Soviet government and the KGB saw as anti-Soviet propaganda by Tarkovsky. Other evidence includes several memoranda that surfaced after the 1991 coup and the claim by one of Tarkovsky's doctors that his cancer could not have developed from a natural cause.
Tarkovsky, his wife Larisa, and actor Anatoly Solonitsyn all died from the same type of cancer. Vladimir Sharun, a sound designer for "Stalker", was convinced that all three died due to exposure to chemicals released from a chemical plant upstream from where the film was shot.
Influences and thoughts on film.
Tarkovsky became a film director during the mid and late 1950s, a period referred to as the Khrushchev Thaw, during which Soviet society opened to foreign films, literature and music, among other things. This allowed Tarkovsky to see films of European, American and Japanese directors, an experience that influenced his own film making. His teacher and mentor at the film school, Mikhail Romm, allowed his students considerable freedom and emphasized the independence of the film director.
Tarkovsky was, according to fellow student Shavkat Abdusalmov, fascinated by Japanese films. He was amazed by how every character on the screen is exceptional and how everyday events such as a Samurai cutting bread with his sword are elevated to something special and put into the limelight. Tarkovsky has also expressed interest in the art of Haiku and its ability to create "images in such a way that they mean nothing beyond themselves".
Tarkovsky was also a deeply religious Orthodox Christian, who believed great art should have a higher spiritual purpose. He was a perfectionist not given to humor or humility: his signature style was ponderous and literary, having many characters that pondered over religious themes and issues regarding faith.
Tarkovsky perceived that the art of cinema has only been truly mastered by very few filmmakers, stating in a 1970 interview with Naum Abramov that "they can be counted on the fingers of one hand". In 1972, Tarkovsky told film historian Leonid Kozlov his ten favorite films. The list is as follows: "Diary of a Country Priest" and "Mouchette" by Robert Bresson; "Winter Light", "Wild Strawberries", and "Persona" by Ingmar Bergman; "Nazarín" by Luis Buñuel; "City Lights" by Charlie Chaplin; "Ugetsu" by Kenji Mizoguchi; "Seven Samurai" by Akira Kurosawa, and "Woman in the Dunes" by Hiroshi Teshigahara. He also liked Pier Paolo Pasolini's film "The Gospel According to St. Matthew". Among his favorite directors were Buñuel, Mizoguchi, Bergman, Bresson, Kurosawa, Michelangelo Antonioni, Jean Vigo, and Carl Theodor Dreyer.
With the exception of "City Lights", the list does not contain any films of the early silent era. The reason is that Tarkovsky saw film as an art as only a relatively recent phenomenon, with the early film-making forming only a prelude. The list has also no films or directors from Tarkovsky's native Ukraine, although he rated Soviet directors such as Boris Barnet, Sergei Parajanov and Alexander Dovzhenko highly. He said of Dovzhenko's "Earth": "I have lived a lot among very simple farmers and met extraordinary people. They spread calmness, had such tact, they conveyed a feeling of dignity and displayed wisdom that I have seldom come across on such a scale. Dovzhenko had obviously understood wherein the sense of life resides. [...] This trespassing of the border between nature and mankind is an ideal place for the existence of man. Dovzhenko understood this."
He was also not a fan of blockbusters or science fiction, largely dismissing the latter for its "comic book" trappings and vulgar commercialism. However, in notable exceptions Tarkovsky praised the James Cameron blockbuster film "The Terminator", saying that its "vision of the future and the relation between man and its destiny is pushing the frontier of cinema as an art". He was critical of the "brutality and low acting skills", but was nevertheless impressed by the film. He equally liked George Lucas's "Star Wars" according to his son, Andrei A. Tarkovsky.
Cinematic style.
In a 1962 interview, Tarkovsky argued: "All art, of course, is intellectual, but for me, all the arts, and cinema even more so, must above all be emotional and act upon the heart." His films are characterized by metaphysical themes, extremely long takes, and images often considered by critics to be of exceptional beauty. Recurring motifs are dreams, memory, childhood, running water accompanied by fire, rain indoors, reflections, levitation, and characters re-appearing in the foreground of long panning movements of the camera. He once said: "Juxtaposing a person with an environment that is boundless, collating him with a countless number of people passing by close to him and far away, relating a person to the whole world, that is the meaning of cinema."
Tarkovsky incorporated levitation scenes into several of his films, most notably "Solaris". To him these scenes possess great power and are used for their photogenic value and magical inexplicability. Water, clouds, and reflections were used by him for their surreal beauty and photogenic value, as well as their symbolism, such as waves or the forms of brooks or running water. Bells and candles are also frequent symbols. These are symbols of film, sight and sound, and Tarkovsky's film frequently has themes of self-reflection.
Tarkovsky developed a theory of cinema that he called "sculpting in time". By this he meant that the unique characteristic of cinema as a medium was to take our experience of time and alter it. Unedited movie footage transcribes time in real time. By using long takes and few cuts in his films, he aimed to give the viewers a sense of time passing, time lost, and the relationship of one moment in time to another.
Up to, and including, his film "Mirror", Tarkovsky focused his cinematic works on exploring this theory. After "Mirror", he announced that he would focus his work on exploring the dramatic unities proposed by Aristotle: a concentrated action, happening in one place, within the span of a single day.
Several of Tarkovsky's films have color or black-and-white sequences. This first occurs in the otherwise monochrome "Andrei Rublev", which features a color epilogue of Rublev's authentic religious icon paintings. All of his films afterwards contain monochrome, and in "Stalker's" case sepia sequences, while otherwise being in color. In 1966, in an interview conducted shortly after finishing "Andrei Rublev", Tarkovsky dismissed color film as a "commercial gimmick" and cast doubt on the idea that contemporary films meaningfully use color. He claimed that in everyday life one does not consciously notice colors most of the time, and that color should therefore be used in film mainly to emphasize certain moments, but not all the time, as this distracts the viewer. To him, films in color were like moving paintings or photographs, which are too beautiful to be a realistic depiction of life.
Director Ingmar Bergman commented on Tarkovsky:
Contrarily, however, Bergman conceded the truth in the claim made by a critic who wrote that "with "Autumn Sonata" Bergman does Bergman", adding: "Tarkovsky began to make Tarkovsky films, and that Fellini began to make Fellini films [...] Buñuel nearly always made Buñuel films." This pastiche of one's own work has been derogatorily termed as "self-karaoke".
Vadim Yusov.
Tarkovsky worked in close collaboration with cinematographer Vadim Yusov from 1958 to 1972, and much of the visual style of Tarkovsky's films can be attributed to this collaboration. Tarkovsky would spend two days preparing for Yusov to film a single long take, and due to the preparation, usually only a single take was needed.
Sven Nykvist.
In his last film, "The Sacrifice", Tarkovsky worked with cinematographer Sven Nykvist, who had worked on many films with director Ingmar Bergman. (Nykvist was not alone: several people involved in the production had previously collaborated with Bergman, notably lead actor Erland Josephson, who had also acted for Tarkovsky in "Nostalghia".) Nykvist complained that Tarkovsky would frequently look through the camera and even direct actors through it, but ultimately stated that choosing to work with Tarkovsky was one of the best choices he had ever made.
Filmography.
Tarkovsky is mainly known as a film director. During his career he directed seven feature films, as well as three shorts from his time at VGIK. His features are:
He also wrote several screenplays. Furthermore, he directed the play "Hamlet" for the stage in Moscow, directed the opera "Boris Godunov" in London, and he directed a radio production of the short story "Turnabout" by William Faulkner. He also wrote "Sculpting in Time", a book on film theory.
Tarkovsky's first feature film was "Ivan's Childhood" in 1962. He then directed "Andrei Rublev" in 1966, "Solaris" in 1972, "Mirror" in 1975 and "Stalker" in 1979. The documentary "Voyage in Time" was produced in Italy in 1982, as was "Nostalghia" in 1983. His last film "The Sacrifice" was produced in Sweden in 1986. Tarkovsky was personally involved in writing the screenplays for all his films, sometimes with a cowriter. Tarkovsky once said that a director who realizes somebody else's screenplay without being involved in it becomes a mere illustrator, resulting in dead and monotonous films.
Unproduced screenplays.
"Concentrate".
Concentrate (, "Kontsentrat") is a never-filmed 1958 screenplay by Tarkovsky. The screenplay is based on Tarkovsky's year in the taiga as a member of a research expedition, prior to his enrollment in film school. It's about the leader of a geological expedition, who waits for the boat that brings back the concentrates collected by the expedition. The expedition is surrounded by mystery, and its purpose is a state secret.
Although some authors claim that the screenplay was filmed, according to Marina Tarkovskaya, Tarkovsky's sister (and wife of Aleksandr Gordon, a fellow student of Tarkovsky during his film school years) the screenplay was never filmed. Tarkovsky wrote the screenplay during his entrance examination at the State Institute of Cinematography (VGIK) in a single sitting. He earned the highest possible grade, "excellent" () for this work. In 1994, fragments of "Concentrate" were filmed and used in the documentary "Andrei Tarkovsky's Taiga Summer" by Marina Tarkovskaya and Aleksandr Gordon.
"Hoffmanniana".
Hoffmanniana () is a never-filmed 1974 screenplay by Tarkovsky. The screenplay is based on the life and work of German author E. T. A. Hoffmann. In 1974, an acquaintance from Tallinnfilm approached Tarkovsky to write a screenplay on a German theme. Tarkovsky considered Thomas Mann and E. T. A. Hoffmann, and also thought about Ibsen's "Peer Gynt". In the end Tarkovsky signed a contract for a script based on the life and work of Hoffmann. He planned to write the script during the summer of 1974 at his dacha. Writing was not without difficulty, less than a month before the deadline he had not written a single page. He finally finished the project in late 1974 and submitted the final script to Tallinnfilm in October.
Although the script was well received by the officials at Tallinnfilm, it was the consensus that no one but Tarkovsky would be able to direct it. The script was sent to Goskino in February 1976, and although approval was granted for proceeding with making the film, the screenplay was never realized. In 1984, during the time of his exile in the West, Tarkovsky revisited the screenplay and made a few changes. He also considered to finally direct a film based on the screenplay but ultimately dropped this idea.
Awards and commemoration.
Numerous awards were bestowed on Tarkovsky throughout his lifetime.
Under the influence of Glasnost and Perestroika, Tarkovsky was finally recognized in the Soviet Union in the Autumn of 1986, shortly before his death, by a retrospective of his films in Moscow. After his death, an entire issue of the film magazine "Iskusstvo Kino" was devoted to Tarkovsky. In their obituaries, the film committee of the Council of Ministers of the Soviet Union and the Union of Soviet Film Makers expressed their sorrow that Tarkovsky had to spend the last years of his life in exile.
Posthumously, he was awarded the Lenin Prize in 1990, one of the highest state honors in the Soviet Union. In 1989, the "Andrei Tarkovsky Memorial Prize" was established, with its first recipient being the Russian animator Yuri Norstein. In three consecutive events, the Moscow International Film Festival awarded the "Andrei Tarkovsky Award" in 1993, 1995, and 1997.
In 1996, the Andrei Tarkovsky Museum opened in Yuryevets, his childhood town. A minor planet, 3345 Tarkovskij, discovered by Soviet astronomer Lyudmila Karachkina in 1982, has been named after him.
Tarkovsky has been the subject of several documentaries. Most notable is the 1988 documentary "Moscow Elegy", by Russian film director Alexander Sokurov. Sokurov's own work has been heavily influenced by Tarkovsky. The film consists mostly of narration over stock footage from Tarkovsky's films. "Directed by Andrei Tarkovsky" is a 1988 documentary film by Michal Leszczylowski, an editor of the film "The Sacrifice". Film director Chris Marker produced the television documentary "One Day in the Life of Andrei Arsenevich" as an homage to Andrei Tarkovsky in 2000.
At the entrance to the Gerasimov Institute of Cinematography in Moscow, there is a monument that includes statues of Tarkovsky, Gennady Shpalikov and Vasily Shukshin.
Reception and legacy.
Andrei Tarkovsky and his works have received praise from many filmmakers, critics and thinkers.
The Swedish filmmaker Ingmar Bergman was quoted as saying: "Tarkovsky for me is the greatest [of us all], the one who invented a new language, true to the nature of film, as it captures life as a reflection, life as a dream".
The Japanese filmmaker Akira Kurosawa remarked on Tarkovsky's films as saying: "His unusual sensitivity is both overwhelming and astounding. It almost reaches a pathological intensity. Probably there is no equal among film directors alive now." Kurosawa also commented: "I love all of Tarkovsky's films. I love his personality and all his works. Every cut from his films is a marvelous image in itself. But the finished image is nothing more than the imperfect accomplishment of his idea. His ideas are only realized in part. And he had to make do with it."
The Iranian filmmaker Abbas Kiarostami remarked that: "Tarkovsky's works separate me completely from physical life, and are the most spiritual films I have seen".
The Polish filmmaker Krzysztof Kieślowski commented that: "Andrei Tarkovsky was one of the greatest directors of recent years," and regarded Tarkovsky's film "Ivan's Childhood" as an influence on his own work.
The Turkish filmmaker Nuri Bilge Ceylan said that when he first discovered the films of Andrei Tarkovsky as a college student, unsure of what he wanted to do with his life, he was utterly baffled by the lauded Ukrainian master. He walked out of a screening of "Solaris" at the halfway point, and stopped a VHS tape of "Mirror" at a similar juncture. Today, he considers the latter to be the greatest film ever made. "I've seen it maybe 20 times," he says.
The Armenian filmmaker Sergei Parajanov remarked that watching Tarkovsky's film, "Ivan's Childhood" was his main inspiration to become a filmmaker by saying: "I did not know how to do anything and I would not have done anything if there had not been "Ivan's Childhood"".
The Austrian filmmaker Michael Haneke voted for "Mirror" on his top 10 films in the 2002 "Sight &amp; Sound" directors' poll and later said that he has seen the picture at least 25 times.
The American filmmaker Stan Brakhage said that: "I personally think that the three greatest tasks for film in the 20th century are (1) To make the epic, that is to tell the tales of the tribes of the world. (2) To keep it personal, because only in the eccentricities of our personal lives do we have any chances at the truth. (3) To do the dream work, that is, to illuminate the borders of the unconscious. The only filmmaker I know that does all these three things equally in every film he makes is Andrei Tarkovsky, and that's why I think he's the greatest living narrative filmmaker."
The German filmmaker Wim Wenders dedicated his film "Wings of Desire" to Tarkovsky (along with François Truffaut and Yasujirō Ozu).
The French filmmaker Chris Marker directed a documentary film as a homage to Tarkovsky called "One Day in the Life of Andrei Arsenevich" and used Tarkovsky's concept of "The Zone" (from the film, "Stalker") for his 1983 film essay, "Sans Soleil".
The Greek filmmaker Theo Angelopoulos regarded Tarkovsky's film "Stalker" as one of the films that influenced him.
The Polish filmmaker Andrzej Żuławski remarked that: "If anybody influenced anybody, it’s me being influenced by Tarkovsky, not the reverse", and called Tarkovsky's film Andrei Rublev a "masterpiece".
The Greek-Australian filmmaker Alex Proyas was "extremely influenced" by Tarkovsky's work and cited "Stalker" as one of his favorite films.
The French philosopher Jean-Paul Sartre highly praised Tarkovsky's film "Ivan's Childhood", saying that it was one of the most beautiful films he had ever seen.
The Japanese anime filmmaker Mamoru Oshii, known for his works such as "Ghost in the Shell", was influenced by Tarkovsky.
The Indian-born British American novelist Salman Rushdie praised Tarkovsky and his work "Solaris" by calling it "a sci-fi masterpiece".
Film historian Steven Dillon says that much of subsequent film was deeply influenced by the films of Tarkovsky.
Mexican filmmaker Alejandro González Iñarritu is a huge fan of Tarkovsky. He once said in an interview: ""Andrei Rublev" is maybe my favorite film ever", and in another interview, he added: "I remember, the first time I saw a Tarkovsky film, I was shocked by it. I did not know what to do. I was shocked by it. I was fascinated, because suddenly I realized that film could have so many more layers to it than what I had imagined before". There are many direct references and hidden tributes to Tarkovsky's movies in Iñarritu's 2015 Oscar-winning drama "The Revenant".
Danish film director Lars von Trier is a fervent admirer of Tarkovsky. He dedicated his 2009 film "Antichrist" to him, and, while discussing it with critic David Jenkins, asked: "Have you seen "Mirror"? I was hypnotised! I've seen it 20 times. It's the closest thing I've got to a religion – to me he is a god".
The Japanese composer Ryuichi Sakamoto was an admirer of Tarkovsky’s work, describing his penultimate solo album, "async" as “a soundtrack for an imaginary Tarkovsky film.” On Tarkovsky’s overall influence on his own work, Sakamoto stated, “As I’ve been making music and trying to go deeper and deeper, I was finally able to understand what the Tarkovsky movies are about — how symphonic they are — it’s almost music. Not just the sounds — it’s a symphony of moving images and sounds. They are more complex than music.”
Film festival.
Two film festivals have been named in his honor:
References.
Bibliography
Notes

</doc>
<doc id="677" url="?curid=677" title="Ambiguity">
Ambiguity

Ambiguity is the type of meaning in which a phrase, statement, or resolution is not explicitly defined, making for several interpretations; others describe it as a concept or statement that has no real reference. A common aspect of ambiguity is uncertainty. It is thus an attribute of any idea or statement whose intended meaning cannot be definitively resolved, according to a rule or process with a finite number of steps. (The prefix "ambi-" reflects the idea of "two", as in "two meanings").
The concept of ambiguity is generally contrasted with vagueness. In ambiguity, specific and distinct interpretations are permitted (although some may not be immediately obvious), whereas with vague information it is difficult to form any interpretation at the desired level of specificity.
Linguistic forms.
Lexical ambiguity is contrasted with semantic ambiguity. The former represents a choice between a finite number of known and meaningful context-dependent interpretations. The latter represents a choice between any number of possible interpretations, none of which may have a standard agreed-upon meaning. This form of ambiguity is closely related to vagueness.
Ambiguity in human language is argued to reflect principles of efficient communication. Languages that communicate efficiently will avoid sending information that is redundant with information provided in the context. This can be shown mathematically to result in a system that is ambiguous when context is neglected. In this way, ambiguity is viewed as a generally useful feature of a linguistic system. 
Linguistic ambiguity can be a problem in law, because the interpretation of written documents and oral agreements is often of paramount importance.
Lexical ambiguity.
The lexical ambiguity of a word or phrase applies to it having more than one meaning in the language to which the word belongs. "Meaning" here refers to whatever should be represented by a good dictionary. For instance, the word "bank" has several distinct lexical definitions, including "financial institution" and "edge of a river". Or consider "apothecary". One could say "I bought herbs from the apothecary". This could mean one actually spoke to the apothecary (pharmacist) or went to the apothecary (pharmacy).
The context in which an ambiguous word is used often makes it clearer which of the meanings is intended. If, for instance, someone says "I put $100 in the bank", most people would not think someone used a shovel to dig in the mud. However, some linguistic contexts do not provide sufficient information to make a used word clearer.
Lexical ambiguity can be addressed by algorithmic methods that automatically associate the appropriate meaning with a word in context, a task referred to as word-sense disambiguation.
The use of multi-defined words requires the author or speaker to clarify their context, and sometimes elaborate on their specific intended meaning (in which case, a less ambiguous term should have been used). The goal of clear concise communication is that the receiver(s) have no misunderstanding about what was meant to be conveyed. An exception to this could include a politician whose "weasel words" and obfuscation are necessary to gain support from multiple constituents with mutually exclusive conflicting desires from his or her candidate of choice. Ambiguity is a powerful tool of political science.
More problematic are words whose multiple meanings express closely related concepts. "Good", for example, can mean "useful" or "functional" ("That's a good hammer"), "exemplary" ("She's a good student"), "pleasing" ("This is good soup"), "moral" ("a good person" versus "the lesson to be learned from a story"), "righteous", etc. "I have a good daughter" is not clear about which sense is intended. The various ways to apply prefixes and suffixes can also create ambiguity ("unlockable" can mean "capable of being opened" or "impossible to lock").
Semantic and syntactic ambiguity.
Semantic ambiguity occurs when a word, phrase or sentence, taken out of context, has more than one interpretation. In "We saw her duck" (example due to Richard Nordquist), the words "her duck" can refer either
Syntactic ambiguity arises when a sentence can have two (or more) different meanings because of the structure of the sentence—its syntax. This is often due to a modifying expression, such as a prepositional phrase, the application of which is unclear. "He ate the cookies on the couch", for example, could mean that he ate those cookies that were on the couch (as opposed to those that were on the table), or it could mean that he was sitting on the couch when he ate the cookies. "To get in, you will need an entrance fee of $10 or your voucher and your drivers' license." This could mean that you need EITHER ten dollars OR BOTH your voucher and your license. Or it could mean that you need your license AND you need EITHER ten dollars OR a voucher. Only rewriting the sentence, or placing appropriate punctuation can resolve a syntactic ambiguity.
For the notion of, and theoretic results about, syntactic ambiguity in artificial, formal languages (such as computer programming languages), see Ambiguous grammar.
Usually, semantic and syntactic ambiguity go hand in hand. The sentence "We saw her duck" is also syntactically ambiguous. Conversely, a sentence like "He ate the cookies on the couch" is also semantically ambiguous. Rarely, but occasionally, the different parsings of a syntactically ambiguous phrase result in the same meaning. For example, the command "Cook, cook!" can be parsed as "Cook (noun used as vocative), cook (imperative verb form)!", but also as "Cook (imperative verb form), cook (noun used as vocative)!". It is more common that a syntactically unambiguous phrase has a semantic ambiguity; for example, the lexical ambiguity in "Your boss is a funny man" is purely semantic, leading to the response "Funny ha-ha or funny peculiar?"
Spoken language can contain many more types of ambiguities that are called phonological ambiguities, where there is more than one way to compose a set of sounds into words. For example, "ice cream" and "I scream". Such ambiguity is generally resolved according to the context. A mishearing of such, based on incorrectly resolved ambiguity, is called a mondegreen.
Philosophy.
Philosophers (and other users of logic) spend a lot of time and effort searching for and removing (or intentionally adding) ambiguity in arguments because it can lead to incorrect conclusions and can be used to deliberately conceal bad arguments. For example, a politician might say, "I oppose taxes which hinder economic growth", an example of a glittering generality. Some will think they oppose taxes in general because they hinder economic growth. Others may think they oppose only those taxes that they believe will hinder economic growth. In writing, the sentence can be rewritten to reduce possible misinterpretation, either by adding a comma after "taxes" (to convey the first sense) or by changing "which" to "that" (to convey the second sense) or by rewriting it in other ways. The devious politician hopes that each constituent will interpret the statement in the most desirable way, and think the politician supports everyone's opinion. However, the opposite can also be true—an opponent can turn a positive statement into a bad one if the speaker uses ambiguity (intentionally or not). The logical fallacies of amphiboly and equivocation rely heavily on the use of ambiguous words and phrases.
In continental philosophy (particularly phenomenology and existentialism), there is much greater tolerance of ambiguity, as it is generally seen as an integral part of the human condition. Martin Heidegger argued that the relation between the subject and object is ambiguous, as is the relation of mind and body, and part and whole. In Heidegger's phenomenology, Dasein is always in a meaningful world, but there is always an underlying background for every instance of signification. Thus, although some things may be certain, they have little to do with Dasein's sense of care and existential anxiety, e.g., in the face of death. In calling his work Being and Nothingness an "essay in phenomenological ontology" Jean-Paul Sartre follows Heidegger in defining the human essence as ambiguous, or relating fundamentally to such ambiguity. Simone de Beauvoir tries to base an ethics on Heidegger's and Sartre's writings (The Ethics of Ambiguity), where she highlights the need to grapple with ambiguity: "as long as there have been philosophers and they have thought, most of them have tried to mask it ... And the ethics which they have proposed to their disciples has always pursued the same goal. It has been a matter of eliminating the ambiguity by making oneself pure inwardness or pure externality, by escaping from the sensible world or being engulfed by it, by yielding to eternity or enclosing oneself in the pure moment." Ethics cannot be based on the authoritative certainty given by mathematics and logic, or prescribed directly from the empirical findings of science. She states: "Since we do not succeed in fleeing it, let us, therefore, try to look the truth in the face. Let us try to assume our fundamental ambiguity. It is in the knowledge of the genuine conditions of our life that we must draw our strength to live and our reason for acting". Other continental philosophers suggest that concepts such as life, nature, and sex are ambiguous. Corey Anton has argued that we cannot be certain what is separate from or unified with something else: language, he asserts, divides what is not, in fact, separate. Following Ernest Becker, he argues that the desire to 'authoritatively disambiguate' the world and existence has led to numerous ideologies and historical events such as genocide. On this basis, he argues that ethics must focus on 'dialectically integrating opposites' and balancing tension, rather than seeking a priori validation or certainty. Like the existentialists and phenomenologists, he sees the ambiguity of life as the basis of creativity.
Literature and rhetoric.
In literature and rhetoric, ambiguity can be a useful tool. Groucho Marx's classic joke depends on a grammatical ambiguity for its humor, for example: "Last night I shot an elephant in my pajamas. How he got in my pajamas, I'll never know". Songs and poetry often rely on ambiguous words for artistic effect, as in the song title "Don't It Make My Brown Eyes Blue" (where "blue" can refer to the color, or to sadness).
In the narrative, ambiguity can be introduced in several ways: motive, plot, character. F. Scott Fitzgerald uses the latter type of ambiguity with notable effect in his novel "The Great Gatsby".
Mathematical notation.
Mathematical notation is a helpful tool that eliminates a lot of misunderstandings associated with natural language in physics and other sciences. Nonetheless, there are still some inherent ambiguities due to lexical, syntactic, and semantic reasons that persist in mathematical notation.
Names of functions.
The ambiguity in the style of writing a function should not be confused with a multivalued function, which can (and should) be defined in a deterministic and unambiguous way. Several special functions still do not have established notations. Usually, the conversion to another notation requires to scale the argument or the resulting value; sometimes, the same name of the function is used, causing confusions. Examples of such underestablished functions:
Expressions.
Ambiguous expressions often appear in physical and mathematical texts.
It is common practice to omit multiplication signs in mathematical expressions. Also, it is common to give the same name to a variable and a function, for example, Then, if one sees there is no way to distinguish whether it means formula_1 multiplied by or function formula_2 evaluated at argument equal to In each case of use of such notations, the reader is supposed to be able to perform the deduction and reveal the true meaning.
Creators of algorithmic languages try to avoid ambiguities. Many algorithmic languages (C++ and Fortran) require the character * as symbol of multiplication. The Wolfram Language used in Mathematica allows the user to omit the multiplication symbol, but requires square brackets to indicate the argument of a function; square brackets are not allowed for grouping of expressions. Fortran, in addition, does not allow use of the same name (identifier) for different objects, for example, function and variable; in particular, the expression formula_3 is qualified as an error.
The order of operations may depend on the context. In most programming languages, the operations of division and multiplication have equal priority and are executed from left to right. Until the last century, many editorials assumed that multiplication is performed first, for example, formula_4 is interpreted as in this case, the insertion of parentheses is required when translating the formulas to an algorithmic language. In addition, it is common to write an argument of a function without parenthesis, which also may lead to ambiguity.
In the scientific journal style, one uses roman letters to denote elementary functions, whereas variables are written using italics.
For example, in mathematical journals the expression formula_5 does not denote the sine function, but the product of the three variables although in the informal notation of a slide presentation it may stand for 
Commas in multi-component subscripts and superscripts are sometimes omitted; this is also potentially ambiguous notation.
For example, in the notation the reader can only infer from the context whether it means a single-index object, taken with the subscript equal to product of variables formula_6 and or it is an indication to a trivalent tensor.
Examples of potentially confusing ambiguous mathematical expressions.
An expression such as formula_7 can be understood to mean either formula_8 or Often the author's intention can be understood from the context, in cases where only one of the two makes sense, but an ambiguity like this should be avoided, for example by writing 
The expression formula_9 means formula_10 in several texts, though it might be thought to mean since formula_11 commonly means Conversely, formula_12 might seem to mean as this exponentiation notation usually denotes function iteration: in general, formula_13 means However, for trigonometric and hyperbolic functions, this notation conventionally means exponentiation of the result of function application.
The expression formula_14 can be interpreted as meaning however, it is more commonly understood to mean 
Notations in quantum optics and quantum mechanics.
It is common to define the coherent states in quantum optics with formula_15 and states with fixed number of photons with Then, there is an "unwritten rule": the state is coherent if there are more Greek characters than Latin characters in the argument, and formula_6-photon state if the Latin characters dominate. The ambiguity becomes even worse, if formula_17 is used for the states with certain value of the coordinate, and formula_18 means the state with certain value of the momentum, which may be used in books on quantum mechanics. Such ambiguities easily lead to confusions, especially if some normalized adimensional, dimensionless variables are used. Expression formula_19 may mean a state with single photon, or the coherent state with mean amplitude equal to 1, or state with momentum equal to unity, and so on. The reader is supposed to guess from the context.
Ambiguous terms in physics and mathematics.
Some physical quantities do not yet have established notations; their value (and sometimes even dimension, as in the case of the Einstein coefficients), depends on the system of notations. Many terms are ambiguous. Each use of an ambiguous term should be preceded by the definition, suitable for a specific case. Just like Ludwig Wittgenstein states in Tractatus Logico-Philosophicus: "... Only in the context of a proposition has a name meaning."
A highly confusing term is "gain". For example, the sentence "the gain of a system should be doubled", without context, means close to nothing.
The term "intensity" is ambiguous when applied to light. The term can refer to any of irradiance, luminous intensity, radiant intensity, or radiance, depending on the background of the person using the term.
Also, confusions may be related with the use of atomic percent as measure of concentration of a dopant, or resolution of an imaging system, as measure of the size of the smallest detail that still can be resolved at the background of statistical noise. See also "Accuracy and precision".
The Berry paradox arises as a result of systematic ambiguity in the meaning of terms such as "definable" or "nameable". Terms of this kind give rise to vicious circle fallacies. Other terms with this type of ambiguity are: satisfiable, true, false, function, property, class, relation, cardinal, and ordinal.
Mathematical interpretation of ambiguity.
In mathematics and logic, ambiguity can be considered to be an instance of the logical concept of underdetermination—for example, formula_20 leaves open what the value of formula_21 is—while overdetermination, except when like formula_22, is a self-contradiction, also called inconsistency, paradoxicalness, or oxymoron, or in mathematics an inconsistent system—such as which has no solution.
Logical ambiguity and self-contradiction is analogous to visual ambiguity and impossible objects, such as the Necker cube and impossible cube, or many of the drawings of M. C. Escher.
Constructed language.
Some languages have been created with the intention of avoiding ambiguity, especially lexical ambiguity. Lojban and Loglan are two related languages that have been created for this, focusing chiefly on syntactic ambiguity as well. The languages can be both spoken and written. These languages are intended to provide a greater technical precision over big natural languages, although historically, such attempts at language improvement have been criticized. Languages composed from many diverse sources contain much ambiguity and inconsistency. The many exceptions to syntax and semantic rules are time-consuming and difficult to learn.
Biology.
In structural biology, ambiguity has been recognized as a problem for studying protein conformations. The analysis of a protein three-dimensional structure consists in dividing the macromolecule into subunits called domains. The difficulty of this task arises from the fact that different definitions of what a domain is can be used (e.g. folding autonomy, function, thermodynamic stability, or domain motions), which sometimes results in a single protein having different—yet equally valid—domain assignments.
Christianity and Judaism.
Christianity and Judaism employ the concept of paradox synonymously with "ambiguity". Many Christians and Jews endorse Rudolf Otto's description of the sacred as 'mysterium tremendum et fascinans', the awe-inspiring mystery that fascinates humans. The apocryphal Book of Judith is noted for the "ingenious ambiguity" expressed by its heroine; for example, she says to the villain of the story, Holofernes, "my lord will not fail to achieve his purposes", without specifying whether "my lord" refers to the villain or to God.
The orthodox Catholic writer G. K. Chesterton regularly employed paradox to tease out the meanings in common concepts that he found ambiguous or to reveal meaning often overlooked or forgotten in common phrases: the title of one of his most famous books, "Orthodoxy" (1908), itself employed such a paradox.
Music.
In music, pieces or sections that confound expectations and may be or are interpreted simultaneously in different ways are ambiguous, such as some polytonality, polymeter, other ambiguous meters or rhythms, and ambiguous phrasing, or (Stein 2005, p. 79) any aspect of music. The music of Africa is often purposely ambiguous. To quote Sir Donald Francis Tovey (1935, p. 195), "Theorists are apt to vex themselves with vain efforts to remove uncertainty just where it has a high aesthetic value."
Visual art.
In visual art, certain images are visually ambiguous, such as the Necker cube, which can be interpreted in two ways. Perceptions of such objects remain stable for a time, then may flip, a phenomenon called multistable perception.
The opposite of such ambiguous images are impossible objects.
Pictures or photographs may also be ambiguous at the semantic level: the visual image is unambiguous, but the meaning and narrative may be ambiguous: is a certain facial expression one of excitement or fear, for instance?
Social psychology and the bystander effect.
In social psychology, ambiguity is a factor used in determining peoples' responses to various situations. High levels of ambiguity in an emergency (e.g. an unconscious man lying on a park bench) make witnesses less likely to offer any sort of assistance, due to the fear that they may have misinterpreted the situation and acted unnecessarily. Alternately, non-ambiguous emergencies (e.g. an injured person verbally asking for help) elicit more consistent intervention and assistance. With regard to the bystander effect, studies have shown that emergencies deemed ambiguous trigger the appearance of the classic bystander effect (wherein more witnesses decrease the likelihood of any of them helping) far more than non-ambiguous emergencies.
Computer science.
In computer science, the SI prefixes kilo-, mega- and giga- were historically used in certain contexts to mean either the first three powers of 1024 (1024, 10242 and 10243) contrary to the metric system in which these units unambiguously mean one thousand, one million, and one billion. This usage is particularly prevalent with electronic memory devices (e.g. DRAM) addressed directly by a binary machine register where a decimal interpretation makes no practical sense.
Subsequently, the Ki, Mi, and Gi prefixes were introduced so that binary prefixes could be written explicitly, also rendering k, M, and G "unambiguous" in texts conforming to the new standard—this led to a "new" ambiguity in engineering documents lacking outward trace of the binary prefixes (necessarily indicating the new style) as to whether the usage of k, M, and G remains ambiguous (old style) or not (new style). 1 M (where M is ambiguously or ) is "less" uncertain than the engineering value (defined to designate the interval ). As non-volatile storage devices begin to exceed 1 GB in capacity (where the ambiguity begins to routinely impact the second significant digit), GB and TB almost always mean 109 and 1012 bytes.

</doc>
<doc id="678" url="?curid=678" title="Abel">
Abel

Abel is a Biblical figure in the Book of Genesis within Abrahamic religions. He was a younger brother of Cain, and the second son of Adam and Eve, the first couple in Biblical history. He was a shepherd who offered his firstborn flock up to God as an offering. God accepted his offering but not his brother's. Cain then killed Abel out of jealousy.
According to Genesis, this was the first murder and Abel was the first dead person in the history of mankind.
Interpretations.
Jewish and Christian interpretations.
According to the narrative in Genesis, Abel ( "Hébel", in pausa "Hā́ḇel"; "Hábel"; , "Hābēl") is Eve's second son. His name in Hebrew is composed of the same three consonants as a root meaning "the air that remains after you exhale" also synonymous in Hebrew to "nothing", as stated in Ecclesiastes. Julius Wellhausen has proposed that the name is independent of the root. Eberhard Schrader had previously put forward the Akkadian (Old Assyrian dialect) "ablu" ("son") as a more likely etymology.
In Christianity, comparisons are sometimes made between the death of Abel and that of Jesus, the former thus seen as being the first martyr. In Jesus speaks of Abel as "righteous", and the Epistle to the Hebrews states that "The blood of sprinkling ... [speaks] better things than that of Abel" (). The blood of Jesus is interpreted as bringing mercy; but that of Abel as demanding vengeance (hence the curse and mark).
Abel is invoked in the litany for the dying in the Roman Catholic Church, and his sacrifice is mentioned in the Canon of the Mass along with those of Abraham and Melchizedek. The Alexandrian Rite commemorates him with a feast day on December 28.
According to the Coptic Book of Adam and Eve (at 2:1–15), and the Syriac Cave of Treasures, Abel's body, after many days of mourning, was placed in the "Cave of Treasures", before which Adam and Eve, and descendants, offered their prayers. In addition, the Sethite line of the Generations of Adam swear by Abel's blood to segregate themselves from the "unrighteous".
In the Book of Enoch (22:7), regarded by most Christian and Jewish traditions as extra-biblical, the soul of Abel is described as having been appointed as the chief of martyrs, crying for vengeance, for the destruction of the seed of Cain. A similar view is later shown in the Testament of Abraham (A:13 / B:11), where Abel has been raised to the position as the judge of the souls.
In Bereshit Rabbah (22:2), a discussion of Gen. 4:1 ff. has Rabbi Yehoshua ben Korcha mentioning that Cain was born with a twin sister, and Abel with two twin sisters. This is based on the principle that the otherwise superfluous accusative article "et" always conveys some additional teaching (Pesachim 22b). The "et"'s are parsed slightly differently in Yebamot 62a where the two "et"'s in Gen. 4:2 indicate Cain and his sister, and Abel and his (one) sister.
Sethian Gnostic interpretation.
In the Apocryphon of John, a work belonging to Sethian Gnosticism, Abel is the offspring of Yaldaboath and Eve, who is placed over the elements of water and earth as Elohim, but was only given his name as a form of deception.
Mandaean interpretation.
According to Mandaean beliefs and scriptures including the Qulasta, the Book of John and Genzā Rabbā, Abel is cognate with the angelic soteriological figure Hibil Ziwa, (, sometimes translated "Splendid Hibel"), who is spoken of as a son of Hayyi or of Manda d-Hayyi, and as a brother to Anush (Enosh) and to Sheetil (Seth), who is the son of Adam. Elsewhere, Anush is spoken of as the son of Sheetil, and Sheetil as the son of Hibil, where Hibil came to Adam and Eve as a young boy when they were still virgins, but was called their son. Hibil is an important lightworld being (uthra) who conquered the World of Darkness. As Yawar Hibil, he is one of multiple figures known as Yawar (), being so named by and after his father.
Islamic interpretation.
According to Shi'a Muslim belief, Abel ("Habeel") is buried in the Nabi Habeel Mosque, located on the west mountains of Damascus, near the Zabadani Valley, overlooking the villages of the Barada river (Wadi Barada), in Syria. Shi'a are frequent visitors of this mosque for ziyarat. The mosque was built by Ottoman Wali Ahmad Pasha in 1599.

</doc>
<doc id="679" url="?curid=679" title="Animal (disambiguation)">
Animal (disambiguation)

An animal is a multicellular, eukaryotic organism of the kingdom Animalia or Metazoa.
Animal, Animals, or The Animal may also refer to:

</doc>
<doc id="680" url="?curid=680" title="Aardvark">
Aardvark

Aardvarks ( ; Orycteropus afer) are medium-sized, burrowing, nocturnal mammals native to Africa. They have a long snout, similar to that of a pig, which is used to sniff out food. 
Aardvarks are the only living species of the order Tubulidentata, although other prehistoric species and genera of Tubulidentata are known. They are afrotheres, a clade that also includes elephants, manatees, and hyraxes. 
They are found over much of the southern two-thirds of the African continent, avoiding areas that are mainly rocky. Nocturnal feeders, aardvarks subsist on ants and termites by using their sharp claws and powerful legs to dig the insects out of their hills. Aardvarks also dig to create burrows in which to live and rear their young. 
Aardvarks are listed as "least concern" by the IUCN, although their numbers are decreasing. 
Name and taxonomy.
Name.
The aardvark is sometimes colloquially called the "African ant bear", "anteater" (not to be confused with the South American anteaters), or the "Cape anteater" after the Cape of Good Hope.
The name "aardvark" is Afrikaans () and comes from earlier Afrikaans . It means "earth pig" or "ground pig" (: , : ), because of its burrowing habits.
The name "Orycteropus" means "burrowing foot", and the name "afer" refers to Africa. The name of the aardvark's order, "Tubulidentata," comes from the tubule-style teeth.
Taxonomy.
The aardvark is not closely related to the pig; rather, it is the sole extant representative of the obscure mammalian order Tubulidentata, in which it is usually considered to form one variable species of the genus "Orycteropus", the sole surviving genus in the family Orycteropodidae. The aardvark is not closely related to the South American anteater, despite sharing some characteristics and a superficial resemblance. The similarities are the outcome of convergent evolution. The closest living relatives of the aardvark are the elephant shrews, tenrecidae, and golden moles. Along with sirenians, hyraxes, elephants, and their extinct relatives, these animals form the superorder Afrotheria. Studies of the brain have shown the similarities with Condylarthra.
Evolutionary history.
Based on his study of fossils, Bryan Patterson has concluded that early relatives of the aardvark appeared in Africa around the end of the Paleocene. The ptolemaiidans, a mysterious clade of mammals with uncertain affinities, may actually be stem-aardvarks, either as a sister clade to Tubulidentata or as a grade leading to true tubulidentates.
The first unambiguous tubulidentate was probably "Myorycteropus africanus" from Kenyan Miocene deposits. The earliest example from the genus "Orycteropus" was "Orycteropus mauritanicus", found in Algeria in deposits from the middle Miocene, with an equally old version found in Kenya. Fossils from the aardvark have been dated to 5 million years, and have been located throughout Europe and the Near East.
The mysterious Pleistocene "Plesiorycteropus" from Madagascar was originally thought to be a tubulidentate that was descended from ancestors that entered the island during the Eocene. However, a number of subtle anatomical differences coupled with recent molecular evidence now lead researchers to believe that "Plesiorycteropus" is a relative of golden moles and tenrecs that achieved an aardvark-like appearance and ecological niche through convergent evolution.
Subspecies.
The aardvark has seventeen poorly defined subspecies listed:
The 1911 Encyclopædia Britannica also mentions "O. a. capensis" or Cape ant-bear from South Africa.
Description.
The aardvark is vaguely pig-like in appearance. Its body is stout with a prominently arched back and is sparsely covered with coarse hairs. The limbs are of moderate length, with the rear legs being longer than the forelegs. The front feet have lost the pollex (or 'thumb'), resulting in four toes, while the rear feet have all five toes. Each toe bears a large, robust nail which is somewhat flattened and shovel-like, and appears to be intermediate between a claw and a hoof. Whereas the aardvark is considered digitigrade, it appears at times to be plantigrade. This confusion happens because when it squats it stands on its soles. A contributing characteristic to the burrow digging capabilities of aardvarks is an endosteal tissue called compacted coarse cancellous bone (CCCB). The stress and strain resistance provided by CCCB allows aardvarks to create their burrows, ultimately leading to a favourable environment for plants and a variety of animals.An aardvark's weight is typically between . An aardvark's length is usually between , and can reach lengths of when its tail (which can be up to ) is taken into account. It is tall at the shoulder, and has a girth of about . It is the largest member of the proposed clade Afroinsectiphilia. The aardvark is pale yellowish-grey in colour and often stained reddish-brown by soil. The aardvark's coat is thin, and the animal's primary protection is its tough skin. Its hair is short on its head and tail; however its legs tend to have longer hair. The hair on the majority of its body is grouped in clusters of 3–4 hairs. The hair surrounding its nostrils is dense to help filter particulate matter out as it digs. Its tail is very thick at the base and gradually tapers.
Head.
The greatly elongated head is set on a short, thick neck, and the end of the snout bears a disc, which houses the nostrils. It contains a thin but complete zygomatic arch. The head of the aardvark contains many unique and different features. One of the most distinctive characteristics of the Tubulidentata is their teeth. Instead of having a pulp cavity, each tooth has a cluster of thin, hexagonal, upright, parallel tubes of vasodentin (a modified form of dentine), with individual pulp canals, held together by cementum. The number of columns is dependent on the size of the tooth, with the largest having about 1,500. The teeth have no enamel coating and are worn away and regrow continuously. The aardvark is born with conventional incisors and canines at the front of the jaw, which fall out and are not replaced. Adult aardvarks have only cheek teeth at the back of the jaw, and have a dental formula of: These remaining teeth are peg-like and rootless and are of unique composition. The teeth consist of 14 upper and 12 lower jaw molars. The nasal area of the aardvark is another unique area, as it contains ten nasal conchae, more than any other placental mammal.
The sides of the nostrils are thick with hair. The tip of the snout is highly mobile and is moved by modified mimetic muscles. The fleshy dividing tissue between its nostrils probably has sensory functions, but it is uncertain whether they are olfactory or vibratory in nature. Its nose is made up of more turbinate bones than any other mammal, with between 9 and 11, compared to dogs with 4 to 5. With a large quantity of turbinate bones, the aardvark has more space for the moist epithelium, which is the location of the olfactory bulb. The nose contains nine olfactory bulbs, more than any other mammal. Its keen sense of smell is not just from the quantity of bulbs in the nose but also in the development of the brain, as its olfactory lobe is very developed. The snout resembles an elongated pig snout. The mouth is small and tubular, typical of species that feed on ants and termites. The aardvark has a long, thin, snakelike, protruding tongue (as much as long) and elaborate structures supporting a keen sense of smell. The ears, which are very effective, are disproportionately long, about long. The eyes are small for its head, and consist only of rods.
Digestive system.
The aardvark's stomach has a muscular pyloric area that acts as a gizzard to grind swallowed food up, thereby rendering chewing unnecessary. Its cecum is large. Both sexes emit a strong smelling secretion from an anal gland. Its salivary glands are highly developed and almost completely ring the neck; their output is what causes the tongue to maintain its tackiness. The female has two pairs of teats in the inguinal region.
Genetically speaking, the aardvark is a living fossil, as its chromosomes are highly conserved, reflecting much of the early eutherian arrangement before the divergence of the major modern taxa.
Habitat and range.
Aardvarks are found in sub-Saharan Africa, where suitable habitat (savannas, grasslands, woodlands and bushland) and food (i.e., ants and termites) is available. They spend the daylight hours in dark burrows to avoid the heat of the day. The only major habitat that they are not present in is swamp forest, as the high water table precludes digging to a sufficient depth. They also avoid terrain rocky enough to cause problems with digging. They have been documented as high as in Ethiopia. They are present throughout sub-Saharan Africa all the way to South Africa with few exceptions including the coastal areas of Namibia, Ivory Coast, and Ghana. They are not found in Madagascar.
Ecology and behaviour.
Aardvarks live for up to 23 years in captivity. Its keen hearing warns it of predators: lions, leopards, cheetahs, African wild dogs, hyenas, and pythons. Some humans also hunt aardvarks for meat. Aardvarks can dig fast or run in zigzag fashion to elude enemies, but if all else fails, they will strike with their claws, tail and shoulders, sometimes flipping onto their backs lying motionless except to lash out with all four feet. They are capable of causing substantial damage to unprotected areas of an attacker. They will also dig to escape as they can. Sometimes, when pressed, aardvarks can dig extremely quickly.
Feeding.
The aardvark is nocturnal and is a solitary creature that feeds almost exclusively on ants and termites (myrmecophagy); the only fruit eaten by aardvarks is the aardvark cucumber. In fact, the cucumber and the aardvark have a symbiotic relationship as they eat the subterranean fruit, then defecate the seeds near their burrows, which then grow rapidly due to the loose soil and fertile nature of the area. The time spent in the intestine of the aardvark helps the fertility of the seed, and the fruit provides needed moisture for the aardvark. They avoid eating the African driver ant and red ants. Due to their stringent diet requirements, they require a large range to survive. An aardvark emerges from its burrow in the late afternoon or shortly after sunset, and forages over a considerable home range encompassing . While foraging for food, the aardvark will keep its nose to the ground and its ears pointed forward, which indicates that both smell and hearing are involved in the search for food. They zig-zag as they forage and will usually not repeat a route for 5–8 days as they appear to allow time for the termite nests to recover before feeding on it again.
During a foraging period, they will stop to dig a V-shaped trench with their forefeet and then sniff it profusely as a means to explore their location. When a concentration of ants or termites is detected, the aardvark digs into it with its powerful front legs, keeping its long ears upright to listen for predators, and takes up an astonishing number of insects with its long, sticky tongue—as many as 50,000 in one night have been recorded. Its claws enable it to dig through the extremely hard crust of a termite or ant mound quickly. It avoids inhaling the dust by sealing the nostrils. When successful, the aardvark's long (up to ) tongue licks up the insects; the termites' biting, or the ants' stinging attacks are rendered futile by the tough skin. After an aardvark visit at a termite mound, other animals will visit to pick up all the leftovers. Termite mounds alone do not provide enough food for the aardvark, so they look for termites that are on the move. When these insects move, they can form columns long and these tend to provide easy pickings with little effort exerted by the aardvark. These columns are more common in areas of livestock or other hoofed animals. The trampled grass and dung attract termites from the "Odontotermes", "Microtermes", and "Pseudacanthotermes" genera.
On a nightly basis they tend to be more active during the first portion of night (roughly the four hours between 8:00p.m. and 12:00a.m.); however, they do not seem to prefer bright or dark nights over the other. During adverse weather or if disturbed they will retreat to their burrow systems. They cover between per night; however, some studies have shown that they may traverse as far as in a night.
Aardvarks shift their circadian rhythms to more diurnal activity patterns in response to a reduced food supply. This survival tactic may signify an increased risk of imminent mortality.
Vocalisation.
The aardvark is a rather quiet animal. However, it does make soft grunting sounds as it forages and loud grunts as it makes for its tunnel entrance. It makes a bleating sound if frightened. When it is threatened it will make for one of its burrows. If one is not close it will dig a new one rapidly. This new one will be short and require the aardvark to back out when the coast is clear.
Movement.
The aardvark is known to be a good swimmer and has been witnessed successfully swimming in strong currents. It can dig a yard of tunnel in about five minutes, but otherwise moves fairly slowly.
When leaving the burrow at night, they pause at the entrance for about ten minutes, sniffing and listening. After this period of watchfulness, it will bound out and within seconds it will be away. It will then pause, prick its ears, twisting its head to listen, then jump and move off to start foraging.
Aside from digging out ants and termites, the aardvark also excavates burrows in which to live, which generally fall into one of three categories: burrows made while foraging, refuge and resting location, and permanent homes. Temporary sites are scattered around the home range and are used as refuges, while the main burrow is also used for breeding. Main burrows can be deep and extensive, have several entrances and can be as long as . These burrows can be large enough for a person to enter. The aardvark changes the layout of its home burrow regularly, and periodically moves on and makes a new one. The old burrows are an important part of the African wildlife scene. As they are vacated, then they are inhabited by smaller animals like the African wild dog, ant-eating chat, "Nycteris thebaica" and warthogs. Other animals that use them are hares, mongooses, hyenas, owls, pythons, and lizards. Without these refuges many animals would die during wildfire season. Only mothers and young share burrows; however, the aardvark is known to live in small family groups or as a solitary creature. If attacked in the tunnel, it will escape by digging out of the tunnel thereby placing the fresh fill between it and its predator, or if it decides to fight it will roll onto its back, and attack with its claws. The aardvark has been known to sleep in a recently excavated ant nest, which also serves as protection from its predators.
Reproduction.
Aardvarks pair only during the breeding season; after a gestation period of seven months, one cub weighing around is born during May–July. When born, the young has flaccid ears and many wrinkles. When nursing, it will nurse off each teat in succession. After two weeks, the folds of skin disappear and after three, the ears can be held upright. After 5–6 weeks, body hair starts growing. It is able to leave the burrow to accompany its mother after only two weeks and eats termites at 9 weeks, and is weaned between three months and 16 weeks. At six months of age, it is able to dig its own burrows, but it will often remain with the mother until the next mating season, and is sexually mature from approximately two years of age.
Conservation.
Aardvarks were thought to have declining numbers, however, this is possibly because they are not readily seen. There are no definitive counts because of their nocturnal and secretive habits; however, their numbers seem to be stable overall. They are not considered common anywhere in Africa, but due to their large range, they maintain sufficient numbers. There may be a slight decrease in numbers in eastern, northern, and western Africa. Southern African numbers are not decreasing. It has received an official designation from the IUCN as least concern. However, they are a species in a precarious situation, as they are so dependent on such specific food; therefore if a problem arises with the abundance of termites, the species as a whole would be affected drastically.
Recent research suggests that aardvarks may be particularly vulnerable to alterations in temperature caused by climate change. Droughts negatively impact the availability of termites and ants, which comprise the bulk of an aardvark's diet. Nocturnal species faced with resource scarcity may increase their diurnal activity to spare the energy costs of staying warm at night, but this comes at the cost of withstanding high temperatures during the day. A study on aardvarks in the Kalahari Desert saw that five out of six aardvarks being studied perished following a drought. Aardvarks that survive droughts can take long periods of time to regain health and optimal thermoregulatory physiology, reducing the reproductive potential of the species.
Aardvarks handle captivity well. The first zoo to have one was London Zoo in 1869, which had an animal from South Africa.
Mythology and popular culture.
In African folklore, the aardvark is much admired because of its diligent quest for food and its fearless response to soldier ants. Hausa magicians make a charm from the heart, skin, forehead, and nails of the aardvark, which they then proceed to pound together with the root of a certain tree. Wrapped in a piece of skin and worn on the chest, the charm is said to give the owner the ability to pass through walls or roofs at night. The charm is said to be used by burglars and those seeking to visit young girls without their parents' permission. Also, some tribes, such as the Margbetu, Ayanda, and Logo, will use aardvark teeth to make bracelets, which are regarded as good luck charms. The meat, which has a resemblance to pork, is eaten in certain cultures. In the mythology of the Dagbon people of Ghana, the aardvark is believed to possess superpowers. The Dagombas believe this animal can transfigure into and interact with humans.
The ancient Egyptian god Set is usually depicted with the head of an unidentified animal, whose similarity to an aardvark has been noted in scholarship.
The titular character and his families from "Arthur", an animated television series for children based on a book series and produced by WGBH, shown in more than 180 countries, is an aardvark. In the first book of the series, "Arthur's Nose" (1976), he has a long, aardvark-like nose, but in later books, his face becomes more rounded.
Otis the Aardvark was a puppet character used on Children's BBC programming.
An aardvark features as the antagonist in the cartoon "The Ant and the Aardvark" as well as in the Canadian animated series "The Raccoons".
The supersonic fighter-bomber F-111/FB-111 was nicknamed the Aardvark because of its long nose resembling the animal. It also had similarities with its nocturnal missions flown at a very low level employing ordnance that could penetrate deep into the ground. In the US Navy, the squadron VF-114 was nicknamed the Aardvarks, flying F-4s and then F-14s. The squadron mascot was adapted from the animal in the comic strip "B.C.", which the F-4 was said to resemble.
"Cerebus the Aardvark" is a 300-issue comic book series by Dave Sim.

</doc>
<doc id="681" url="?curid=681" title="Aardwolf">
Aardwolf

The aardwolf (Proteles cristatus) is an insectivorous hyaenid species, native to East and Southern Africa. Its name means "earth-wolf" in Afrikaans and Dutch. It is also called the maanhaar-jackal (Afrikaans for "mane-jackal"), termite-eating hyena and civet hyena, based on its habit of secreting substances from its anal gland, a characteristic shared with the African civet.
Unlike many of its relatives in the order Carnivora, the aardwolf does not hunt large animals. It eats insects and their larvae, mainly termites; one aardwolf can lap up as many as 300,000 termites during a single night using its long, sticky tongue. The aardwolf's tongue has adapted to be tough enough to withstand the strong bite of termites.
The aardwolf lives in the shrublands of eastern and southern Africa – open lands covered with stunted trees and shrubs. It is nocturnal, resting in burrows during the day and emerging at night to seek food.
Taxonomy.
The aardwolf is generally classified with the hyena family Hyaenidae, though it was formerly placed in its own family Protelidae. Early on, scientists felt that it was merely mimicking the striped hyena, which subsequently led to the creation of Protelidae. Recent studies have suggested that the aardwolf probably diverged from other hyaenids early on; how early is still unclear, as the fossil record and genetic studies disagree by 10 million years.
The aardwolf is the only surviving species in the subfamily Protelinae. There is disagreement as to whether the species is monotypic, or can be divided into subspecies:
A 2006 molecular analysis indicates that it is phylogenetically the most basal of the four extant hyaenidae species.
Etymology.
The generic name "proteles" comes from two words both of Greek origin, "protos" and "teleos" which combined means "complete in front" based on the fact that they have five toes on their front feet and four on the rear. The specific name, "cristatus", comes from Latin and means "provided with a comb", relating to their mane.
Description.
The aardwolf resembles a much smaller and thinner striped hyena, with a more slender muzzle, black vertical stripes on a coat of yellowish fur, and a long, distinct mane down the midline of the neck and back. It also has one or two diagonal stripes down the fore- and hind-quarters, along with several stripes on its legs. The mane is raised during confrontations to make the aardwolf appear larger. It is missing the throat spot that others in the family have. Its lower leg (from the knee down) is all black, and its tail is bushy with a black tip.
The aardwolf is about long, excluding its bushy tail, which is about long, and stands about tall at the shoulders. An adult aardwolf weighs approximately , sometimes reaching . The aardwolves in the south of the continent tend to be smaller (about ) than the eastern version (around ). This makes the aardwolf the smallest extant member of the Hyaenidae family. The front feet have five toes each, unlike the four-toed hyena. The skull is similar in shape to those of other hyenas, though much smaller, and its cheek teeth are specialised for eating insects. It does still have canines, but, unlike other hyenas, these teeth are used primarily for fighting and defense. Its ears, which are large, are very similar to those of the striped hyena.
As an aardwolf ages, it will normally lose some of its teeth, though this has little impact on its feeding habits due to the softness of the insects that it eats.
Distribution and habitat.
Aardwolves live in open, dry plains and bushland, avoiding mountainous areas. Due to their specific food requirements, they are found only in regions where termites of the family Hodotermitidae occur. Termites of this family depend on dead and withered grass and are most populous in heavily grazed grasslands and savannahs, including farmland. For most of the year, aardwolves spend time in shared territories consisting of up to a dozen dens, which are occupied for six weeks at a time.
There are two distinct populations: one in Southern Africa, and another in East and Northeast Africa. The species does not occur in the intermediary miombo forests.
An adult pair, along with their most-recent offspring, occupies a territory of .
Behavior and ecology.
Aardwolves are shy and nocturnal, sleeping in burrows by day. They will, on occasion during the winter, become diurnal feeders. This happens during the coldest periods as they then stay in at night to conserve heat.
They are primarily solitary animals, though during mating season they form monogamous pairs which occupy a territory with their young. If their territory is infringed upon by another aardwolf, they will chase the intruder away for up to or to the border. If the intruder is caught, which rarely happens, a fight will occur, which is accompanied by soft clucking, hoarse barking, and a type of roar. The majority of incursions occur during mating season, when they can occur once or twice per week. When food is scarce, the stringent territorial system may be abandoned and as many as three pairs may occupy a single territory.
The territory is marked by both sexes, as they both have developed anal glands from which they extrude a black substance that is smeared on rocks or grass stalks in -long streaks. Aardwolves also have scent glands on the forefoot and penile pad. They often mark near termite mounds within their territory every 20 minutes or so. If they are patrolling their territorial boundaries, the marking frequency increases drastically, to once every . At this rate, an individual may mark 60 marks per hour, and upwards of 200 per night.
An aardwolf pair may have up to 10 dens, and numerous feces middens, within their territory. When they deposit excreta at their middens, they dig a small hole and cover it with sand. Their dens are usually abandoned aardvark, springhare, or porcupine dens, or on occasion they are crevices in rocks. They will also dig their own dens, or enlarge dens started by springhares. They typically will only use one or two dens at a time, rotating through all of their dens every six months. During the summer, they may rest outside their den during the night and sleep underground during the heat of the day.
Aardwolves are not fast runners nor are they particularly adept at fighting off predators. Therefore, when threatened, the aardwolf may attempt to mislead its foe by doubling back on its tracks. If confronted, it may raise its mane in an attempt to appear more menacing. It also emits a foul-smelling liquid from its anal glands.
Feeding.
The aardwolf feeds primarily on termites and more specifically on "Trinervitermes". This genus of termites has different species throughout the aardwolf's range. In East Africa, they eat "Trinervitermes bettonianus", in central Africa, they eat "Trinervitermes rhodesiensis", and in southern Africa, they eat "T. trinervoides". Their technique consists of licking them off the ground as opposed to the aardvark, which digs into the mound. They locate their food by sound and also from the scent secreted by the soldier termites. An aardwolf may consume up to 250,000 termites per night using its long, broad, sticky tongue.
They do not destroy the termite mound or consume the entire colony, thus ensuring that the termites can rebuild and provide a continuous supply of food. They often memorize the location of such nests and return to them every few months. During certain seasonal events, such as the onset of the rainy season and the cold of midwinter, the primary termites become scarce, so the need for other foods becomes pronounced. During these times, the southern aardwolf will seek out "Hodotermes mossambicus", a type of harvester termite active in the afternoon, which explains some of their diurnal behavior in the winter. The eastern aardwolf, during the rainy season, subsists on termites from the genera "Odontotermes" and "Macrotermes". They are also known to feed on other insects and larvae, and, some sources mention, occasionally eggs, small mammals and birds, but these constitute a very small percentage of their total diet. They use their wide tongues to lap surface foraging termites off of the ground and consume large quantities of sand in the process, which aids in digestion in the absence of teeth to break down their food.
Unlike other hyenas, aardwolves do not scavenge or kill larger animals. Contrary to popular myths, aardwolves do not eat carrion, and if they are seen eating while hunched over a dead carcass, they are actually eating larvae and beetles. Also, contrary to some sources, they do not like meat, unless it is finely ground or cooked for them. The adult aardwolf was formerly assumed to forage in small groups, but more recent research has shown that they are primarily solitary foragers, necessary because of the scarcity of their insect prey. Their primary source, "Trinervitermes", forages in small but dense patches of . While foraging, the aardwolf can cover about per hour, which translates to per summer night and per winter night.
Breeding.
The breeding season varies depending on location, but normally takes place during autumn or spring. In South Africa, breeding occurs in early July. During the breeding season, unpaired male aardwolves search their own territory, as well as others, for a female to mate with. Dominant males also mate opportunistically with the females of less dominant neighboring aardwolves, which can result in conflict between rival males. Dominant males even go a step further and as the breeding season approaches, they make increasingly greater and greater incursions onto weaker males' territories. As the female comes into oestrus, they add pasting to their tricks inside of the other territories, sometimes doing so more in rivals' territories than their own. Females will also, when given the opportunity, mate with the dominant male, which increases the chances of the dominant male guarding "his" cubs with her. Copulation lasts between 1 and 4.5 hours.
Gestation lasts between 89 and 92 days, producing two to five cubs (most often two or three) during the rainy season (November–December), when termites are more active. They are born with their eyes open, but initially are helpless, and weigh around . The first six to eight weeks are spent in the den with their parents. The male may spend up to six hours a night watching over the cubs while the mother is out looking for food. After three months, they begin supervised foraging, and by four months are normally independent, though they often share a den with their mother until the next breeding season. By the time the next set of cubs is born, the older cubs have moved on. Aardwolves generally achieve sexual maturity at one and a half to two years of age.
Conservation.
The aardwolf has not seen decreasing numbers and is relatively widespread throughout eastern Africa. They are not common throughout their range, as they maintain a density of no more than 1 per square kilometer, if food is abundant. Because of these factors, the IUCN has rated the aardwolf as least concern. In some areas, they are persecuted because of the mistaken belief that they prey on livestock; however, they are actually beneficial to the farmers because they eat termites that are detrimental. In other areas, the farmers have recognized this, but they are still killed, on occasion, for their fur. Dogs and insecticides are also common killers of the aardwolf.
In captivity.
Frankfurt Zoo in Germany was home to the oldest recorded aardwolf in captivity at 18 years and 11 months.

</doc>
<doc id="682" url="?curid=682" title="Adobe">
Adobe

Adobe ( ; ) is a building material made from earth and organic materials. is Spanish for mudbrick. In some English-speaking regions of Spanish heritage, such as the Southwestern United States, the term is used to refer to any kind of earthen construction, or various architectural styles like Pueblo Revival or Territorial Revival. Most adobe buildings are similar in appearance to cob and rammed earth buildings. Adobe is among the earliest building materials, and is used throughout the world.
Adobe architecture has been dated to before 5,100 BC.
Description.
Adobe bricks are rectangular prisms small enough that they can quickly air dry individually without cracking. They can be subsequently assembled, with the application of adobe mud to bond the individual bricks into a structure. There is no standard size, with substantial variations over the years and in different regions. In some areas a popular size measured weighing about ; in other contexts the size is weighing about . The maximum sizes can reach up to ; above this weight it becomes difficult to move the pieces, and it is preferred to ram the mud "in situ", resulting in a different typology known as rammed earth.
Strength.
In dry climates, adobe structures are extremely durable, and account for some of the oldest existing buildings in the world. Adobe buildings offer significant advantages due to their greater thermal mass, but they are known to be particularly susceptible to earthquake damage if they are not reinforced. Cases where adobe structures were widely damaged during earthquakes include the 1976 Guatemala earthquake, the 2003 Bam earthquake, and the 2010 Chile earthquake.
Distribution.
Buildings made of sun-dried earth are common throughout the world (Middle East, Western Asia, North Africa, West Africa, South America, Southwestern North America, Southwestern and Eastern Europe.). Adobe had been in use by indigenous peoples of the Americas in the Southwestern United States, Mesoamerica, and the Andes for several thousand years. Puebloan peoples built their adobe structures with handsful or basketsful of adobe, until the Spanish introduced them to making bricks. Adobe bricks were used in Spain from the Late Bronze and Iron Ages (eighth century BCE onwards). Its wide use can be attributed to its simplicity of design and manufacture, and economics.
Etymology.
The word "adobe" has existed for around 4,000 years with relatively little change in either pronunciation or meaning. The word can be traced from the Middle Egyptian () word "ḏbt" "mud brick" (with vowels unwritten). Middle Egyptian evolved into Late Egyptian and finally to Coptic (), where it appeared as ⲧⲱⲃⲉ "tōbə". This was adopted into Arabic as "aṭ-ṭawbu" or "aṭ-ṭūbu", with the definite article "al-" attached to the root "tuba". This was assimilated into the Old Spanish language as "adobe" , probably via Mozarabic. English borrowed the word from Spanish in the early 18th century, still referring to mudbrick construction.
In more modern English usage, the term "adobe" has come to include a style of architecture popular in the desert climates of North America, especially in New Mexico, regardless of the construction method.
Composition.
An adobe brick is a composite material made of earth mixed with water and an organic material such as straw or dung. The soil composition typically contains sand, silt and clay. Straw is useful in binding the brick together and allowing the brick to dry evenly, thereby preventing cracking due to uneven shrinkage rates through the brick. Dung offers the same advantage. The most desirable soil texture for producing the mud of adobe is 15% clay, 10–30% silt, and 55–75% fine sand. Another source quotes 15–25% clay and the remainder sand and coarser particles up to cobbles , with no deleterious effect. Modern adobe is stabilized with either emulsified asphalt or Portland cement up to 10% by weight.
No more than half the clay content should be expansive clays, with the remainder non-expansive illite or kaolinite. Too much expansive clay results in uneven drying through the brick, resulting in cracking, while too much kaolinite will make a weak brick. Typically the soils of the Southwest United States, where such construction has been widely used, are an adequate composition.
Material properties.
Adobe walls are load bearing, i.e. they carry their own weight into the foundation rather than by another structure, hence the adobe must have sufficient compressive strength. In the United States, most building codes call for a minimum compressive strength of 300 lbf/in2 (2.07 newton/mm2) for the adobe block. Adobe construction should be designed so as to avoid lateral structural loads that would cause bending loads. The building codes require the building sustain a 1 g lateral acceleration earthquake load. Such an acceleration will cause lateral loads on the walls, resulting in shear and bending and inducing tensile stresses. To withstand such loads, the codes typically call for a tensile modulus of rupture strength of at least 50 lbf/in2 (0.345 newton/mm2) for the finished block.
In addition to being an inexpensive material with a small resource cost, adobe can serve as a significant heat reservoir due to the thermal properties inherent in the massive walls typical in adobe construction. In climates typified by hot days and cool nights, the high thermal mass of adobe mediates the high and low temperatures of the day, moderating the temperature of the living space. The massive walls require a large and relatively long input of heat from the sun (radiation) and from the surrounding air (convection) before they warm through to the interior. After the sun sets and the temperature drops, the warm wall will continue to transfer heat to the interior for several hours due to the time-lag effect. Thus, a well-planned adobe wall of the appropriate thickness is very effective at controlling inside temperature through the wide daily fluctuations typical of desert climates, a factor which has contributed to its longevity as a building material.
Thermodynamic material properties have significant variation in the literature. Some experiments suggest that the standard consideration of conductivity is not adequate for this material, as its main thermodynamic property is inertia, and conclude that experimental tests should be performed over a longer period of time than usual - preferably with changing thermal jumps. There is an effective R-value for a north facing 10-in wall of R0=10 hr ft2 °F/Btu, which corresponds to thermal conductivity k=10 in x 1 ft/12 in /R0=0.33 Btu/(hr ft °F) or 0.57 W/(m K) in agreement with the thermal conductivity reported from another source. To determine the total R-value of a wall, scale R0 by the thickness of the wall in inches. The thermal resistance of adobe is also stated as an R-value for a 10-inch wall R0=4.1 hr ft2 °F/Btu. Another source provides the following properties: conductivity=0.30 Btu/(hr ft °F) or 0.52 W/(m K); specific heat capacity=0.24 Btu/(lb °F) or 1 kJ/(kg K) and density=106 lb/ft3 or 1700 kg/m3, giving heat capacity=25.4 Btu/(ft3 °F) or 1700 kJ/(m3 K). Using the average value of the thermal conductivity as k = 32 Btu/(hr ft °F) or 0.55 W/(m K), the thermal diffusivity is calculated to be 0.013 ft2/h or 3.3x10−7 m2/s.
Uses.
Poured and puddled adobe walls.
Poured and puddled adobe (puddled clay, piled earth), today called "cob", is made by placing soft adobe in layers, rather than by making individual dried bricks or using a form. "Puddle" is a general term for a clay or clay and sand-based material worked into a dense, plastic state. These are the oldest methods of building with adobe in the Americas until holes in the ground were used as forms, and later wooden forms used to make individual bricks were introduced by the Spanish.
Adobe bricks.
Bricks made from adobe are usually made by pressing the mud mixture into an open timber frame. In North America, the brick is typically about in size. The mixture is molded into the frame, which is removed after initial setting. After drying for a few hours, the bricks are turned on edge to finish drying. Slow drying in shade reduces cracking.
The same mixture, without straw, is used to make mortar and often plaster on interior and exterior walls. Some cultures used lime-based cement for the plaster to protect against rain damage.
Depending on the form into which the mixture is pressed, adobe can encompass nearly any shape or size, provided drying is even and the mixture includes reinforcement for larger bricks. Reinforcement can include manure, straw, cement, rebar, or wooden posts. Straw, cement, or manure added to a standard adobe mixture can produce a stronger, more crack-resistant brick. A test is done on the soil content first. To do so, a sample of the soil is mixed into a clear container with some water, creating an almost completely saturated liquid. The container is shaken vigorously for one minute. It is then allowed to settle for a day until the soil has settled into layers. Heavier particles settle out first, sand above, silt above that, and very fine clay and organic matter will stay in suspension for days. After the water has cleared, percentages of the various particles can be determined. Fifty to 60 percent sand and 35 to 40 percent clay will yield strong bricks. The Cooperative State Research, Education, and Extension Service at New Mexico State University recommends a mix of not more than clay, not less than sand, and never more than silt.
During the Great Depression, designer and builder Hugh W. Comstock used cheaper materials and made a specialized adobe brick called "Bitudobe." His first adobe house was built in 1936. In 1948, he published the book "Post-Adobe; Simplified Adobe Construction Combining A Rugged Timber Frame And Modern Stabilized Adobe," which described his method of construction, including how to make "Bitudobe." In 1938, he served as an adviser to the architects Franklin &amp; Kump Associates, who built the Carmel High School, which used his Post-adobe system.
Adobe wall construction.
The ground supporting an adobe structure should be compressed, as the weight of adobe wall is significant and foundation settling may cause cracking of the wall. Footing depth is to be below the ground frost level. The footing and stem wall are commonly 24 and 14 inches thick, respectively. Modern construction codes call for the use of reinforcing steel in the footing and stem wall. Adobe bricks are laid by course. Adobe walls usually never rise above two stories as they are load bearing and adobe has low structural strength. When creating window and door openings, a lintel is placed on top of the opening to support the bricks above. Atop the last courses of brick, bond beams made of heavy wood beams or modern reinforced concrete are laid to provide a horizontal bearing plate for the roof beams and to redistribute lateral earthquake loads to shear walls more able to carry the forces. To protect the interior and exterior adobe walls, finishes such as mud plaster, whitewash or stucco can be applied. These protect the adobe wall from water damage, but need to be reapplied periodically. Alternatively, the walls can be finished with other nontraditional plasters that provide longer protection. Bricks made with stabilized adobe generally do not need protection of plasters.
Adobe roof.
The traditional adobe roof has been constructed using a mixture of soil/clay, water, sand and organic materials. The mixture was then formed and pressed into wood forms, producing rows of dried earth bricks that would then be laid across a support structure of wood and plastered into place with more adobe.
Depending on the materials available, a roof may be assembled using wood or metal beams to create a framework to begin layering adobe bricks. Depending on the thickness of the adobe bricks, the framework has been preformed using a steel framing and a layering of a metal fencing or wiring over the framework to allow an even load as masses of adobe are spread across the metal fencing like cob and allowed to air dry accordingly. This method was demonstrated with an adobe blend heavily impregnated with cement to allow even drying and prevent cracking.
The more traditional flat adobe roofs are functional only in dry climates that are not exposed to snow loads. The heaviest wooden beams, called vigas, lie atop the wall. Across the vigas lie smaller members called latillas and upon those brush is then laid. Finally, the adobe layer is applied.
To construct a flat adobe roof, beams of wood were laid to span the building, the ends of which were attached to the tops of the walls. Once the vigas, latillas and brush are laid, adobe bricks are placed. An adobe roof is often laid with bricks slightly larger in width to ensure a greater expanse is covered when placing the bricks onto the roof. Following each individual brick should be a layer of adobe mortar, recommended to be at least thick to make certain there is ample strength between the brick's edges and also to provide a relative moisture barrier during rain.
Roof design evolved around 1850 in the American Southwest. Three inches of adobe mud was applied on top of the latillas, then 18 inches of dry adobe dirt applied to the roof. The dirt was contoured into a low slope to a downspout aka a 'canal'. When moisture was applied to the roof the clay particles expanded to create a waterproof membrane. Once a year it was necessary to pull the weeds from the roof and re-slope the dirt as needed.
Depending on the materials, adobe roofs can be inherently fire-proof. The construction of a chimney can greatly influence the construction of the roof supports, creating an extra need for care in choosing the materials. The builders can make an adobe chimney by stacking simple adobe bricks in a similar fashion as the surrounding walls.
In 1927, the Uniform Building Code (UBC) was adopted in the United States. Local ordinances, referencing the UBC added requirements to building with adobe. These included: restriction of building height of adobe structures to 1-story, requirements for adobe mix (compressive and shear strength) and new requirements which stated that every building shall be designed to withstand seismic activity, specifically lateral forces. By the 1980s however, seismic related changes in the California Building Code effectively ended solid wall adobe construction in California; however Post-and-Beam adobe and veneers are still being used.
Adobe around the world.
The largest structure ever made from adobe is the Arg-é Bam built by the Achaemenid Empire. Other large adobe structures are the Huaca del Sol in Peru, with 100 million signed bricks and the "ciudellas" of Chan Chan and Tambo Colorado, both in Peru.

</doc>
<doc id="683" url="?curid=683" title="Adventure">
Adventure

An adventure is an exciting experience or undertaking that is typically bold, sometimes risky. Adventures may be activities with danger such as traveling, exploring, skydiving, mountain climbing, scuba diving, river rafting, or other extreme sports. Adventures are often undertaken to create psychological arousal or in order to achieve a greater goal, such as the pursuit of knowledge that can only be obtained by such activities.
Motivation.
Adventurous experiences create psychological arousal, which can be interpreted as negative (e.g. fear) or positive (e.g. flow). For some people, adventure becomes a major pursuit in and of itself. According to adventurer André Malraux, in his "Man's Fate" (1933), "If a man is not ready to risk his life, where is his dignity?"
Similarly, Helen Keller stated that "Life is either a daring adventure or nothing."
Outdoor adventurous activities are typically undertaken for the purposes of recreation or excitement: examples are adventure racing and adventure tourism. Adventurous activities can also lead to gains in knowledge, such as those undertaken by explorers and pioneersthe British adventurer Jason Lewis, for example, uses adventures to draw global sustainability lessons from living within finite environmental constraints on expeditions to share with schoolchildren. Adventure education intentionally uses challenging experiences for learning.
Author Jon Levy suggests that an experience should meet several criteria to be considered an adventure:
Mythology and fiction.
Some of the oldest and most widespread stories in the world are stories of adventure, such as Homer's "Odyssey".
The knight errant was the form the "adventure seeker" character took in the Late Middle Ages.
Adventure fiction exhibits these "protagonist on adventurous journey" characteristics, as do many popular feature films, such as "Star Wars" and "Raiders of the Lost Ark".
Outdoors.
Adventure books may have the theme of the hero or main character going to face the wilderness or Mother Nature. Examples include books such as "Hatchet" or "My Side of the Mountain". These books are less about "questing", such as in mythology or other adventure novels, but more about surviving on their own, living off the land, gaining new experiences, and becoming closer to the natural world.
Questing.
Many adventures are based on the idea of a quest: the hero goes off in pursuit of a reward, whether it be a skill, prize, treasure, or perhaps the safety of a person. On the way, the hero must overcome various obstacles to obtain their reward. 
Video games.
In video game culture, an adventure game is a video game in which the player assumes the role of a protagonist in an interactive story driven by exploration and puzzle solving. The genre's focus on story allows it to draw heavily from other narrative-based media, literature and film, encompassing a wide variety of literary genres. Many adventure games (text and graphic) are designed for a single player, since this emphasis on story and character makes multi-player design difficult.
Nonfiction works.
From ancient times, travelers and explorers have written about their adventures. Journals which became best-sellers in their day were written, such as Marco Polo's journal "The Travels of Marco Polo" or Mark Twain's "Roughing It". Others were personal journals, only later published, such as the journals of Meriwether Lewis and William Clark or Captain James Cook's journals. There are also books written by those not directly a part of the adventure in question, such as "The Right Stuff" by Tom Wolfe or books written by those participating in the adventure but in a format other than that of a journal, such as "Conquistadors of the Useless" by Lionel Terray. Documentaries often use the theme of adventure as well.
Adventure sports.
There are many sports classified as adventure sports, due to their inherent danger and excitement. Some of these include mountain climbing, skydiving, or other extreme sports.

</doc>
<doc id="686" url="?curid=686" title="Amaltheia">
Amaltheia


</doc>
<doc id="687" url="?curid=687" title="Analysis of Variance">
Analysis of Variance


</doc>
<doc id="689" url="?curid=689" title="Asia">
Asia

Asia ( , ) is the largest continent in the world by both land area and population. It covers an area of more than 44 million square kilometers, about 30% of Earth's total land area and 8% of Earth's total surface area. The continent, which has long been home to the majority of the human population, was the site of many of the first civilizations. Its 4.7 billion people constitute roughly 60% of the world's population.
Asia shares the landmass of Eurasia with Europe, and of Afro-Eurasia with both Europe and Africa. In general terms, it is bounded on the east by the Pacific Ocean, on the south by the Indian Ocean, and on the north by the Arctic Ocean. The border of Asia with Europe is a historical and cultural construct, as there is no clear physical and geographical separation between them. It is somewhat arbitrary and has moved since its first conception in classical antiquity. The division of Eurasia into two continents reflects East–West cultural, linguistic, and ethnic differences, some of which vary on a spectrum rather than with a sharp dividing line. A commonly accepted division places Asia to the east of the Suez Canal separating it from Africa; and to the east of the Turkish Straits, the Ural Mountains and Ural River, and to the south of the Caucasus Mountains and the Caspian and Black seas, separating it from Europe.
China and India traded places as the largest economies in the world from 1 to 1800 CE. China was a major economic power for much of recorded history, with the highest GDP per capita until 1500. The Silk Road became the main east–west trading route in the Asian hinterlands while the Straits of Malacca stood as a major sea route. Asia has exhibited economic dynamism as well as robust population growth during the 20th century, but overall population growth has since fallen. Asia was the birthplace of most of the world's mainstream religions including Hinduism, Zoroastrianism, Judaism, Jainism, Buddhism, Confucianism, Taoism, Christianity, Islam, Sikhism, and many other religions.
Asia varies greatly across and within its regions with regard to ethnic groups, cultures, environments, economics, historical ties, and government systems. It also has a mix of many different climates ranging from the equatorial south via the hot deserts in parts of West Asia, Central Asia and South Asia, temperate areas in the east and the continental centre to vast subarctic and polar areas in North Asia.
Definition and boundaries.
Asia–Africa boundary.
The boundary between Asia and Africa is the Suez Canal, the Gulf of Suez, the Red Sea, and the Bab-el-Mandeb. This makes Egypt a transcontinental country, with the Sinai peninsula in Asia and the remainder of the country in Africa.
Asia–Europe boundary.
The threefold division of the Old World into Africa, Asia, and Europe has been in use since the 6th century BCE, due to Greek geographers such as Anaximander and Hecataeus. Anaximander placed the boundary between Asia and Europe along the Phasis River (the modern Rioni river) in Georgia of Caucasus (from its mouth by Poti on the Black Sea coast, through the Surami Pass and along the Kura River to the Caspian Sea), a convention still followed by Herodotus in the 5th century BCE. During the Hellenistic period, this convention was revised, and the boundary between Europe and Asia was now considered to be the Tanais (the modern Don River). This is the convention used by Roman era authors such as Posidonius, Strabo and Ptolemy.
The border between Asia and Europe was historically defined by European academics.
In Sweden, five years after Peter's death, in 1730 Philip Johan von Strahlenberg published a new atlas proposing the Ural Mountains as the border of Asia. Tatishchev announced that he had proposed the idea to von Strahlenberg. The latter had suggested the Emba River as the lower boundary. Over the next century various proposals were made until the Ural River prevailed in the mid-19th century. The border had been moved perforce from the Black Sea to the Caspian Sea into which the Ural River projects. The border between the Black Sea and the Caspian is usually placed along the crest of the Caucasus Mountains, although it is sometimes placed further north.
Asia–Oceania boundary.
The border between Asia and the region of Oceania is usually placed somewhere in the Indonesia Archipelago, specifically in Eastern Indonesia. The Wallace Line separates the Asian and Wallacea biogeographical realms, a transition zone of deep water straits between the Asian and Australian continental shelves. The Weber's Line split the region in two with regard to the balance of fauna between Asian origin or Australo-Papuan origin. Wallacea's eastern boundary with Sahul is represented by the Lydekker's Line. The Maluku Islands (except the Aru Islands) are often considered to lie on the border of southeast Asia, with the Aru Islands and Indonesian New Guinea, to the east of the Lydekker's Line, being wholly part of Oceania, as both lie on the Australian continental plate. Culturally, the Wallacea region denoted the transition between Austronesian and Melanesian people, with varying degrees of intermixing between the two. In general, the further west and coastal a region is, the stronger the Austronesian influences, and the further east and inland a region is, the stronger the Melanesian influences. The terms Southeast Asia and Oceania, devised in the 19th century, have had several vastly different geographic meanings since their inception. The chief factor in determining which islands of the Indonesian Archipelago are Asian has been the location of the colonial possessions of the various empires there (not all European). Lewis and Wigen assert, "The narrowing of 'Southeast Asia' to its present boundaries was thus a gradual process."
Asia–North America boundary.
The Bering Strait and Bering Sea separate the landmasses of Asia and North America, as well as forming the international boundary between Russia and the United States. This national and continental boundary separates the Diomede Islands in the Bering Strait, with Big Diomede in Russia and Little Diomede in the United States. The Aleutian Islands are an island chain extending westward from the Alaskan Peninsula toward Russia's Komandorski Islands and Kamchatka Peninsula. Most of them are always associated with North America, except for the westernmost Near Islands group, which is on Asia's continental shelf beyond the North Aleutians Basin and on rare occasions could be associated with Asia, which could then allow the U.S. state of Alaska as well as the United States itself to be considered a transcontinental state. The Aleutian Islands are sometimes associated with Oceania, owing to their status as remote Pacific islands, and their proximity to the Pacific Plate. This is extremely rare however, due to their non-tropical biogeography, as well as their inhabitants, who have historically been related to Indigenous Americans.
St. Lawrence Island in the northern Bering Sea belongs to Alaska and may be associated with either continent but is almost always considered part of North America, as with the Rat Islands in the Aleutian chain. At their nearest points, Alaska and Russia are separated by only .
Ongoing definition.
Geographical Asia is a cultural artifact of European conceptions of the world, beginning with the Ancient Greeks, being imposed onto other cultures, an imprecise concept causing endemic contention about what it means. Asia does not exactly correspond to the cultural borders of its various types of constituents.
From the time of Herodotus a minority of geographers have rejected the three-continent system (Europe, Africa, Asia) on the grounds that there is no substantial physical separation between them. For example, Sir Barry Cunliffe, the emeritus professor of European archeology at Oxford, argues that Europe has been geographically and culturally merely "the western excrescence of the continent of Asia".
Geographically, Asia is the major eastern constituent of the continent of Eurasia with Europe being a northwestern peninsula of the landmass. Asia, Europe and Africa make up a single continuous landmass—Afro-Eurasia (except for the Suez Canal)—and share a common continental shelf. Almost all of Europe and a major part of Asia sit atop the Eurasian Plate, adjoined on the south by the Arabian and Indian Plate and with the easternmost part of Siberia (east of the Chersky Range) on the North American Plate.
Etymology.
The term "Asia" is believed to originate in the Bronze Age placename "Assuwa" () which originally referred only to a portion of northwestern Anatolia. The term appears in Hittite records recounting how a confederation of Assuwan states including Troy unsuccessfully rebelled against the Hittite king Tudhaliya I around 1400 BCE. Roughly contemporary Linear B documents contain the term "aswia" (), seemingly in reference to captives from the same area.
Herodotus used the term Ἀσία in reference to Anatolia and the territory of the Persian Empire, in contrast to Greece and Egypt. He reports that Greeks assumed that Asia was named after the wife of Prometheus, but that Lydians say it was named after "Asies", son of Cotys, who passed the name on to a tribe at Sardis. In Greek mythology, "Asia" ("Ἀσία") or "Asie" ("Ἀσίη") was the name of a "Nymph or Titan goddess of Lydia". The Iliad (attributed by the ancient Greeks to Homer) mentions two Phrygians in the Trojan War named Asios (an adjective meaning "Asian"); and also a marsh or lowland containing a marsh in Lydia as .
The term was later adopted by the Romans, who used it in reference to the province of Asia, located in western Anatolia. One of the first writers to use Asia as a name of the whole continent was Pliny.
History.
The history of Asia can be seen as the distinct histories of several peripheral coastal regions: East Asia, South Asia, Southeast Asia, Central Asia, and West Asia. The coastal periphery was home to some of the world's earliest known civilizations, each of them developing around fertile river valleys. The civilizations in Mesopotamia, the Indus Valley and the Yellow River shared many similarities. These civilizations may well have exchanged technologies and ideas such as mathematics and the wheel. Other innovations, such as writing, seem to have been developed individually in each area. Cities, states and empires developed in these lowlands.
The central steppe region had long been inhabited by horse-mounted nomads who could reach all areas of Asia from the steppes. The earliest postulated expansion out of the steppe is that of the Indo-Europeans, who spread their languages into West Asia, South Asia, and the borders of China, where the Tocharians resided. The northernmost part of Asia, including much of Siberia, was largely inaccessible to the steppe nomads, owing to the dense forests, climate and tundra. These areas remained very sparsely populated.
The center and the peripheries were mostly kept separated by mountains and deserts. The Caucasus and Himalaya mountains and the Karakum and Gobi deserts formed barriers that the steppe horsemen could cross only with difficulty. While the urban city dwellers were more advanced technologically and socially, in many cases they could do little in a military aspect to defend against the mounted hordes of the steppe. However, the lowlands did not have enough open grasslands to support a large horsebound force; for this and other reasons, the nomads who conquered states in China, India, and the Middle East often found themselves adapting to the local, more affluent societies.
The Islamic Caliphate's defeats of the Byzantine and Persian empires led to West Asia and southern parts of Central Asia and western parts of South Asia under its control during its conquests of the 7th century. The Mongol Empire conquered a large part of Asia in the 13th century, an area extending from China to Europe. Before the Mongol invasion, Song dynasty reportedly had approximately 120 million citizens; the 1300 census which followed the invasion reported roughly 60 million people.
The Black Death, one of the most devastating pandemics in human history, is thought to have originated in the arid plains of central Asia, where it then travelled along the Silk Road.
The Russian Empire began to expand into Asia from the 17th century, and would eventually take control of all of Siberia and most of Central Asia by the end of the 19th century. The Ottoman Empire controlled Anatolia, the Levant, North Africa and the Balkans from the mid 16th century onwards. In the 17th century, the Manchu conquered China and established the Qing dynasty. The Islamic Mughal Empire and the Hindu Maratha Empire controlled much of India in the 16th and 18th centuries respectively.
Western European colonisation of Asia coincided with the Industrial Revolution in the West and the dethroning of India and China as the world's foremost economies. The British Empire became dominant in South Asia, with large parts of the region first being conquered by British traders before falling under direct British rule; extreme poverty doubled to over 50% during this era. The Middle East was contested and partitioned by the British and French, while Southeast Asia was carved up between the British, Dutch and French. Various Western powers dominated China in what later became known as the "century of humiliation", with the British-supported opium trade and later Opium Wars resulting in China being forced into an unprecedented situation of importing more than it exported. Foreign domination of China was furthered by the Empire of Japan, which controlled most of East Asia and much of Southeast Asia, New Guinea and the Pacific islands during this era; Japan's domination was enabled by its rapid rise that had taken place during the Meiji era of the late 19th century, in which it applied industrial knowledge learned from the West and thus overtook the rest of Asia.
With the end of World War II in 1945 and the wartime ruination of Europe and imperial Japan, many countries in Asia were able to rapidly free themselves of colonial rule. The independence of India came along with the carving out of a separate nation for the majority of Indian Muslims, which today has become the countries Pakistan and Bangladesh.
Some Arab countries took economic advantage of massive oil deposits that were discovered in their territory, becoming globally influential. East Asian nations (along with Singapore in Southeast Asia) became economically prosperous with high-growth "tiger economies", with China regaining its place among the top two economies of the world by the 21st century. India has grown significantly because of economic liberalisation that started in the 1990s, with extreme poverty now below 20%.
Geography.
Asia is the largest continent on Earth. It covers 9% of the Earth's total surface area (or 30% of its land area), and has the longest coastline, at . Asia is generally defined as comprising the eastern four-fifths of Eurasia. It is located to the east of the Suez Canal and the Ural Mountains, and south of the Caucasus Mountains (or the Kuma–Manych Depression) and the Caspian and Black Seas. It is bounded on the east by the Pacific Ocean, on the south by the Indian Ocean and on the north by the Arctic Ocean. Asia is subdivided into 49 countries, five of them (Georgia, Azerbaijan, Russia, Kazakhstan and Turkey) are transcontinental countries lying partly in Europe. Geographically, Russia is partly in Asia, but is considered a European nation, both culturally and politically.
The Gobi Desert is in Mongolia and the Arabian Desert stretches across much of the Middle East. The Yangtze River in China is the longest river in the continent. The Himalayas between Nepal and China is the tallest mountain range in the world. Tropical rainforests stretch across much of southern Asia and coniferous and deciduous forests lie farther north.
Main regions.
There are various approaches to the regional division of Asia. The following subdivision into regions is used, among others, by the UN statistics agency UNSD. This division of Asia into regions by the United Nations is done solely for statistical reasons and does not imply any assumption about political or other affiliations of countries and territories.
Climate.
Asia has extremely diverse climate features. Climates range from arctic and subarctic in Siberia to tropical in southern India and Southeast Asia. It is moist across southeast sections, and dry across much of the interior. Some of the largest daily temperature ranges on Earth occur in western sections of Asia. The monsoon circulation dominates across southern and eastern sections, due to the presence of the Himalayas forcing the formation of a thermal low which draws in moisture during the summer. Southwestern sections of the continent are hot. Siberia is one of the coldest places in the Northern Hemisphere, and can act as a source of arctic air masses for North America. The most active place on Earth for tropical cyclone activity lies northeast of the Philippines and south of Japan.
Economy.
Asia has the largest continental economy in the world by both GDP nominal and PPP values, and is the fastest growing economic region. , China is by far the largest economy on the continent, making up nearly half of the continent's economy by GDP nominal. It is followed by Japan, India, South Korea, Indonesia, Saudi Arabia and Turkey, which are all ranked amongst the top 20 largest economies both by nominal and PPP values. Based on Global Office Locations 2011, Asia dominated the office locations with 4 of the top 5 being in Asia: Hong Kong, Singapore, Tokyo and Seoul. Around 68 percent of international firms have an office in Hong Kong.
In the late 1990s and early 2000s, the economy of China had an average annual growth rate of more than 8%. According to economic historian Angus Maddison, India had the world's largest economy during 1000 BCE and 1 CE. India was the largest economy in the world for most of the two millennia from the 1st until 19th century, contributing 25% of the world's industrial output. China was the largest and most advanced economy on earth for much of recorded history and shared the mantle with India. For several decades in the late twentieth century Japan was the largest economy in Asia and second-largest of any single nation in the world, after surpassing the Soviet Union (measured in net material product) in 1990 and Germany in 1968. (NB: A number of supernational economies are larger, such as the European Union (EU), the North American Free Trade Agreement (NAFTA) or APEC). This ended in 2010 when China overtook Japan to become the world's second largest economy. It is forecasted that India will overtake Japan in terms of nominal GDP by 2027.
In the late 1980s and early 1990s, Japan's GDP by currency exchange rates was almost as large as that of the rest of Asia combined. In 1995, Japan's economy nearly equaled that of the US as the largest economy in the world for a day, after the Japanese currency reached a record high of 79 yen/US$. Economic growth in Asia since World War II to the 1990s had been concentrated in Japan as well as the four regions of South Korea, Taiwan, Hong Kong and Singapore located in the Pacific Rim, known as the Asian tigers, which are now all considered developed economies, having amongst the highest GDP per capita in Asia.
Asia is the largest continent in the world by a considerable margin, and it is rich in natural resources, such as petroleum, forests, fish, water, rice, copper and silver. Manufacturing in Asia has traditionally been strongest in East and Southeast Asia, particularly in China, Taiwan, South Korea, Japan, India, the Philippines, and Singapore. Japan and South Korea continue to dominate in the area of multinational corporations, but increasingly the PRC and India are making significant inroads. Many companies from Europe, North America, South Korea and Japan have operations in Asia's developing countries to take advantage of its abundant supply of cheap labour and relatively developed infrastructure.
According to Citigroup in 2011, 9 of 11 Global Growth Generators countries came from Asia driven by population and income growth. They are Bangladesh, China, India, Indonesia, Iraq, Mongolia, the Philippines, Sri Lanka and Vietnam. Asia has three main financial centers: Hong Kong, Tokyo and Singapore. Call centers and business process outsourcing (BPOs) are becoming major employers in India and the Philippines due to the availability of a large pool of highly skilled, English-speaking workers. The increased use of outsourcing has assisted the rise of India and the China as financial centers. Due to its large and extremely competitive information technology industry, India has become a major hub for outsourcing.
Trade between Asian countries and countries on other continents is largely carried out on the sea routes that are important for Asia. Individual main routes have emerged from this. The main route leads from the Chinese coast south via Hanoi to Jakarta, Singapore and Kuala Lumpur through the Strait of Malacca via the Sri Lankan Colombo to the southern tip of India via Malé to East Africa Mombasa, from there to Djibouti, then through the Red Sea over the Suez Canal into Mediterranean, there via Haifa, Istanbul and Athens to the upper Adriatic to the northern Italian hub of Trieste with its rail connections to Central and Eastern Europe or further to Barcelona and around Spain and France to the European northern ports. A far smaller part of the goods traffic runs via South Africa to Europe. A particularly significant part of the Asian goods traffic is carried out across the Pacific towards Los Angeles and Long Beach. In contrast to the sea routes, the Silk Road via the land route to Europe is on the one hand still under construction and on the other hand is much smaller in terms of scope. Intra-Asian trade, including sea trade, is growing rapidly.
In 2010, Asia had 3.3 million millionaires (people with net worth over US$1 million excluding their homes), slightly below North America with 3.4 million millionaires. In 2011, Asia topped Europe in number of millionaires.
Citigroup in The Wealth Report 2012 stated that Asian centa-millionaire overtook North America's wealth for the first time as the world's "economic center of gravity" continued moving east. At the end of 2011, there were 18,000 Asian people mainly in Southeast Asia, China and Japan who have at least $100 million in disposable assets, while North America with 17,000 people and Western Europe with 14,000 people.
Tourism.
With growing Regional Tourism with domination of Chinese visitors, MasterCard has released Global Destination Cities Index 2013 with 10 of 20 are dominated by Asia and Pacific Region Cities and also for the first time a city of a country from Asia (Bangkok) set in the top-ranked with 15.98 million international visitors.
Demographics.
East Asia had by far the strongest overall Human Development Index (HDI) improvement of any region in the world, nearly doubling average HDI attainment over the past 40 years, according to the report's analysis of health, education and income data. China, the second highest achiever in the world in terms of HDI improvement since
1970, is the only country on the "Top 10 Movers" list due to income rather than health or education achievements. Its per capita income increased a stunning 21-fold over the last four decades, also lifting hundreds of millions out of income poverty. Yet it was not among the region's top performers in improving school enrollment and life expectancy.
Nepal, a South Asian country, emerges as one of the world's fastest movers since 1970 mainly due to health and education achievements. Its present life expectancy is 25 years longer than in the 1970s. More than four of every five children of school age in Nepal now attend primary school, compared to just one in five 40 years ago.
 Hong Kong ranked highest among the countries grouped on the HDI (number 7 in the world, which is in the "very high human development" category), followed by Singapore (9), Japan (19) and South Korea (22). Afghanistan (155) ranked lowest amongst Asian countries out of the 169 countries assessed.
Languages.
Asia is home to several language families and many language isolates. Most Asian countries have more than one language that is natively spoken. For instance, according to Ethnologue, more than 700 languages are spoken in Indonesia, more than 400 languages spoken in India, and more than 100 are spoken in the Philippines. China has many languages and dialects in different provinces.
Religions.
Many of the world's major religions have their origins in Asia, including the five most practiced in the world (excluding irreligion), which are Christianity, Islam, Hinduism, Chinese folk religion (classified as Confucianism and Taoism), and Buddhism. Asian mythology is complex and diverse. The story of the Great Flood for example, as presented to Jews in the Hebrew Bible in the narrative of Noah—and later to Christians in the Old Testament, and to Muslims in the Quran—is earliest found in Mesopotamian mythology, in the Enûma Eliš and "Epic of Gilgamesh". Hindu mythology similarly tells about an avatar of Vishnu in the form of a fish who warned Manu of a terrible flood. Ancient Chinese mythology also tells of a Great Flood spanning generations, one that required the combined efforts of emperors and divinities to control.
Abrahamic.
The Abrahamic religions including Judaism, Christianity, Islam, Druze faith, and Baháʼí Faith originated in West Asia.
Judaism, the oldest of the Abrahamic faiths, is practiced primarily in Israel, the indigenous homeland and historical birthplace of the Hebrew nation: which today consists both of those Jews who remained in the Middle East and those who returned from diaspora in Europe, North America, and other regions; though various diaspora communities persist worldwide. Jews are the predominant ethnic group in Israel (75.6%) numbering at about 6.1 million, although the levels of adherence to Jewish religion vary. Outside of Israel there are small ancient Jewish communities in Turkey (17,400), Azerbaijan (9,100), Iran (8,756), India (5,000) and Uzbekistan (4,000), among many other places. In total, there are 14.4–17.5 million (2016, est.) Jews alive in the world today, making them one of the smallest Asian minorities, at roughly 0.3 to 0.4 percent of the total population of the continent.
Christianity is a widespread religion in Asia with more than 286 million adherents according to Pew Research Center in 2010, and nearly 364 million according to Britannica Book of the Year 2014. Christians constitute around 12.6% of the total population of Asia. In the Philippines and East Timor, Roman Catholicism is the predominant religion; it was introduced by the Spaniards and the Portuguese, respectively. In Armenia and Georgia, Eastern Orthodoxy is the predominant religion. In the Middle East, such as in the Levant, Anatolia and Fars, Syriac Christianity (Church of the East) and Oriental Orthodoxy are prevalent minority denominations, which are both Eastern Christian sects mainly adhered to Assyrian people or Syriac Christians. Vibrant indigenous minorities in West Asia are adhering to the Eastern Catholic Churches and Eastern Orthodoxy. Saint Thomas Christians in India trace their origins to the evangelistic activity of Thomas the Apostle in the 1st century. Significant Christian communities also found in Central Asia, South Asia, Southeast Asia and East Asia.
Islam, which originated in the Hejaz located in modern-day Saudi Arabia, is the second largest and most widely-spread religion in Asia with at least 1 billion Muslims constituting around 23.8% of the total population of Asia. With 12.7% of the world Muslim population, the country currently with the largest Muslim population in the world is Indonesia, followed by Pakistan (11.5%), India (10%), Bangladesh, Iran and Turkey. Mecca, Medina and Jerusalem are the three holiest cities for Islam in all the world. The Hajj and Umrah attract large numbers of Muslim devotees from all over the world to Mecca and Medina. Iran is the largest Shi'a country.
The Druze Faith or Druzism originated in West Asia, is a monotheistic religion based on the teachings of figures like Hamza ibn-'Ali ibn-Ahmad and Al-Hakim bi-Amr Allah, and Greek philosophers such as Plato and Aristotle. The number of Druze people worldwide is around one million. About 45% to 50% live in Syria, 35% to 40% live in Lebanon, and less than 10% live in Israel. Recently there has been a growing Druze diaspora.
The Baháʼí Faith originated in Asia, in Iran (Persia), and spread from there to the Ottoman Empire, Central Asia, India, and Burma during the lifetime of Bahá'u'lláh. Since the middle of the 20th century, growth has particularly occurred in other Asian countries, because Baháʼí activities in many Muslim countries has been severely suppressed by authorities. Lotus Temple is a big Baháʼí temple in India.
Indian and East Asian religions.
Almost all Asian religions have philosophical character and Asian philosophical traditions cover a large spectrum of philosophical thoughts and writings. Indian philosophy includes Hindu philosophy and Buddhist philosophy. They include elements of nonmaterial pursuits, whereas another school of thought from India, Cārvāka, preached the enjoyment of the material world. The religions of Hinduism, Buddhism, Jainism and Sikhism originated in India, South Asia. In East Asia, particularly in China and Japan, Confucianism, Taoism and Zen Buddhism took shape.
, Hinduism has around 1.1 billion adherents. The faith represents around 25% of Asia's population and is the largest religion in Asia. However, it is mostly concentrated in South Asia. Over 80% of the populations of both India and Nepal adhere to Hinduism, alongside significant communities in Bangladesh, Pakistan, Bhutan, Sri Lanka and Bali, Indonesia. Many overseas Indians in countries such as Burma, Singapore and Malaysia also adhere to Hinduism.
Buddhism has a great following in mainland Southeast Asia and East Asia. Buddhism is the religion of the majority of the populations of Cambodia (96%), Thailand (95%), Burma (80–89%), Japan (36–96%), Bhutan (75–84%), Sri Lanka (70%), Laos (60–67%) and Mongolia (53–93%). Taiwan (35–93%), South Korea (23–50%), Malaysia (19–21%), Nepal (9–11%), Vietnam (10–75%), China (20–50%), North Korea (2–14%), and small communities in India and Bangladesh. The Communist-governed countries of China, Vietnam and North Korea are officially atheist, thus the number of Buddhists and other religious adherents may be under-reported.
Jainism is found mainly in India and in overseas Indian communities such as the United States and Malaysia. Sikhism is found in Northern India and amongst overseas Indian communities in other parts of Asia, especially Southeast Asia. Confucianism is found predominantly in mainland China, South Korea, Taiwan and in overseas Chinese populations. Taoism is found mainly in mainland China, Taiwan, Malaysia and Singapore. In many Chinese communities, Taoism is easily syncretized with Mahayana Buddhism, thus exact religious statistics are difficult to obtain and may be understated or overstated.
Modern conflicts and events.
Some of the events pivotal in Asia related to the relationship with the outside world in the post-Second World War were:
Led to the creation of India and Pakistan, shaping the political landscape in South Asia.
Fought over the princely state of Jammu and Kashmir, setting the stage for future conflicts.
Culminated in the establishment of the People's Republic of China under the Communist Party.
Involved international forces and led to the division of the Korean Peninsula.
Ended with the defeat of French colonial forces and the partition of Vietnam.
A protracted conflict with significant global implications, especially during the Cold War.
Conflict between China and Vietnam following Vietnam's invasion of Cambodia.
Involved Indonesia's annexation and subsequent independence through a UN-backed referendum.
Soviet intervention in Afghanistan, contributing to the rise of the mujahideen.
Long-lasting conflict with regional and international implications.
Resulted from Iraq's invasion of Kuwait, with international intervention.
Marked the end of the Cold War and the emergence of independent states.
U.S.-led intervention post-9/11 with long-lasting consequences.
Led to the overthrow of Saddam Hussein and subsequent instability.
Series of uprisings and protests across the Arab world, influencing regional dynamics.
Ongoing conflict with widespread humanitarian implications.
Culture.
The culture of Asia is a diverse blend of customs and traditions that have been practiced by the various ethnic groups of the continent for centuries. The continent is divided into six geographic sub-regions: Central Asia, East Asia, North Asia, South Asia, Southeast Asia, and West Asia. These regions are defined by their cultural similarities, including common religions, languages, and ethnicities. West Asia, also known as Southwest Asia or the Middle East, has cultural roots in the ancient civilizations of the Fertile Crescent and Mesopotamia, which gave rise to the Persian, Arab, Ottoman empires, as well as the Abrahamic religions of Judaism, Christianity and Islam. These civilizations, which are located in the Hilly flanks, are among the oldest in the world, with evidence of farming dating back to around 9000 BCE. Despite the challenges posed by the vast size of the continent and the presence of natural barriers such as deserts and mountain ranges, trade and commerce have helped to create a Pan-Asian culture that is shared across the region.
Nobel prizes.
The polymath Rabindranath Tagore, a Bengali poet, dramatist, and writer from Santiniketan, now in West Bengal, India, became in 1913 the first Asian Nobel laureate. He won his Nobel Prize in Literature for notable impact his prose works and poetic thought had on English, French, and other national literatures of Europe and the Americas. He is also the writer of the national anthems of Bangladesh and India.
Other Asian writers who won Nobel Prize for literature include Yasunari Kawabata (Japan, 1968), Kenzaburō Ōe (Japan, 1994), Gao Xingjian (China, 2000), Orhan Pamuk (Turkey, 2006), and Mo Yan (China, 2012). Some may consider the American writer, Pearl S. Buck, an honorary Asian Nobel laureate, having spent considerable time in China as the daughter of missionaries, and based many of her novels, namely "The Good Earth" (1931) and "The Mother" (1933), as well as the biographies of her parents for their time in China, "The Exile" and "Fighting Angel", all of which earned her the Literature prize in 1938.
Also, Mother Teresa of India and Shirin Ebadi of Iran were awarded the Nobel Peace Prize for their significant and pioneering efforts for democracy and human rights, especially for the rights of women and children. Ebadi is the first Iranian and the first Muslim woman to receive the prize. Another Nobel Peace Prize winner is Aung San Suu Kyi from Burma for her peaceful and non-violent struggle under a military dictatorship in Burma. She is a nonviolent pro-democracy activist and leader of the National League for Democracy in Burma (Myanmar) and a noted prisoner of conscience. She is a Buddhist and was awarded the Nobel Peace Prize in 1991. Chinese dissident Liu Xiaobo was awarded the Nobel Peace Prize for "his long and non-violent struggle for fundamental human rights in China" on 8 October 2010. He is the first Chinese citizen to be awarded a Nobel Prize of any kind while residing in China. In 2014, Kailash Satyarthi from India and Malala Yousafzai from Pakistan were awarded the Nobel Peace Prize "for their struggle against the suppression of children and young people and for the right of all children to education".
Sir C.V. Raman is the first Asian to get a Nobel prize in Sciences. He won the Nobel Prize in Physics "for his work on the scattering of light and for the discovery of the effect named after him".
Japan has won the most Nobel Prizes of any Asian nation with 24 followed by India which has won 13.
Amartya Sen (born 3 November 1933) is an Indian economist who was awarded the 1998 Nobel Memorial Prize in Economic Sciences for his contributions to welfare economics and social choice theory, and for his interest in the problems of society's poorest members.
Other Asian Nobel Prize winners include Subrahmanyan Chandrasekhar, Abdus Salam, Robert Aumann, Menachem Begin, Aaron Ciechanover, Avram Hershko, Daniel Kahneman, Shimon Peres, Yitzhak Rabin, Ada Yonath, Yasser Arafat, José Ramos-Horta and Bishop Carlos Filipe Ximenes Belo of Timor Leste, Kim Dae-jung, and 13 Japanese scientists. Most of the said awardees are from Japan and Israel except for Chandrasekhar and Raman (India), Abdus Salam (Pakistan), Arafat (Palestinian Territories), Kim (South Korea), and Horta and Belo (Timor Leste).
In 2006, Muhammad Yunus of Bangladesh was awarded the Nobel Peace Prize for the establishment of Grameen Bank, a community development bank that lends money to poor people, especially women. He is known for the concept of micro credit which, allows poor and destitute people to borrow money. The borrowers pay back money within the specified period and defaulting is very low. The Dalai Lama received the Nobel Peace Prize, in Oslo, Norway in 1989.
States of Asia.
Within the above-mentioned states are several partially recognized countries with limited to no international recognition. None of them are members of the UN:
The most democratic countries in Asia are Japan, Taiwan and Israel according to the V-Dem Democracy indices in 2024.
See also.
Special topics:
Lists:
Projects

</doc>
<doc id="690" url="?curid=690" title="Aruba">
Aruba

Aruba ( , , ), officially the Country of Aruba (; ), is a constituent country within the Kingdom of the Netherlands, situated in the south of the Caribbean Sea. Aruba is located approximately north of the Venezuelan peninsula of Paraguaná and northwest of Curaçao.
Aruba measures in length from its northwestern to its southeastern end and is across at its widest point. Alongside Bonaire and Curaçao, Aruba forms a group referred to as the ABC islands. The Dutch Caribbean encompasses the ABC islands, along with the other three Dutch substantial islands in the Caribbean, the SSS islands. Aruba contributes to about one-third of the population of the Dutch Caribbean.
In 1986, it became a constituent country within the Kingdom of the Netherlands, and acquired the formal name the Country of Aruba.
As one of the four countries in the Kingdom of the Netherlands, alongside the Netherlands, Curaçao, and Sint Maarten, Aruba shares Dutch nationality with its citizens. Aruba lacks administrative subdivisions but is divided into eight regions for census purposes with Oranjestad as its capital.
In contrast to much of the Caribbean, which experiences humid tropical climates, Aruba has a dry climate with an arid xeric landscape. The relatively warm and sunny weather persists throughout the year. Aruba has an area of and a dense population of 108,166 as per the 2020 census.
Etymology.
The name Aruba most likely came from the Caquetío "Oruba" which means "Well situated island", seeing as it was the Caquetío who were present on the island when it was first set foot upon by Alonso de Ojeda. Between 1529 and the signing of the Treaty of Westphalia (1648), the name "Isla de Oruba" was used for the island by the Spanish. After the signing, the island was ceded to the Dutch and gradually its name changed to Aruba.
There were many different names for Aruba used by other Amerindian groups, all of which could have contributed to the present-day name Aruba. Another Caquetío name for the island was "Oibubia" which means "Guided island". The Taino name for the island was "Arubeira". The Kalinago also had two names for the island "Ora Oubao" which means "Shell island" and "Oirubae" which means "Companion of Curaçao".
A common misconception is that the name "Aruba" came from "Oro hubo," (Spanish for "There was gold once"). However, the Spanish declared these islands "islas inútiles", meaning "useless islands", due to their lack of mineral wealth. It was not until, a century later, in 1824, that gold was discovered on Aruba by a twelve-year-old herder named Willem Rasmijn; leading to the Aruban Gold Rush.
History.
Pre-ceramic age.
In Aruba's prehistoric era, there were distinct periods: the Archaic or Pre-Ceramic and the Neo-Indian or Ceramic (Dabajuroïd)  period. The Archaic occupation of Aruba continued well into the first millennium AD, which is relatively late in compared to other parts of the insular Caribbean. The archaic lifestyle revolved around a food economy based on fishing, hunting, and gathering, with a strong emphasis on marine resources. Ceramics were absent, as was horticulture and agriculture. These people not only chipped stones but also polished and sharpened them. Weapons and tools were predominantly crafted from stone. Sharp-edged ax blades, chisels, and knives were commonly used, with the knives distinguishable by their elongated shape and flat blades. One notable site, Sero Muskita, yielded a tool that is older than other archaic age sites on the island. The finishing techniques and shape of this tool resembles one found at Arikok, suggesting a date before approximately 2000 BC. The presence of these tools on the island may be due to occasional visits from the mainland. In total, 33 archaic age sites have been identified on Aruba.
Early human migration and cultural exchange.
During this period, the Leeward Islands maintained connections and engaged in trade with mainland South America, particularly with partners in the present-day Falcón-Zulia state in Venezuela and possibly the La Guajira Peninsula (Venezuela/Colombia). The specific language group to which they belonged remains uncertain. This theory is supported by the discovery of 60 to 70 Amerindian cemetery burial grounds in Malmok and Canashito, Aruba. Among these burial sites, five are found smaller Canashito burial ground and are dated between 100 BC to 100 AD. Interestingly, isotopic research revealed that one of the individuals buried there was not from Aruba and had a different diet compared to the other four individuals of Aruban origin. This finding suggests that early human migration and cultural exchange were already part of the cultural pattern of these archaic Indians at an early stage.
Moving forward to a later period, the burial site in Malmok, dates back to the period between 450 and 1000 AD. The Arubans of that time had a short and stocky physique, with adult men averaging in height and women averaging . The burial customs offer insight into the social dynamics of the archaic island inhabitants. Based on the burial patterns, it was deduced that they traveled in clans of fifteen to thirty people. These groups were led by an adult man, who was buried at the center of the cluster. His elevated status was emphasized by the presence of several stones marking his grave. The rest of the family group was buried around him.
Neo-Indian period: the Caquetío.
The archaic population disappeared from Aruba from the archeological record around 950 AD, shortly after the arrival of the neo-Indian—Caquetío. It is clear that the Caquetíos had a superior culture in socio-economic and technological terms. It is possible that the new Caquetío Indians lived alongside the archaic Indians on Aruba for a time and that they were ultimately displaced or assimilated.
Arrival of the Caquetío.
In the year 1500, the Caquetío people lived on Aruba. They belonged to the Arawak people. The origin of Arawak civilization (a name based on a linguistic classification) is located in the central Amazon region. Between 1500 and 500 BC, the influence of the Arawaks had expanded to the Caribbean basin and the Guianas. Between 850 and 1000 AD, Caquetío Indians migrated from western Venezuela, probably from the Paraguaná and Guajire peninsulas, to the Leeward Antilles. They belonged to the Arawak-Maipure language family. The name Caquetío refers to how this group referred to themselves during their first contact with Europeans. They had longer and narrower skulls than the archaic population, and their height was up to . The newcomers brought pottery and agriculture to the islands and are therefore classified as part of the neo-Indian period.
Caquetío chiefdom.
The area over which the legendary cacique Manaure exercised his authority was the coastal region of the current state Falcón-Zulia at Venezuela, including the Paraguaná peninsula, as well as Aruba, Curaçao and Bonaire. The Caquetío people had a highly developed process of state formation. They had a chiefdom, which in human evolution is often a precursor to a kingdom, where central leaders—"paramount chiefs"—controlled multiple subordinate political-administrative units. The emphasis was more on the political and religious alliances between indigenous communities than on the military control or subjugate vast territories. At the head of the Caquetío chiefdom was a spiritual leader called "diao" who had both secular and religious authority in modern terms. He was endowed with powers that could influence nature: a shaman. The diao position was hereditary. By being allowed to marry multiple wives, the diao was able to establish and maintain political alliances with other groups, tribes, or villages. The chiefdom was centralized in its design, but not based on authoritarian or violence-based subjugation. The Spanish conquistador interrupted this process of expansion at the time of the European contact (AD 1499–1535).
Political units and governance.
The Caquetío territory consisted of several, and therefore smaller, political units that were under the authority of lower "second-tier chiefs" who were subordinate to the highest authority. How the central authority was exercised over the units is not clear. However, there are reports from the contact period that suggest the diao did not exert his power over the lower units in arbitrary manner. Likely there was a form of consultation between the diao and lower leaders. In the 16th century, two sub-units, the Guaranos and Amuayes, lived on the Paraguaná peninsula. Aruba, which is less than 30 kilometers away from Paraguaná, was previously connected to one of these units.
After the Diao and the regional sub-units, such as the mentioned Guaranaos an Amuayes in Paraguaná, the village formed the third level of governance in the hierarchy of the chiefdom. Aruba had (not simultaneously) five villages: three larger ones Ceri Noca (Santa Cruz), Tanki Flip (Noord), and Savaneta, and two smaller ones near Tanki Leendert and Parkietenbos, which have not yet been systematically studied. The location of Aruban villages varied. They were situated in places where beneficial agriculture land was available and where the most favorable hydrological conditions prevailed, such as where several "rooi" (gullies) came together and where relatively much water was available.
Agriculture, trade, and network.
The Caquetío people probably used a shifting cultivation farming method, also known as "slash-and-burn". The yields from agriculture and fishing were supplemented by engaging in trade of raw materials and artifacts that were not locally available or producible. Sixteenth century sources indicate that the Caquetíos traded in, among other things, salt, canoes, tobacco, and beads. The Leeward Caquetíos certainly did not live in isolation, but formed outlying regions of a dynamic chiefdom with regional trading networks.
Burial practices.
In 1882, a French explorer named Alphonse L. Pinart, documented an account provided by an old Aruba Indian. According to the Indian's account, witnessed at the former Indian encampment at "Saboneta" (Savaneta), a native female was inhumed in one of the large conical ollas. Her body was doubled up inside the vase, with the head protruding through the orifice. Subsequently, a smaller urn was placed upside down on the head and the entire burial was covered with earth.
The Caquetío people were buried in clusters, both within and potentially outside village boundaries. At times, there was a secondary burial, possibly reserved for exceptional individuals. In the primary burial, the deceased were buried in a large pot, covered with a smaller pot placed on top. In a secondary burial, the body was initially buried without a pot, and after a few months or years, the bones were exhumed and reburied in smaller pots for a second time. Some pots contained grave offerings such as axes, shells, and pottery. Remarkably, the secondary burial method was practiced until recently in South America. The striking similarity between the Neo-Indian burial practices in Aruba and the post-Columbian variant in Guajira justifies the assumption that the similar beliefs about life after death existed in both societies.
Last indigenous Aruban.
Nicolaas Pyclas was regarded as the last known indigenous Aruban. Pyclas spoke and understood the extinct language of the original inhabitants of Aruba, adhering to their way of life and customs. He resided in a hut in "Savonet" (Savaneta). His diet included sea snails, such as "cocolishi" ("Cerun uva") and "carco" ("Aliger gigas"), as well as wild herbs. Pyclas rejected any involvement in religious practices. Around 1840, he was found dead hanging from a tree branch not far from his hut. Estimated to be approximately 50 years old, he was buried in situ and was not properly buried due to the hard rocky surface, he was only covered with a layer of earth and stones. Pyclas' skull was gifted to the former "Rijks Ethnographisch Museum", presently National Museum of Ethnology in Leiden, with the mediation of A.J. van Koolwijk.
Spanish period.
New route to India.
It is known that Christopher Columbus was not searching for a new continent, but for the shortest route to India. India had been the spearhead of European trade expansion and the foreign policy of the Spanish Crown since the travels of Marco Polo a century earlier. India, China, and Japan formed the focal point of medieval ideas about boundless riches; cities with houses covered in gold, and islands with inexhaustible amounts of spices, pearls, and silk. The suspicion arose that India could be reached via the relatively short route to the west, across the ocean of Atlantis.
During his third voyage to the New World, Columbus was searching for the southern route to India and explored the Paria Peninsula (Eastern Venezuela) and the Orinoco region, where he discovered the fresh river water of the Orinoco delta. The suspicion arose that he had not found islands off the coast of India but a much more extensive land mass; an extension of Asia. Columbus did not realize that this was an unknown continent. Characteristic of his Christian, medieval perspective, Columbus solved the puzzle by assuming that he had discovered the earthly paradise. The earthly paradise was inaccessible to humans without God's permission. Columbus experienced the geographical discovery of the New World in Christian terms and assigned himself a special role assigned by a divine power. With the discovery of the Americas the myths of the Golden Age, Atlantis, and the earthly paradise moved from Asia to the New World. He died on May 20, 1506, believing that he had found new islands of the coast of or possibly a peninsula of India—pre-islands: "Ant-ilha." These Ant-ilhas were inhabited by peoples whom he called "Indians".
Early explorations.
In 1500, Juan de la Cosa drew the first map of the New World, which depicted the two Leeward Antilles known at the time. This was followed by the more accurate Cantino map, created anonymously in 1502, which also showed an extensive landmass and mentioned the "Isla do gigante" (Island of Giants) and "Isla" "do brasil" (Island of Brasil)"." The location of the Isla do gigante southwest of the Isla do brasil suggest that it refers to Bonaire and Curaçao since Aruba is located more to the northwest.
In 1493, the year in which the West Indian islands became known in Europe, the division between the secular (civil) and religious authorities in the New World had to be arranged. The newly appointed Spanish Pope, Alexander VI, issued the "Inter Caetera" bull, granting the Spanish Crown sovereignty over the newly discovered territories and the responsibility of the holy task: "to send good, God-fearing men, who are earned and capable, to those islands and continents to teach the natives living there about the Catholic faith and instill in them good habits". In exchange for the papal approval of the treaty, Spain promised to vigorously carry out missionary work in the discovered territories. This gave Spain the right to evangelize the Americas and appoint and dismiss priests, blurring the separation between church and state in the region. In other words, the Spanish Crown was granted significant religious authority in the Americas, which was not strictly separate from the state and weakened the distinction between the religious and secular spheres—a key aspect of separation between church and state.
The conquistador.
"Conquistadors" were fascinated by legends of inexhaustible gold reserves of El Dorado. The conquest was characterized by bloodshed, destruction, and forced assimilation of the native peoples into European society, such as the initiation of Indian slavery by Columbus in 1492. Europeans had an advantage because they had superior weapons, such as firearms, steel swords, armor, ships, horses, and targeted military strategies. While expedition leaders mostly came from the higher echelons of late medieval society, their foot soldiers were usually from the lower middle class of southern Europe. These soldiers formed the basis of the future group of "encomenderos." The encomienda system granted Spanish colonizers right by the Spanish Crown to extract tribute and labor from indigenous peoples. For example, indigenous communities had to give up a portion of the yields from their agricultural or farm land, known as "conucos" in Taino, as a form of taxation and to provide for the food supply of the colonists. The defeated were often kidnapped and forced to participate in expeditions elsewhere in the New World as slave laborers.
The Caquetío population of the Leeward Antilles was incorporated into the Spanish colonial empire . On June 8 and 10, 1501, Alonso De Ojeda acquired the exclusive right to exploit the current Venezuelan coastal area, known as Coquibacoa, and the islands of the coast Curaçao, Aruba, and Bonaire, and probably also the Mongues and Aves Islands. De Ojeda had to form an administration as far west as possible on the "Tierra Firme" to secure the Spanish presence. It is believed that De Ojeda and Amerigo Vespucci visited Bonaire and Curaçao, but neither Vespucci nor De Ojeda are thought to have set foot on Aruban soil. He was explicitly forbidden to enslave natives. However, De Ojeda lacked administrative skills and intentions, and he became a controversial figure. De Ojeda established a base, named Santa Cruz, at the tip of Guajira, from where he conducted trade and, probably even more importantly, carried out his raids, including slave hunts. During his third voyage to the New World in 1502, De Ojeda visited Curaçao, but his attempt to exploit the region failed. Instead, Bartolomé de Las Casas documented De Ojeda's raids, slave hunts, and atrocities in the rural areas of present-day Cartagena in his book, . These raids were disastrous, even for the Spaniards, and marked the end of the first attempt to control the region. Upon returning to Spain in 1504, De Ojeda was brought to trial to account for his failed venture and the atrocities he had committed. De Ojeda lost his case, and the court stripped him of his earthly possessions and the rights to exploit the region. De Ojeda died in 1515 or 1516, impoverished, in a Franciscan monastery in Hispaniola. As a sign of his forced simplicity, he was buried in the entrance of the monastery.
Between 1513 and 1515, the Leeward Antilles, including Curaçao, Aruba, and Bonaire, were depopulated. Captain Diego Salazar led this effort, which affected an estimated 2,000 indigenous inhabitants from these islands, and likely more from Tierra Firme. Most of the Caquetío were taken to Hispaniola as forced laborers. Many of them likely died on the way or later in the gold mines by the Spanish colonizers or during the devastating smallpox epidemic of 1518. Later on, new Indians migrated from the mainland to Aruba, while Indians were brought to Curaçao by Juan de Ampiés. The Indigenous population was under encomienda, which ended the autonomy of Caquetío community in the islands. Their relatives on the mainland did not fare any better. After an unsuccessful attempt by Bartolomé de Las Casas to convert the local population to Christianity, the coastal region of the mainland was leased to the banking firm of the Welsers in 1528. This led to the violent conquest of the Caquetío kingdom. Before 1634, Curaçao, along with its neighboring islands Bonaire and Aruba, were considered part of the province of Venezuela. They had been separated from Venezuela only during the period of the Welser grant.
The appropriation of the Caribbean region turned out to be a failure for the Spaniards. The exploitation of the West Indian islands proved unprofitable, and gold mining on Hispaniola and Puerto Rico declined. Spanish settlers moved elsewhere, and In 1569, the Spanish Crown banned settlement on the Caribbean islands by Royal decree. This measure that did not apply to the leased islands of Curaçao, Aruba, and Bonaire. The colonization of the large Caribbean islands, such as Cuba, was encouraged, while the small islands were abandoned. Most of the islands remained largely uncontrolled and undefended, making them a potential opportunity for northwestern European countries that wanted to break Spain's monopoly on colonizing the New World. England, France, the Netherlands, and Denmark explored the possibilities of piracy and trade on the Caribbean islands.
On the continent, the great empires declined, but indigenous societies continued to exist and were exposed to a long process of miscegenation. In the Falcón-Zulia province, among other places, Caquetío societies survived on Tierra Firme. Although their cultures and social structures were largely destroyed by the Spaniards. On the (former) Caquetío coastal islands of Aruba, Curaçao, and Trinidad, indigenous people lived well into the colonial period. The indigenous history of the Guajira peninsula extends to the present day. The Wayú are increasingly being recognized on the Leeward Antilles as possible contemporary ancestors or lost relatives from (pre)historic times.
Spanish ranch.
The conquistadors brought not only people but also European cattle to Aruba. Over time, they also introduced goats, sheep, dogs, donkeys, cows, pigs, and possibly even cats. It's believed that rabbits, brought by the Dutch, later became wild on the island. Aruba essentially became a Spanish ranch, with cattle roaming freely in search of food. Despite more trees in the past, the overall vegetation was similar to today. The horses introduced here were lighter than Dutch ones, and their hooves became so hard from roaming freely that they didn't need horseshoes. To avoid stallions injuring each other during mating battles, horseshoes were impractical. After three weeks of service, particularly in the dry season, the horses were released to recover. Occasionally, a small group of Spaniards would disembark on the island, but typically, Aruba was left to fend for itself. According to De Laet, in 1630, there were "few Indians and some Spaniards" on Aruba.
Early Dutch period.
Dutch conquest: salt.
The Dutch were compelled to venture into forbidden waters of the Caribbean, known as Spain's "mare clausum," due to their need for salt, in open defiance of Phillip II . Since the mid-15th century, the prosperous Dutch herring industry had been steadily expanding. The West Frisian towns of Hoorn, Enkhuizen, and Medemblik were particularly active in the salt trade, thanks to their thriving fishing industries. Herring was a crucial commodity for Dutch commerce, requiring salt for preservation. Salt also played a vital role in the butter and cheese industry, as well as in preserving food during long voyages. The curing or pickling process for herring was well-established during the Middle Ages. After catching the herring, the packers would remove the internal organs, mix them with salt to create a brine, and pack them in barrels along with additional salt. While Zeeland was not heavily involved in fishing, they were renowned for their salt whitening process, which was highly sought after throughout Europe.
During the 1400s, shipbuilders in the Low countries created a new type of fishing vessel called the herring buss, specifically designed for deep-sea fishing. These busses proved to be significantly more effective than the smaller flat-bottom commonly used for coastal fishing. As time went on and the early 16th century approached, the buss underwent modifications, evolving into three-masted ship with distinctively curved bows.
Salt importation began in the 15th century when the Dutch discovered high-quality salt in Setúbal the Iberian coast. This sea salt was perfect for preserving herring because of its magnesium sulfate and magnesium chloride content. It was also more affordable than domestic salt, which was produced by burning peat from coastal bogs infused with sea salt over the centuries. The Dutch obtained salt from Setúbal by trading goods from the Baltic Sea region. However, the Eighty Years' War, which started in 1585, prompted Phillip II to halt this trade. With the salt supply cut off, the Dutch were forced to seek new trade routes. Their quest for salt led them as far south as the "Isla de Mayo" and "Isla de Sal" (the Cape Verde Islands) until 1598, eventually expanding to the West Indies.
Shortages of salt in Cape Verde, combined with unsafe conditions under the Spanish rule, compelled the Dutch to venture across the Atlantic. Seeking alternative sources, they initially explored Brazil but abandoned it due to inadequate saltpans. They then turned their attention to Punta de Araya in Tierra Firme before the 17th century. The salt reserves in Punta de Araya had been largely untouched by the Spaniards, with numerous accessible pans spread along the coast and islands. While the Iberians were primarily attracted to the area for its pearls, the salt in Punta de Araya was abundant and of high quality, surpassing that of the Iberian peninsula. Rather than being a rock salt deposit, it was a gem salt derived from the clay of the surrounding hills. However, following the Truce of twelve years, the Dutch discovered that the Spaniards had fortified the saltpans, forcing them to give up their stake in Araya salt.
After hostilities resumed, the Dutch established the West India Company (WIC) with the main objective of engaging in strategic military actions and privateering organization against Spain. This was the or reason for the existence of the WIC. Their secondary objective was focused on commerce and colonization, a choice that ultimately led to the downfall of the WIC in 1674. The WIC also gathered information on Spanish treasure fleets. In 1623, the first official fleet of the new WIC, a small squadron of only three ships commanded by Pieter Schouten, set sail for the Caribbean to engage in looting and plundering in the Lesser Antilles and the Yucátan peninsula. It was during this voyage that the Dutch first encountered Aruba.
The Dutch herring production reached its peak in the seventeenth century, earning it the nickname "gold mine of the republic" by the Dutch government in 1624. Around 1628 or 1629, the Dutch started obtaining salt regularly on Tortuga. Governor Francisco Núñez Melián of Venezuela destroyed the saltpans and took some Dutch prisoners, forced them to cut Brazilwood in Curaçao. One of these Dutchmen, Jan Janszoon Otzen, carefully assessed the island's excellent harbor and profitable saltpans, which he later communicated to the WIC. Recognizing their struggle for salt, the Dutch realized the need to establish a base in these waters to secure Curaçao. Johannes van Walbeeck, a renowned Company agent with a commendable record in Brazil, was appointed as the expedition's commander and future Governor of Curaçao, Bonaire, and Aruba. Curaçao was captured and acquired by the WIC in June 1634, primarily by their desire to obtain salt. In Van Walbeeck's report of 1634, Aruba is mentioned only in relation to Curaçao, where he refers to Bonaire and Aruba collectively as the "islands of Curaçao". Spain had another reason to regret its past neglect of the "islas inútiles" (usless islands) as they came under control of foreign invaders disputing Spain's arrogant claims. However, the Dutch were dissatisfied with the available salt supply. The natural salt pans were inadequate, and their attempts to create an artificial one at the entrance of Sint Ann Bay were a complete failure. Although the salt pans in Bonaire showed more potential, they never developed into a major salt trading center. In 1816, Aruba possessed seven salt pans, all of which yielded salt of subpar quality. The salt production was just sufficient to meet the local demand. Aruban laborers, often assisted by donkeys, were tasked with gathering the salt, which was subsequently distributed among the island's inhabitants. Around 1924, salt extraction at Rancho had limited benefits, primarily being used in the preservation of fish during shipping. Paardenbaai (Horses' Bay) contained salt pans up until 1949 when it was dredged and disappeared beneath the sand.
1648–1687.
Between the Peace of Westphalia in 1648 and the Peace of Nijmegen in 1678, there were 30 years of crisis in the Dutch Antilles and the entire Caribbean region. By 1648, Curaçao had lost its importance as a military outpost. Governor Peter Stuyvesant had a plan to strengthen the connections between the islands and New Netherland. He believed that the two colonies could support each other: New Netherland would provide food in exchange for slaves from Curaçao, horses from Aruba, and salt from Bonaire.
But Stuyvesant didn't anticipate the rivalry between the two colonies, which prevented them from working together effectively. The Dutch in Curaçao preferred to sell their goods to other Caribbean islands where they could get a better price, rather than trading with their fellow countrymen in New Netherland. Additionally, the islands were involved in illegal trade with the Spanish mainland and didn't want to switch to legal trade with New Netherland. Governor Stuyvesant needed slaves to strengthen New Amsterdam's defenses, but he mostly received old or sick slaves, called "mancarrons", in response to his requests. The better slaves were sold elsewhere to the highest bidder. However, the people in New Netherland weren't motivated by unselfish reasons or a strong sense of patriotism. They continued to trade with their French, English, and Swedish neighbors across the border. Only in extreme situations did their shared heritage become more important than making money. For example, when the islands faced famine due to a series of dry seasons, Stuyvesant came to the rescue by sending a ship with food just in time.
The troubled relationship between the Curaçao islands and New Netherland came to a sudden end in 1664. At that time, even though a war between England and the United Provinces hadn't been officially declared yet, an English fleet led by Richard Nicolls demanded that New Amsterdam surrender. While the Dutch briefly regained control of the colony in 1673, it was once again used as leverage in 1674 to show the English the dangers of their alliance with France.
During the 17th century, the Dutch considered England their main adversary, as evidenced by the three wars they fought against the English. Unlike the first Anglo-Dutch War (1652–1654), the second Anglo-Dutch war (1665–1667) had a long-lasting impact in the Caribbean.
In the end, the Second Anglo-Dutch War and the subsequent peace treaty in 1667 marked a pivotal moment in Caribbean colonial possessions. Dutch supremacy waned, and the enforcement of English Navigation Acts left a lasting impact on regional trade. Nevertheless, the Caribbean islands eventually regained stability and prosperity, experiencing fewer changes in colonial holdings for centuries to come.
Slavery.
In the 16th century, Spaniards engaged in coercive labor practices, deporting Arawak Indians to Hispaniola in 1515. Colonists exerted control over Indians on the "useless islands", mirroring the hardships of these faced by subsequent African slaves, marked by a denial of freedom and forced labor.
After 1775 is when the names of African slaves began appearing in records, with examples such as "Cecilia" and "Apolinar" tied to families like Silvester and Alvarez from Alto Vista near the coast.
The Dutch colonizer recognized red slavery, particularly of Indians captured in wars. In the Guyanas, Indians taken as prisoners in conflicts were traded, even following peace treaties. Though Indians on Aruba weren't officially classified as slaves during the West India Company's rule, oral tradition in Aruba mentioned Amerindian slaves in the early 20th century. Father noted their presence in Curaçao as pseudo-slaves.
In 1827, Commander Simon Plats found 51 Amerindians treated as pseudo-slaves by Aruban families. Some were brought by shipowners involved in the slave trade. Plats had masters sign a declaration recognizing the freedom of the Amerindians, ensuring proper upbringing, education, and accommodation.
Contrary to common belief, Aruba had a history of slavery, challenging the notion that conditions were considerably better than in other Caribbean regions. Records are limited, with mentions primarily concerning Curaçao in 1750 and 1795. Aruba's circumstances surrounding slavery were comparatively less severe, leading to misconceptions that indigenous people were not enslaved. However, by 1862, 15 percent of Aruba's population were slaves, with 27 percent in Bonaire.
A "Population Report" from 1820 indicates 331 slaves in Aruba—157 indigenous people and 174 of African descent. In 1840, the number increased to 497 slaves, with 269 being indigenous people and 228 of African descent. Approximately, half of Aruba's slaves were of indigenous origin, and the other half were of African descent. Although Dutch law generally prohibited the enslavement of indigenous people, the actual practice varied.
English interregnum and economic development.
The British Empire took control of the island during the Napoleonic Wars holding it from 1806 to 1816, after which it was returned to Dutch authority in accordance with the Anglo-Dutch Treaty of 1814. Aruba was then integrated into the Colony of Curaçao and Dependencies, along with Bonaire. Throughout the 19th century, the island's economy evolved, centered around gold, phosphate (Aruba Phosphate Company), and the aloe vera industry (Royal Aruba Aloe), However, despite these economic activities, Aruba continued to be a relatively underdeveloped and economically disadvantaged region during this period.
20th and 21st centuries.
The first oil refinery, Lago Oil and Transport Company, in San Nicolas was built in 1924 and a subsidiary of Standard Oil (Esso). The refinery on Aruba grew to become one of the largest in the world. In 1927, the Arend Petroleum Company was established, to the west of the capital city of Oranjestad, and was commonly called the Eagle. The refineries processed crude oil from the vast Venezuelan oil fields, bringing greater prosperity to the island.
During World War II, the Netherlands was occupied by Nazi Germany. In 1940, the oil facilities in Aruba came under the administration of the Dutch government-in-exile in London, causing them to be attacked by the German navy in 1942.
In August 1947, Aruba formulated its first "Staatsreglement" (constitution) for Aruba's "status aparte" as an autonomous state within the Kingdom of the Netherlands, prompted by the efforts of Henny Eman, a noted Aruban politician. By 1954, the Charter of the Kingdom of the Netherlands was established, providing a framework for relations between Aruba and the rest of the Kingdom. That created the Netherlands Antilles, which united all of the Dutch colonies in the Caribbean into one administrative structure. Many Arubans were unhappy with the arrangement, however, as the new policy was perceived as being dominated by Curaçao.
In 1972, at a conference in Suriname, Betico Croes, a politician from Aruba, proposed the creation of a Dutch Commonwealth of four states: Aruba, the Netherlands, Suriname, and the Netherlands Antilles, each to have its own nationality. Backed by his newly created party, the Movimiento Electoral di Pueblo, Croes sought greater autonomy for Aruba, with the long-term goal of independence, adopting the trappings of an independent state in 1976 with the creation of a flag and national anthem. In March 1977, a referendum was held with the support of the United Nations. 82% of the participants voted for complete independence from the Netherlands. Tensions mounted as Croes stepped up the pressure on the Dutch government by organising a general strike in 1977. Croes later met with Dutch Prime Minister Joop den Uyl, with the two sides agreeing to assign the Institute of Social Studies in The Hague to prepare a study for independence, entitled "Aruba en Onafhankelijkheid, achtergronden, modaliteiten, en mogelijkheden; een rapport in eerste aanleg" (Aruba and independence, backgrounds, modalities, and opportunities; a preliminary report) (1978).
Autonomy.
In March 1983, Aruba reached an official agreement within the Kingdom for its independence, to be developed in a series of steps as the Crown granted increasing autonomy. In August 1985, Aruba drafted a constitution that was unanimously approved. On 1 January 1986, after elections were held for its first parliament, Aruba seceded from the Netherlands Antilles, officially becoming a country of the Kingdom of the Netherlands, with full independence planned for 1996. However, Croes was seriously injured in a traffic accident in 1985, slipping into a coma. He died in 1986, never seeing the enacting of "status aparte" for Aruba for which he had worked over many years.
After his death, Croes was proclaimed "Libertador di Aruba". Croes' successor, Henny Eman, of the Aruban People's Party (AVP), became the first Prime Minister of Aruba. In 1985, Aruba's oil refinery had closed. It had provided Aruba with 30 percent of its real income and 50 percent of government revenue. The significant blow to the economy led to a push for a dramatic increase in tourism, and that sector has expanded to become the island's largest industry. At a convention in The Hague in 1990, at the request of Aruba's Prime Minister Nelson Oduber, the governments of Aruba, the Netherlands, and the Netherlands Antilles postponed indefinitely Aruba's transition to full independence. The article scheduling Aruba's complete independence was rescinded in 1995, although it was decided that the process could be revived after another referendum.
Geography.
Aruba is located 77 km (48 mi) west of Curaçao and 29 km (18 mi) north of Paraguaná Peninsula of Venezuela.
Aruba showcases three distinct landscapes. The northwestern region is primarily characterized by flat batholith landscapes. Notable landmarks here include the conical Hooiberg hill, and rock formations like Ayo and Casibari. Moreover, the northeastern part of the island features the oldest formations known as the Aruba Lava Formation (ALF). This region is marked by rolling hills, including Jamanota, and is home to Arikok National Park, and lastly the limestone terraces surround these two landscapes. The low-lying limestone terrace regions are defined by their white sandy beaches and the high plateaus on the north side of the island, in contrast, are constantly battered by the rough waters of the ocean, featuring caves and small natural bridge formations.
The arid landscape in Aruba is not solely a product of its climate but is also a consequence of extensive deforestation and exploitation during the Spanish colonization of the island. Consequently, certain crops, such as aloe vera, thrive in this environment, due to the high calcium-rich soil known as liming. As of 2022, Aruba only has 2.3% of forest-covered land area and only 0.5% of protected natural area.
Additionally, Aruba's geography includes naturally formed "rooi" or gullies that channel rainwater towards dams and ultimately the ocean. Other than Arikok National Park, "Bubaliplas", also known as the Bubali Bird Sanctuary, is the only significant body of water on the island, that holds the status of protected nature reserve and serves as a brackish water lagoon located in the Bubali district of Noord.
Regions.
Aruba is divided into eight regions for census purposes, with no administrative function. Some correspond to parishes and include several community facilities.
Flora and fauna.
Aruba's flora distinguishes itself from the typical tropical island vegetation. The landscape is characterized by common Xeric scrublands featuring various cacti, thorny shrubs, and evergreen plants. Notably, Aloe vera is also found on the island, and its economic significance has led to its inclusion on the coat of arms of Aruba.
Cacti such as "Melocactus" and "Opuntia" are part of Aruba's landscape, with species like "Opuntia stricta" being prominent. Additionally, drought-tolerant trees like "Caesalpinia coriaria" and "Vachellia tortuosa" are present.
The isolation of Aruba from the South America mainland contributed to the evolution of multiple endemic species. The island provides a habitat for unique wildlife, including the endemic Aruban Whiptail, Aruba Rattlesnake, as well as subspecies of Aruban Burrowing Owl and Brown-throated Parakeet.
Climate and natural hazards.
According to the Köppen climate classification, Aruba is characterized by a hot semi-arid climate (Köppen "BSh"), characterized by limited rainfall, totaling just annually. Notably, Aruba remains drier even during its supposed rainy season.
The arid landscape is a direct result of the limited rainfall, shaping the island's predominantly arid environment. In Oranjestad, mean monthly temperatures remain consistently moderate, with little variation (low diurnal temperature variation) ranging from to . This temperature stability is moderated by the constant trade winds originating from the northeast, sweeping in from the Atlantic Ocean.
Oranjestad experiences an annual rainfall barely exceeding . It's important to note the high variability in Aruba's rainfall, ranging from as little as during strong El Niño years (e.g. 1911/1912, 1930/1931, 1982/1983, 1997/1998) to over in La Niña years, such as 1933/1934, 1970/1971 or 1988/1989.
An exception to the general aridity is observed during the short rainy season from September to January. During this period, the southward retreat of the Intertropical Convergence Zone leads to more frequent moist northeasterly winds. Aruba is positioned south of the Main Development Region for tropical cyclones, and generally avoids the direct impact of these storms. However, in late 2020, the island was affected by two hurricanes in their early stages (2020 Atlantic hurricane season).
Demographics.
In terms of country of birth, the population is estimated to be 66% Aruban, 9.1% Colombian, 4.3% Dutch, 5.1% Dominican, 3.2% Venezuelan, 2.2% Curaçaoan, 1.5% Haitian, 1.2% Surinamese, 1.1% Peruvian, 1.1% Chinese, 6.2% from other backgrounds. In 2019, recently arrived Venezuelan refugees were estimated to number around 17,000 on Aruba, accounting for some 15% of the island's population.
In terms of nationality, the population is estimated to be 78.7% Dutch, 6.6% Colombian, 5.5% Venezuelan, 2.8% Dominican; 1.3% Haitian, and 5.1% from other backgrounds (). Aruba has a strong Arawak heritage compared to most Caribbean islands, although there are no full-blooded Aboriginals remaining. The islanders' features clearly reflect their genetic Arawak heritage. The majority of the population is descended from Caquetío, African slaves, and Dutch settlers, and to a lesser extent, various other groups that have settled on the island over time, including the Spanish, Venezuelans, Portuguese, English, French, West Indians, Asians, and Sephardic Jews. 
The population of Aruba has experienced fluctuations between 1972 and 2022, primarily influenced by net migration. While there have been periods of growth, there have also been declines, especially during economic challenges. Notably, between 1988 and 2016, the population nearly doubled. However, in 2017, a decline occurred, breaking almost three decades of continuous growth. The pandemic years (2020–2022) also contributed to a population decrease, mainly due to reduced births and immigration.
As of the first quarter in 2023, Aruba's population stood at 107,354 people, marking a modest 0.2% growth compared to the previous year. This increase was driven by a significant rise in immigration, which saw a 20.4 percent uptick.
Language.
Aruba's official languages are Dutch and Papiamento. While Dutch is the sole language for all administration and legal matters, Papiamento is the predominant language used on Aruba. Papiamento is a Spanish/Portuguese-based creole language, spoken on Aruba, Bonaire, and Curaçao, that also incorporates words from Dutch and various West African languages. English and Spanish are also spoken, their usage having grown due to tourism. Other common languages spoken, based on the size of their community, are Portuguese, Cantonese, French and German.
In recent years, the government of Aruba has shown an increased interest in acknowledging the cultural and historical importance of Papiamento. Although spoken Papiamento is fairly similar among the several Papiamento-speaking islands, there is a big difference in written Papiamento: The orthography differs per island, with Aruba using etymological spelling, and Curaçao and Bonaire a phonetic spelling.
The book "Buccaneers of America", first published in 1678, states through eyewitness account that the natives on Aruba spoke Spanish already. Spanish became an important language in the 18th century due to the close economic ties with Spanish colonies in what are now Venezuela and Colombia. Venezuelan TV networks are received on the island, and Aruba also has significant Venezuelan and Colombian communities. Around 13% of the population today speaks Spanish natively. Use of English dates to the early 19th century, when the British took Curaçao, Aruba, and Bonaire. When Dutch rule resumed in 1815, officials already noted wide use of the language. There is also a little studied native variety of English Creole spoken in San Nicolaas.
Aruba has newspapers published in Papiamento: "Diario", "Bon Dia", "Solo di Pueblo", and "Awe Mainta"; English: "Aruba Daily", "Aruba Today", and "The News"; and Dutch: "Amigoe". Aruba has 18 radio stations (two AM and sixteen FM) and two local television stations (Telearuba and Channel 22).
Religion.
Roman Catholicism is the dominant religion, followed by approximately 75% of the population. In addition to Catholicism, there is a diverse range of religions practiced in Aruba, including Protestantism, Islam, Hinduism, Judaism, and spiritual beliefs of the indigenous people (e.g. African diaspora reigions, Santería, or brua).
Shrine.
The Lourdes Grotto, named after the famous French religious pilgrimage site, was constructed in 1958 by a priest named Erkamp and his parishioners. This shrine is nestled into the rocks of Seroe Preto, just off the main road to San Nicolas. Inside the cave, there is a statue of the Virgin Mary, easily visible from the main road. Each year, on February 11 (the feast of Lady Lourdes), a procession departs from St. Theresita Church in San Nicolas and heads to the grotto, where a Mass is held.
Government.
Along with the Netherlands, Curaçao, and Sint Maarten, Aruba is a constituent country of the Kingdom of the Netherlands, with internal autonomy. Matters such as foreign affairs and defense are handled by the Netherlands. Aruba's politics take place within a framework of a 21-member Staten (Parliament) and an eight-member Cabinet; the Staten's 21 members are elected by direct, popular vote to serve a four-year term. The governor of Aruba is appointed for a six-year term by the monarch, and the prime minister and deputy prime minister are indirectly elected by the Staten for four-year terms.
Aruba was formerly a part of the (now-dissolved) Netherlands Antilles; however, it separated from that entity in 1986, gaining its own constitution.Aruba is designated as a member of the Overseas Countries and Territories (OCT) and is thus officially not a part of the European Union, though Aruba can and does receive support from the European Development Fund.
Politics.
The Aruban legal system is based on the Dutch model. In Aruba, legal jurisdiction lies with the "Gerecht in Eerste Aanleg" (Court of First Instance) on Aruba, the "Gemeenschappelijk Hof van Justitie van Aruba, Curaçao, Sint Maarten, en van Bonaire, Sint Eustatius en Saba" (Joint Court of Justice of Aruba, Curaçao, Sint Maarten, and of Bonaire, Sint Eustatius and Saba) and the "Hoge Raad der Nederlanden" (Supreme Court of Justice of the Netherlands). The "Korps Politie Aruba" (Aruba Police Force) is the island's law enforcement agency and operates district precincts in Oranjestad, Noord, San Nicolaas, and Santa Cruz, where it is headquartered.
Divergent Legal Protections from the rest of the Kingdom of the Netherlands include:
Not being part of the Law Enforcement Council (Raad van de Rechtshandhaving), which is a legal entity based on the Kingdom Act of the July 7, 2010 the Kingdom Act on the Law Enforcement Council.
Being the only country in the Kingdom of the Netherlands that does not have an ombudsman.
Deficit spending has been a staple in Aruba's history, and modestly high inflation has been present as well. By 2006, the government's debt had grown to 1.883 billion Aruban florins. In 2006, the Aruban government changed several tax laws to reduce the deficit. Direct taxes have been converted to indirect taxes as proposed by the IMF.
Foreign relations.
Aruba is one of the overseas countries and territories (OCT) of the European Union and maintains economic and cultural relations with the European Union and the United States. Aruba is also a member of several international organizations such as the International Monetary Fund and Interpol.
Military.
Defence on Aruba is the responsibility of the Kingdom of the Netherlands. The Dutch Armed Forces that protect the island include the Navy, Marine Corps, and the Coastguard including a platoon sized national guard.
All forces are stationed at Marines base in Savaneta. Furthermore, in 1999, the U.S. Department of Defense established a Forward Operating Location (FOL) at the airport.
Education.
Historically, Dutch was not widely spoken on the island, except within colonial administration, and its usage increased in the late 19th and early 20th centuries. Students in Curaçao, Aruba, and Bonaire were predominantly taught in Spanish until the late 18th century. In Aruba, Dutch serves as the primary language of instruction, with Papiamento taught as a subject in the lower grades of secondary education.
Aruba's educational system mirrors the Dutch education structure, with public national education financed by the government of Aruba. The education landscape includes a mix of public and private institutions, such as the International School of Aruba, the Schakel College, and Colegio Arubano.
In addition to the national education system, Aruba hosts three medical schools, American University School of Medicine Aruba (AUSOMA), Aureus University School of Medicine and Xavier University School of Medicine. The island also has its own national university, the University of Aruba.
Economy.
The island's economy is dominated by four main industries: tourism, aloe export, petroleum refining, and offshore banking. Aruba has one of the highest standards of living in the Caribbean region. The GDP per capita (PPP) for Aruba was estimated to be $37,500 in 2017. Its main trading partners are Colombia, the United States, Venezuela, and the Netherlands.
The agriculture and manufacturing sectors are fairly minimal. Gold mining was important in the 19th century. Aloe was introduced to Aruba in 1840 but did not become a big export until 1890. Cornelius Eman founded Aruba Aloe Balm, and over time the industry became very important to the economy. At one point, two-thirds of the island was covered in aloe vera fields, and Aruba became the largest exporter of aloe in the world. The industry continues today, though on a smaller scale.
Access to biocapacity in Aruba is much lower than world average. In 2016, Aruba had 0.57 global hectares of biocapacity per person within its territory, much less than the world average of 1.6 global hectares per person. In 2016, Aruba used 6.5 global hectares of biocapacity per person - their ecological footprint of consumption. This means they use almost 12 times the biocapacity that Aruba contains. This is the extent of Aruba's biocapacity deficit.
The official exchange rate of the Aruban florin is pegged to the US dollar at Afl 1.80 to US$1.00. This fact, and the majority of tourists being US, means businesses of hotel and resort districts prefer to bank and trade with the consumer in US dollars.
Aruba is a prosperous country. Unemployment is low (although the government has not published statistics since 2013) and per capita income is one of the highest in the Caribbean (approximately $24,087). At the end of 2018, the labor force participation rate was 56.6% for women.
Until the mid-1980s, Aruba's main industry was oil refining. Then the refinery was shut down and the island's economy shifted towards tourism. Currently, Aruba receives about 1,235,673 (2007) guests per year, of which three-quarters are Americans. Tourism is mainly focused on the beaches and the sea. The refinery has been closed and restarted repeatedly during the last decades. In recent years a letter of intent was signed with CITGO (the US subsidiary of the Venezuelan state oil company PDVSA) to explore the possibility of reopening the refinery again.
Until 2009, the Netherlands granted development aid to Aruba. This aid was mainly for law enforcement, education, administrative development, health care and sustainable economic development. This aid was discontinued at Aruba's request in 2009. Since 2015, however, a form of financial supervision has been reintroduced because Aruba's debt has risen sharply to over 80% of GDP.
Aruba also has two free trade zones (Barcadera and Bushiri), where import and export and the movement of services are tax-free.
Tourism.
Aruba has a large and well-developed tourism industry, receiving 1,082,000 tourists who stayed overnight in its territory in 2018. About of the Aruban gross national product is earned through tourism and related activities. Most tourists are from North America, with a market-share of 73.3%, followed by Latin America with 15.2% and Europe with 8.3%. In 2018, there were 40,231 visitors from the Netherlands.
For private aircraft passengers bound for the United States, the United States Department of Homeland Security (DHS), U.S. Customs and Border Protection (CBP) has a full pre-clearance facility since 1 February 2001 when Queen Beatrix Airport expanded. Since 2008, Aruba has been the only island to have this service for private flights.
Culture.
Aruba boasts a diverse culture. According to the "Bureau Burgelijke Stand en Bevolkingsregister" (BBSB, Civil Registry and Population Register), in 2005, the island was home to people from ninety-two different nationalities. Dutch influence is still evident in traditions like the celebration of "Sinterklaas" (Saint Nicholas) on December 5 and 6, as well as national holidays like April 27 when Aruba, along with the rest of the Kingdom of the Netherlands, celebrates "Koningsdag" (King's day) or "Dia di Rey" (in Papiamento) is celebrated.
On 18 March, Aruba celebrates its National Anthem and Flag Day. Christmas and New Year's Eve are celebrated with the typical music and songs of gaitas for Christmas and the for New Year. Traditional food and drinks like "ayaca", "ponche crema", ham, and more are also parts of the festive season. January 25 is dedicated to celebrating Betico Croes day, while June 24 is the day for "Dia di San Juan". In addition to Christmas, religious holidays such as the Feast of the Ascension and Good Friday are also observed on the island.
Aruba's Carnaval is a significant cultural event, akin to celebrations in other Caribbean and Latin American countries. It began in the 1950s, influenced by residents from Venezuela and nearby islands (Curaçao, St. Vincent, Trinidad, Barbados, St. Maarten, and Anguilla) who worked at the oil refinery. The Carnaval Celebrations now spans from early January until the Tuesday before Ash Wednesday, featuring a grand parade on the final Sunday of the festivities.
Aruba has seen an increased influence of American culture due to rising tourism from the United States. This is evident in the adoption of American celebrations like Halloween in October and Thanksgiving Day in November.
Architecture.
From the beginning of the colonization of the Netherlands until the beginning of the 20th century, the architecture in the most inhabited areas of Aruba was influenced by the Dutch colonial style and also some Spanish elements from the Catholic missionaries present in Aruba who later settled in Venezuela as well. After the boom of the oil industry and the tourist sector in the 20th century the architectural style of the island incorporated a more American and international influence. In addition, elements of the Art Deco style can still be seen in several buildings in San Nicolas. Therefore, it can be said that the island's architecture is a mixture of Spanish, Dutch, American and Caribbean influences.
Sport.
The most popular sports in Aruba are football, basketball, baseball, and volleyball, as well as beach sports. Aruba has competed at the Olympic Games since 1988.
Infrastructure.
Aruba's Queen Beatrix International Airport is near Oranjestad.
Aruba has four ports: Barcadera, the main cargo port, Paardenbaai, the cruise ship terminal in Oranjestad/Taratata, Commandeurs Baai (Commander's Bay) in Savaneta, and Sint Nicolaas Baai in San Nicolaas. Paardenbaai services all the cruise-ship lines such as Royal Caribbean, Carnival, NCL, Holland America, MSC Cruises, Costa Cruises, P&amp;O Cruises and Disney. Nearly one million tourists enter this port per year. Aruba Ports Authority, owned and operated by the Aruban government, runs these seaports.
"Arubus" is a government-owned bus company. Its buses operate from 3:30 a.m. until 12:30 a.m., 365 days a year. Private minibuses/people movers service zones such as the Hotel Area, San Nicolaas, Santa Cruz and Noord.
A streetcar service runs on rails on the mainstreet of Oranjestad.
Utilities.
Water- en Energiebedrijf Aruba, N.V. (W.E.B.) produces potable water and power. Average daily consumption in Aruba is about 35600 m3 (46,500 cu. yd.) per day., and average power generation is 104 MW. Besides production, WEB also takes care of the water distribution on the island. Elektriciteits Maatschappij Aruba, N.V. (N.V. Elmar) is the sole distributor of electricity on the island of Aruba. N.V. Elmar also offers its customers the opportunity to add solar panel or wind turbines. Together with W.E.B. Aruba N.V., both companies share the same parent holding which is Utilities Aruba N.V. The Sunrise Solar Park was installed and opened in 2018.
Waste management.
Sewage plant: there are three around the island; Zeewijk, Parkietenbos and Bubali. The one in Bubali (near the bird sanctuary) is 4 decades old and is processing over 8000 m3 (10,000 cu. yd.) per day. Around double its original capacity of 4500 m3 (5900 cu. yd.) per day (due to Aruba's growth).
Solid waste landfill: the major one (16 hectares; 40 acres) is at Parkietenbos since the 1950. The capacity is between 130 and 150 kilotons per year. Sometimes there are huge spontaneous fires creating pollution.
Communications.
There are two telecommunications providers: government-based Setar, and privately owned Digicel. Digicel is Setar's competitor in wireless technology using the GSM platform.

</doc>
<doc id="691" url="?curid=691" title="Articles of Confederation">
Articles of Confederation

The Articles of Confederation and Perpetual Union was an agreement among the 13 states of the United States, formerly the Thirteen Colonies, that served as the nation's first frame of government. It was debated by the Second Continental Congress at Independence Hall in Philadelphia between July 1776 and November 1777, and finalized by the Congress on November 15, 1777. It came into force on March 1, 1781, after being ratified by all 13 colonial states. A guiding principle of the Articles was the establishment and preservation of the independence and sovereignty of the states. The Articles consciously established a weak federal government, affording it only those powers the former colonies had recognized as belonging to king and parliament. The document provided clearly written rules for how the states' league of friendship, known as the Perpetual Union, would be organized.
While waiting for all states to ratify, the Congress observed the Articles as it conducted business, directing the war effort, conducting diplomacy with foreign states, addressing territorial issues and dealing with Native American relations. Little changed procedurally once the Articles of Confederation went into effect, as ratification did little more than constitutionalize what the Continental Congress had been doing. That body was renamed the Congress of the Confederation; but most Americans continued to call it the Continental Congress, since its organization remained the same.
As the Confederation Congress attempted to govern the continually growing U.S. states, its delegates discovered that the limitations placed upon the central government (such as in assembling delegates, raising funds, and regulating commerce) rendered it ineffective at doing so. As the government's weaknesses became apparent, especially after Shays' Rebellion, some prominent political thinkers in the fledgling union began asking for changes to the Articles. Their hope was to create a stronger government. Initially, in September 1786, some states met to address interstate protectionist trade barriers between them. Shortly thereafter, as more states became interested in meeting to revise the Articles, a meeting was set in Philadelphia on May 25, 1787. This became the Constitutional Convention. Delegates quickly agreed that the defects of the frame of government could not be remedied by altering the Articles, and so went beyond their mandate by replacing it with a new constitution. On March 4, 1789, the government under the Articles was replaced with the federal government under the Constitution. The new Constitution provided for a much stronger federal government by establishing a chief executive (the president), courts, and taxing powers.
Background and context.
The political push to increase cooperation among the then-loyal colonies began with the Albany Congress in 1754 and Benjamin Franklin's proposed Albany Plan, an inter-colonial collaboration to help solve mutual local problems. Over the next two decades, some of the basic concepts it addressed would strengthen; others would weaken, especially in the degree of loyalty (or lack thereof) owed the Crown. Colonists' civil disobedience resulted in the British enacting coercive and quelling measures, such as the passage of what colonists called the Intolerable Acts in the British Parliament, and armed skirmishes which resulted in dissidents being proclaimed rebels. These actions eroded the number of colonists continuing to be Loyalists to the Crown. Together with the highly effective propaganda campaign of the Patriot leaders, caused an increasing number of colonists to begin agitating for independence from the mother country. In 1775, with events outpacing communications, the Second Continental Congress began acting as the provisional government for the United Colonies.
It was an era of constitution writing—most states were busy at the task—and leaders felt the new nation must have a written constitution; a "rulebook" for how the new nation should function. During the war, Congress exercised an unprecedented level of political, diplomatic, military and economic authority. It adopted trade restrictions, established and maintained an army, issued fiat money, created a military code and negotiated with foreign governments.
To transform themselves from outlaws into a legitimate nation, the colonists needed international recognition for their cause and foreign allies to support it. In early 1776, Thomas Paine argued in the closing pages of the first edition of "Common Sense" that the "custom of nations" demanded a formal declaration of American independence if any European power were to mediate a peace between the Americans and Great Britain. The monarchies of France and Spain, in particular, could not be expected to aid those they considered rebels against another legitimate monarch. Foreign courts needed to have American grievances laid before them persuasively in a "manifesto" which could also reassure them that the Americans would be reliable trading partners. Without such a declaration, Paine concluded, "[t]he custom of all courts is against us, and will be so, until, by an independence, we take rank with other nations."
Beyond improving their existing association, the records of the Second Continental Congress show that the need for a declaration of independence was intimately linked with the demands of international relations. On June 7, 1776, Richard Henry Lee introduced a resolution before the Continental Congress declaring the colonies independent; at the same time, he also urged Congress to resolve "to take the most effectual measures for forming foreign Alliances" and to prepare a plan of confederation for the newly independent states. Congress then created three overlapping committees to draft the Declaration, a model treaty, and the Articles of Confederation. The Declaration announced the states' entry into the international system; the model treaty was designed to establish amity and commerce with other states; and the Articles of Confederation, which established "a firm league" among the thirteen free and independent states, constituted an international agreement to set up central institutions for the conduct of vital domestic and foreign affairs.
Drafting.
On June 12, 1776, a day after appointing the Committee of Five to prepare a draft of the Declaration of Independence, the Second Continental Congress resolved to appoint a committee of 13 with one representative from each colony to prepare a draft of a constitution for a union of the states. The committee was made up of the following individuals:
The committee met frequently, and chairman John Dickinson presented their results to the Congress on July 12, 1776. Afterward, there were long debates on such issues as state sovereignty, the exact powers to be given to Congress, whether to have a judiciary, western land claims, and voting procedures. To further complicate work on the constitution, Congress was forced to leave Philadelphia twice, for Baltimore, Maryland, in the winter of 1776, and later for Lancaster then York, Pennsylvania, in the fall of 1777, to evade advancing British troops. Even so, the committee continued with its work.
The final draft of the "Articles of Confederation and Perpetual Union" was completed on November 15, 1777. Consensus was achieved by including language guaranteeing that each state retained its sovereignty, leaving the matter of western land claims in the hands of the individual states, including language stating that votes in Congress would be "en bloc" by state, and establishing a unicameral legislature with limited and clearly delineated powers.
Ratification.
The Articles of Confederation was submitted to the states for ratification in late November 1777. The first state to ratify was Virginia on December 16, 1777; 12 states had ratified the Articles by February 1779, 14 months into the process. The lone holdout, Maryland, refused to go along until the landed states, especially Virginia, had indicated they were prepared to cede their claims west of the Ohio River to the Union. It would be two years before the Maryland General Assembly became satisfied that the various states would follow through, and voted to ratify. During this time, Congress observed the Articles as its de facto frame of government. Maryland finally ratified the Articles on February 2, 1781. Congress was informed of Maryland's assent on March 1, and officially proclaimed the Articles of Confederation to be the law of the land.
The several states ratified the Articles of Confederation on the following dates:
Article summaries.
The Articles of Confederation contain a preamble, thirteen articles, a conclusion, and a signatory section. The individual articles set the rules for current and future operations of the confederation's central government. Under the Articles, the states retained sovereignty over all governmental functions not specifically relinquished to the national Congress, which was empowered to make war and peace, negotiate diplomatic and commercial agreements with foreign countries, and to resolve disputes between the states. The document also stipulates that its provisions "shall be inviolably observed by every state" and that "the Union shall be perpetual".
Summary of the purpose and content of each of the 13 articles:
Congress under the Articles.
Army.
Under the Articles, Congress had the authority to regulate and fund the Continental Army, but it lacked the power to compel the States to comply with requests for either troops or funding. This left the military vulnerable to inadequate funding, supplies, and even food. Further, although the Articles enabled the states to present a unified front when dealing with the European powers, as a tool to build a centralized war-making government, they were largely a failure; Historian Bruce Chadwick wrote:
Phelps wrote:
The Continental Congress, before the Articles were approved, had promised soldiers a pension of half pay for life. However Congress had no power to compel the states to fund this obligation, and as the war wound down after the victory at Yorktown the sense of urgency to support the military was no longer a factor. No progress was made in Congress during the winter of 1783–84. General Henry Knox, who would later become the first Secretary of War under the Constitution, blamed the weaknesses of the Articles for the inability of the government to fund the army. The army had long been supportive of a strong union.
Knox wrote:
As Congress failed to act on the petitions, Knox wrote to Gouverneur Morris, four years before the Philadelphia Convention was convened, "As the present Constitution is so defective, why do not you great men call the people together and tell them so; that is, to have a convention of the States to form a better Constitution."
Once the war had been won, the Continental Army was largely disbanded. A very small national force was maintained to man the frontier forts and to protect against Native American attacks. Meanwhile, each of the states had an army (or militia), and 11 of them had navies. The wartime promises of bounties and land grants to be paid for service were not being met. In 1783, George Washington defused the Newburgh conspiracy, but riots by unpaid Pennsylvania veterans forced Congress to leave Philadelphia temporarily.
The Congress from time to time during the Revolutionary War requisitioned troops from the states. Any contributions were voluntary, and in the debates of 1788, the Federalists (who supported the proposed new Constitution) claimed that state politicians acted unilaterally, and contributed when the Continental army protected their state's interests. The Anti-Federalists claimed that state politicians understood their duty to the Union and contributed to advance its needs. Dougherty (2009) concludes that generally the States' behavior validated the Federalist analysis. This helps explain why the Articles of Confederation needed reforms.
Foreign policy.
The 1783 Treaty of Paris, which ended hostilities with Great Britain, languished in Congress for several months because too few delegates were present at any one time to constitute a quorum so that it could be ratified. Afterward, the problem only got worse as Congress had no power to enforce attendance. Rarely did more than half of the roughly sixty delegates attend a session of Congress at the time, causing difficulties in raising a quorum. The resulting paralysis embarrassed and frustrated many American nationalists, including George Washington. Many of the most prominent national leaders, such as Washington, John Adams, John Hancock, and Benjamin Franklin, retired from public life, served as foreign delegates, or held office in state governments; and for the general public, local government and self-rule seemed quite satisfactory. This served to exacerbate Congress's impotence.
Inherent weaknesses in the confederation's frame of government also frustrated the ability of the government to conduct foreign policy. In 1786, Thomas Jefferson, concerned over the failure of Congress to fund an American naval force to confront the Barbary pirates, wrote in a diplomatic correspondence to James Monroe that, "It will be said there is no money in the treasury. There never will be money in the treasury till the Confederacy shows its teeth."
Furthermore, the 1786 Jay–Gardoqui Treaty with Spain also showed weakness in foreign policy. In this treaty, which was never ratified, the United States was to give up rights to use the Mississippi River for 25 years, which would have economically strangled the settlers west of the Appalachian Mountains. Finally, due to the Confederation's military weakness, it could not compel the British army to leave frontier forts which were on American soil — forts which, in 1783, the British promised to leave, but which they delayed leaving pending U.S. implementation of other provisions such as ending action against Loyalists and allowing them to seek compensation. This incomplete British implementation of the Treaty of Paris would later be resolved by the implementation of Jay's Treaty in 1795 after the federal Constitution came into force.
Taxation and commerce.
Under the Articles of Confederation, the central government's power was kept quite limited. The Confederation Congress could make decisions but lacked enforcement powers. Implementation of most decisions, including modifications to the Articles, required unanimous approval of all thirteen state legislatures.
Congress was denied any powers of taxation: it could only request money from the states. The states often failed to meet these requests in full, leaving both Congress and the Continental Army chronically short of money. As more money was printed by Congress, the continental dollars depreciated. In 1779, George Washington wrote to John Jay, who was serving as the president of the Continental Congress, "that a wagon load of money will scarcely purchase a wagon load of provisions." Mr. Jay and the Congress responded in May by requesting $45 million from the States. In an appeal to the States to comply, Jay wrote that the taxes were "the price of liberty, the peace, and the safety of yourselves and posterity." He argued that Americans should avoid having it said "that America had no sooner become independent than she became insolvent" or that "her infant glories and growing fame were obscured and tarnished by broken contracts and violated faith." The States did not respond with any of the money requested from them.
Congress had also been denied the power to regulate either foreign trade or interstate commerce and, as a result, all of the States maintained control over their own trade policies. The states and the Confederation Congress both incurred large debts during the Revolutionary War, and how to repay those debts became a major issue of debate following the War. Some States paid off their war debts and others did not. Federal assumption of the states' war debts became a major issue in the deliberations of the Constitutional Convention.
Accomplishments.
Nevertheless, the Confederation Congress did take two actions with long-lasting impact. The Land Ordinance of 1785 and Northwest Ordinance created territorial government, set up protocols for the admission of new states and the division of land into useful units, and set aside land in each township for public use. This system represented a sharp break from imperial colonization, as in Europe, and it established the precedent by which the national (later, federal) government would be sovereign and expand westward—as opposed to the existing states doing so under their sovereignty.
The Land Ordinance of 1785 established both the general practices of land surveying in the west and northwest and the land ownership provisions used throughout the later westward expansion beyond the Mississippi River. Frontier lands were surveyed into the now-familiar squares of land called the township (36 square miles), the section (one square mile), and the quarter section (160 acres). This system was carried forward to most of the States west of the Mississippi (excluding areas of Texas and California that had already been surveyed and divided up by the Spanish Empire). Then, when the Homestead Act was enacted in 1867, the quarter section became the basic unit of land that was granted to new settler-farmers.
The Northwest Ordinance of 1787 noted the agreement of the original states to give up northwestern land claims, organized the Northwest Territory and laid the groundwork for the eventual creation of new states. Although it did not happen under the articles, the land north of the Ohio River and west of the (present) western border of Pennsylvania ceded by Massachusetts, Connecticut, New York, Pennsylvania, and Virginia, eventually became the states of Ohio, Indiana, Illinois, Michigan, and Wisconsin, and the part of Minnesota that is east of the Mississippi River. The Northwest Ordinance of 1787 also made great advances in the abolition of slavery. New states admitted to the union in this territory would never be slave states.
No new states were admitted to the Union under the Articles of Confederation. The Articles provided for a blanket acceptance of the Province of Quebec (referred to as "Canada" in the Articles) into the United States if it chose to do so. It did not, and the subsequent Constitution carried no such special provision of admission. Additionally, ordinances to admit Frankland (later modified to Franklin), Kentucky, and Vermont to the Union were considered, but none were approved.
Presidents of Congress.
Under the Articles of Confederation, the presiding officer of Congress—referred to in many official records as "President of the United States in Congress Assembled"—chaired the Committee of the States when Congress was in recess, and performed other administrative functions. He was not, however, an executive in the way the later president of the United States is a chief executive, since all of the functions he executed were under the direct control of Congress.
There were 10 presidents of Congress under the Articles. The first, Samuel Huntington, had been serving as president of the Continental Congress since September 28, 1779.
U.S. under the Articles.
The peace treaty left the United States independent and at peace but with an unsettled governmental structure. The Articles envisioned a permanent confederation but granted to the Congress—the only federal institution—little power to finance itself or to ensure that its resolutions were enforced. There was no president, no executive agencies, no judiciary, and no tax base. The absence of a tax base meant that there was no way to pay off state and national debts from the war years except by requesting money from the states, which seldom arrived. Although historians generally agree that the Articles were too weak to hold the fast-growing nation together, they do give credit to the settlement of the western issue, as the states voluntarily turned over their lands to national control.
By 1783, with the end of the British blockade, the new nation was regaining its prosperity. However, trade opportunities were restricted by the mercantilism of the British and French empires. The ports of the British West Indies were closed to all staple products which were not carried in British ships. France and Spain established similar policies. Simultaneously, new manufacturers faced sharp competition from British products which were suddenly available again. Political unrest in several states and efforts by debtors to use popular government to erase their debts increased the anxiety of the political and economic elites which had led the Revolution. The apparent inability of the Congress to redeem the public obligations (debts) incurred during the war, or to become a forum for productive cooperation among the states to encourage commerce and economic development, only aggravated a gloomy situation. In 1786–87, Shays' Rebellion, an uprising of dissidents in western Massachusetts against the state court system, threatened the stability of state government.
The Continental Congress printed paper money which was so depreciated that it ceased to pass as currency, spawning the expression "not worth a continental". Congress could not levy taxes and could only make requisitions upon the States. Less than a million and a half dollars came into the treasury between 1781 and 1784, although the governors had been asked for two million in 1783 alone.
When John Adams went to London in 1785 as the first representative of the United States, he found it impossible to secure a treaty for unrestricted commerce. Demands were made for favors and there was no assurance that individual states would agree to a treaty. Adams stated it was necessary for the States to confer the power of passing navigation laws to Congress, or that the States themselves pass retaliatory acts against Great Britain. Congress had already requested and failed to get power over navigation laws. Meanwhile, each State acted individually against Great Britain to little effect. When other New England states closed their ports to British shipping, Connecticut hastened to profit by opening its ports.
By 1787, Congress was unable to protect manufacturing and shipping. State legislatures were unable or unwilling to resist attacks upon private contracts and public credit. Land speculators expected no rise in values when the government could not defend its borders nor protect its frontier population.
The idea of a convention to revise the Articles of Confederation grew in favor. Alexander Hamilton realized while serving as Washington's top aide that a strong central government was necessary to avoid foreign intervention and allay the frustrations due to an ineffectual Congress. Hamilton led a group of like-minded nationalists, won Washington's endorsement, and convened the Annapolis Convention in 1786 to petition Congress to call a constitutional convention to meet in Philadelphia to remedy the long-term crisis.
Signatures.
The Second Continental Congress approved the Articles for distribution to the states on November 15, 1777. A copy was made for each state and one was kept by the Congress. On November 28, the copies sent to the states for ratification were unsigned, and the cover letter, dated November 17, had only the signatures of Henry Laurens and Charles Thomson, who were the President and Secretary to the Congress.
The Articles, however, were unsigned, and the date was blank. Congress began the signing process by examining their copy of the Articles on June 27, 1778. They ordered a final copy prepared (the one in the National Archives), and that delegates should inform the secretary of their authority for ratification.
On July 9, 1778, the prepared copy was ready. They dated it and began to sign. They also requested each of the remaining states to notify its delegation when ratification was completed. On that date, delegates present from New Hampshire, Massachusetts, Rhode Island, Connecticut, New York, Pennsylvania, Virginia and South Carolina signed the Articles to indicate that their states had ratified. New Jersey, Delaware and Maryland could not, since their states had not ratified. North Carolina and Georgia also were unable to sign that day, since their delegations were absent.
After the first signing, some delegates signed at the next meeting they attended. For example, John Wentworth of New Hampshire added his name on August 8. John Penn was the first of North Carolina's delegates to arrive (on July 10), and the delegation signed the Articles on July 21, 1778.
The other states had to wait until they ratified the Articles and notified their Congressional delegation. Georgia signed on July 24, New Jersey on November 26, and Delaware on February 12, 1779. Maryland refused to ratify the Articles until every state had ceded its western land claims. Chevalier de La Luzerne, French Minister to the United States, felt that the Articles would help strengthen the American government. In 1780, when Maryland requested France provide naval forces in the Chesapeake Bay for protection from the British (who were conducting raids in the lower part of the bay), he indicated that French Admiral Destouches would do what he could but La Luzerne also "sharply pressed" Maryland to ratify the Articles, thus suggesting the two issues were related.
On February 2, 1781, the much-awaited decision was taken by the Maryland General Assembly in Annapolis. As the last piece of business during the afternoon Session, "among engrossed Bills" was "signed and sealed by Governor Thomas Sim Lee in the Senate Chamber, in the presence of the members of both Houses... an Act to empower the delegates of this state in Congress to subscribe and ratify the articles of confederation" and perpetual union among the states. The Senate then adjourned "to the first Monday in August next." The decision of Maryland to ratify the Articles was reported to the Continental Congress on February 12. The confirmation signing of the Articles by the two Maryland delegates took place in Philadelphia at noon time on March 1, 1781, and was celebrated in the afternoon. With these events, the Articles were entered into force and the United States of America came into being as a sovereign federal state.
Congress had debated the Articles for over a year and a half, and the ratification process had taken nearly three and a half years. Many participants in the original debates were no longer delegates, and some of the signers had only recently arrived. The Articles of Confederation and Perpetual Union were signed by a group of men who were never present in the Congress at the same time.
Signers.
The signers and the states they represented were:
Roger Sherman (Connecticut) was the only person to sign all four great state papers of the United States: the Continental Association, the United States Declaration of Independence, the Articles of Confederation and the United States Constitution.
Robert Morris (Pennsylvania) signed three of the great state papers of the United States: the United States Declaration of Independence, the Articles of Confederation and the United States Constitution.
John Dickinson (Delaware), Daniel Carroll (Maryland) and Gouverneur Morris (New York), along with Sherman and Robert Morris, were the only five people to sign both the Articles of Confederation and the United States Constitution (Gouverneur Morris represented Pennsylvania when signing the Constitution).
Parchment pages.
Original parchment pages of the Articles of Confederation, National Archives and Records Administration.
Revision and replacement.
In September 1786, delegates from five states met at what became known as the Annapolis Convention to discuss the need for reversing the protectionist interstate trade barriers that each state had erected. At its conclusion, delegates voted to invite all states to a larger convention to be held in Philadelphia in 1787. The Confederation Congress later endorsed this convention "for the sole and express purpose of revising the Articles of Confederation". Although the states' representatives to the Constitutional Convention in Philadelphia were only authorized to amend the Articles, delegates held secret, closed-door sessions and wrote a new constitution. The new frame of government gave much more power to the central government, but characterization of the result is disputed. The general goal of the authors was to get close to a republic as defined by the philosophers of the Age of Enlightenment, while trying to address the many difficulties of the interstate relationships. Historian Forrest McDonald, using the ideas of James Madison from "Federalist 39", described the change this way:
In May 1786, Charles Pinckney of South Carolina proposed that Congress revise the Articles of Confederation. Recommended changes included granting Congress power over foreign and domestic commerce, and providing means for Congress to collect money from state treasuries. Unanimous approval was necessary to make the alterations, however, and Congress failed to reach a consensus. The weakness of the Articles in establishing an effective unifying government was underscored by the threat of internal conflict both within and between the states, especially after Shays' Rebellion threatened to topple the state government of Massachusetts.
Historian Ralph Ketcham commented on the opinions of Patrick Henry, George Mason, and other Anti-Federalists who were not so eager to give up the local autonomy won by the revolution:
Historians have given many reasons for the perceived need to replace the articles in 1787. Jillson and Wilson (1994) point to the financial weakness as well as the norms, rules and institutional structures of the Congress, and the propensity to divide along sectional lines.
Rakove identifies several factors that explain the collapse of the Confederation. The lack of compulsory direct taxation power was objectionable to those wanting a strong centralized state or expecting to benefit from such power. It could not collect customs after the war because tariffs were vetoed by Rhode Island. Rakove concludes that their failure to implement national measures "stemmed not from a heady sense of independence but rather from the enormous difficulties that all the states encountered in collecting taxes, mustering men, and gathering supplies from a war-weary populace." The second group of factors Rakove identified derived from the substantive nature of the problems the Continental Congress confronted after 1783, especially the inability to create a strong foreign policy. Finally, the Confederation's lack of coercive power reduced the likelihood for profit to be made by political means, thus potential rulers were uninspired to seek power.
When the war ended in 1783, certain special interests had incentives to create a new "merchant state," much like the British state people had rebelled against. In particular, holders of war scrip and land speculators wanted a central government to pay off scrip at face value and to legalize western land holdings with disputed claims. Also, manufacturers wanted a high tariff as a barrier to foreign goods, but competition among states made this impossible without a central government.
Legitimacy of closing down.
Two prominent political leaders in the Confederation, John Jay of New York and Thomas Burke of North Carolina believed that "the authority of the congress rested on the prior acts of the several states, to which the states gave their voluntary consent, and until those obligations were fulfilled, neither nullification of the authority of congress, exercising its due powers, nor secession from the compact itself was consistent with the terms of their original pledges."
According to Article XIII of the Confederation, any alteration had to be approved unanimously:
[T]he Articles of this Confederation shall be inviolably observed by every State, and the Union shall be perpetual; nor shall any alteration at any time hereafter be made in any of them; unless such alteration be agreed to in a Congress of the United States, and be afterwards confirmed by the legislatures of every State.
On the other hand, Article VII of the proposed Constitution stated that it would become effective after ratification by a mere nine states, without unanimity:
The Ratification of the Conventions of nine States, shall be sufficient for the Establishment of this Constitution between the States so ratifying the Same.
The apparent tension between these two provisions was addressed at the time, and remains a topic of scholarly discussion. In 1788, James Madison remarked (in "Federalist No. 40") that the issue had become moot: "As this objection… has been in a manner waived by those who have criticised the powers of the convention, I dismiss it without further observation." Nevertheless, it is a historical and legal question whether opponents of the Constitution could have plausibly attacked the Constitution on that ground. At the time, there were state legislators who argued that the Constitution was not an alteration of the Articles of Confederation, but rather would be a complete replacement so the unanimity rule did not apply. Moreover, the Confederation had proven woefully inadequate and therefore was supposedly no longer binding.
Modern scholars such as Francisco Forrest Martin agree that the Articles of Confederation had lost its binding force because many states had violated it, and thus "other states-parties did not have to comply with the Articles' unanimous consent rule". In contrast, law professor Akhil Amar suggests that there may not have really been any conflict between the Articles of Confederation and the Constitution on this point; Article VI of the Confederation specifically allowed side deals among states, and the Constitution could be viewed as a side deal until all states ratified it.
Final months.
On July 3, 1788, the Congress received New Hampshire's all-important ninth ratification of the proposed Constitution, thus, according to its terms, establishing it as the new framework of governance for the ratifying states. The following day delegates considered a bill to admit Kentucky into the Union as a sovereign state. The discussion ended with Congress making the determination that, in light of this development, it would be "unadvisable" to admit Kentucky into the Union, as it could do so "under the Articles of Confederation" only, but not "under the Constitution".
By the end of July 1788, 11 of the 13 states had ratified the new Constitution. Congress continued to convene under the Articles with a quorum until October. On Saturday, September 13, 1788, the Confederation Congress voted the resolve to implement the new Constitution, and on Monday, September 15 published an announcement that the new Constitution had been ratified by the necessary nine states, set the first Wednesday in January 1789 for appointing electors, set the first Wednesday in February 1789 for the presidential electors to meet and vote for a new president, and set the first Wednesday of March 1789 as the day "for commencing proceedings" under the new Constitution. On that same September 13, it determined that New York would remain the national capital.

</doc>
<doc id="693" url="?curid=693" title="Archaeology/Broch">
Archaeology/Broch


</doc>
<doc id="694" url="?curid=694" title="Asia Minor (disambiguation)">
Asia Minor (disambiguation)

Asia Minor is an alternative name for Anatolia, the westernmost protrusion of Asia, comprising the majority of the Republic of Turkey.
Asia Minor may also refer to:

</doc>
<doc id="696" url="?curid=696" title="Aa River">
Aa River


</doc>
<doc id="698" url="?curid=698" title="Atlantic Ocean">
Atlantic Ocean

The Atlantic Ocean is the second-largest of the world's five oceanic divisions, with an area of about . It covers approximately 17% of Earth's surface and about 24% of its water surface area. During the Age of Discovery, it was known for separating the New World of the Americas (North America and South America) from the Old World of Afro-Eurasia (Africa, Asia, and Europe).
Through its separation of Afro-Eurasia from the Americas, the Atlantic Ocean has played a central role in the development of human society, globalization, and the histories of many nations. While the Norse were the first known humans to cross the Atlantic, it was the expedition of Christopher Columbus in 1492 that proved to be the most consequential. Columbus' expedition ushered in an age of exploration and colonization of the Americas by European powers, most notably Portugal, Spain, France, and the United Kingdom. From the 16th to 19th centuries, the Atlantic Ocean was the center of both an eponymous slave trade and the Columbian exchange while occasionally hosting naval battles. Such naval battles, as well as growing trade from regional American powers like the United States and Brazil, both increased in degree during the early 20th century, and while no major military conflicts took place in the Atlantic in the present day, the ocean remains a core component of trade around the world.
The Atlantic Ocean occupies an elongated, S-shaped basin extending longitudinally between Europe and Africa to the east, and the Americas to the west. As one component of the interconnected World Ocean, it is connected in the north to the Arctic Ocean, to the Pacific Ocean in the southwest, the Indian Ocean in the southeast, and the Southern Ocean in the south. Other definitions describe the Atlantic as extending southward to Antarctica. The Atlantic Ocean is divided in two parts, the northern and southern Atlantic, by the Equator.
Toponymy.
The oldest known mentions of an "Atlantic" sea come from Stesichorus around mid-sixth century BC (Sch. A. R. 1. 211): (, , . ) and in "The Histories" of Herodotus around 450 BC (Hdt. 1.202.4): (, or ) where the name refers to "the sea beyond the pillars of Heracles" which is said to be part of the sea that surrounds all land. In these uses, the name refers to Atlas, the Titan in Greek mythology, who supported the heavens and who later appeared as a frontispiece in medieval maps and also lent his name to modern atlases. On the other hand, to early Greek sailors and in ancient Greek mythological literature such as the "Iliad" and the "Odyssey", this all-encompassing ocean was instead known as Oceanus, the gigantic river that encircled the world; in contrast to the enclosed seas well known to the Greeks: the Mediterranean and the Black Sea. In contrast, the term "Atlantic" originally referred specifically to the Atlas Mountains in Morocco and the sea off the Strait of Gibraltar and the West African coast.
The term "Aethiopian Ocean", derived from Ancient Ethiopia, was applied to the southern Atlantic as late as the mid-19th century. During the Age of Discovery, the Atlantic was also known to English cartographers as the Great Western Ocean.
The pond is a term often used by British and American speakers in reference to the northern Atlantic Ocean, as a form of meiosis, or ironic understatement. It is used mostly when referring to events or circumstances "on this side of the pond" or "on the other side of the pond" or "across the pond", rather than to discuss the ocean itself. The term dates to 1640, first appearing in print in pamphlet released during the reign of Charles I, and reproduced in 1869 in Nehemiah Wallington's "Historical Notices of Events Occurring Chiefly in The Reign of Charles I", where "great Pond" is used in reference to the Atlantic Ocean by Francis Windebank, Charles I's Secretary of State.
Extent and data.
The International Hydrographic Organization (IHO) defined the limits of the oceans and seas in 1953, but some of these definitions have been revised since then and some are not recognized by various authorities, institutions, and countries, for example the CIA World Factbook. Correspondingly, the extent and number of oceans and seas vary.
The Atlantic Ocean is bounded on the west by North and South America. It connects to the Arctic Ocean through the Denmark Strait, Greenland Sea, Norwegian Sea and Barents Sea. To the east, the boundaries of the ocean proper are Europe: the Strait of Gibraltar (where it connects with the Mediterranean Seaone of its marginal seasand, in turn, the Black Sea, both of which also touch upon Asia) and Africa.
In the southeast, the Atlantic merges into the Indian Ocean. The 20° East meridian, running south from Cape Agulhas to Antarctica defines its border. In the 1953 definition it extends south to Antarctica, while in later maps it is bounded at the 60° parallel by the Southern Ocean.
The Atlantic has irregular coasts indented by numerous bays, gulfs and seas. These include the Baltic Sea, Black Sea, Caribbean Sea, Davis Strait, Denmark Strait, part of the Drake Passage, Gulf of Mexico, Labrador Sea, Mediterranean Sea, North Sea, Norwegian Sea, almost all of the Scotia Sea, and other tributary water bodies. Including these marginal seas the coast line of the Atlantic measures compared to for the Pacific.
Including its marginal seas, the Atlantic covers an area of or 23.5% of the global ocean and has a volume of or 23.3% of the total volume of the Earth's oceans. Excluding its marginal seas, the Atlantic covers and has a volume of . The North Atlantic covers (11.5%) and the South Atlantic (11.1%). The average depth is and the maximum depth, the Milwaukee Deep in the Puerto Rico Trench, is .
Biggest seas in Atlantic Ocean.
Top large seas:
Bathymetry.
The bathymetry of the Atlantic is dominated by a submarine mountain range called the Mid-Atlantic Ridge (MAR). It runs from 87°N or south of the North Pole to the subantarctic Bouvet Island at 54°S. Expeditions to explore the bathymertry of the Atlantic include the "Challenger" expedition and the German "Meteor" expedition; , Columbia University's Lamont–Doherty Earth Observatory and the United States Navy Hydrographic Office conduct research on the ocean.
Mid-Atlantic Ridge.
The MAR divides the Atlantic longitudinally into two halves, in each of which a series of basins are delimited by secondary, transverse ridges. The MAR reaches above along most of its length, but is interrupted by larger transform faults at two places: the Romanche Trench near the Equator and the Gibbs Fracture Zone at 53°N. The MAR is a barrier for bottom water, but at these two transform faults deep water currents can pass from one side to the other.
The MAR rises above the surrounding ocean floor and its rift valley is the divergent boundary between the North American and Eurasian plates in the North Atlantic and the South American and African plates in the South Atlantic. The MAR produces basaltic volcanoes in Eyjafjallajökull, Iceland, and pillow lava on the ocean floor. The depth of water at the apex of the ridge is less than in most places, while the bottom of the ridge is three times as deep.
The MAR is intersected by two perpendicular ridges: the Azores–Gibraltar Transform Fault, the boundary between the Nubian and Eurasian plates, intersects the MAR at the Azores Triple Junction, on either side of the Azores microplate, near the 40°N. A much vaguer, nameless boundary, between the North American and South American plates, intersects the MAR near or just north of the Fifteen-Twenty Fracture Zone, approximately at 16°N.
In the 1870s, the "Challenger" expedition discovered parts of what is now known as the Mid-Atlantic Ridge, or:
 The remainder of the ridge was discovered in the 1920s by the German "Meteor" expedition using echo-sounding equipment. The exploration of the MAR in the 1950s led to the general acceptance of seafloor spreading and plate tectonics.
Most of the MAR runs under water but where it reaches the surfaces it has produced volcanic islands. While nine of these have collectively been nominated a World Heritage Site for their geological value, four of them are considered of "Outstanding Universal Value" based on their cultural and natural criteria: Þingvellir, Iceland; Landscape of the Pico Island Vineyard Culture, Portugal; Gough and Inaccessible Islands, United Kingdom; and Brazilian Atlantic Islands: Fernando de Noronha and Atol das Rocas Reserves, Brazil.
Ocean floor.
Continental shelves in the Atlantic are wide off Newfoundland, southernmost South America, and northeastern Europe.
In the western Atlantic carbonate platforms dominate large areas, for example, the Blake Plateau and Bermuda Rise.
The Atlantic is surrounded by passive margins except at a few locations where active margins form deep trenches: the Puerto Rico Trench ( maximum depth) in the western Atlantic and South Sandwich Trench () in the South Atlantic. There are numerous submarine canyons off northeastern North America, western Europe, and northwestern Africa. Some of these canyons extend along the continental rises and farther into the abyssal plains as deep-sea channels.
In 1922, a historic moment in cartography and oceanography occurred. The USS "Stewart" used a Navy Sonic Depth Finder to draw a continuous map across the bed of the Atlantic. This involved little guesswork because the idea of sonar is straightforward with pulses being sent from the vessel, which bounce off the ocean floor, then return to the vessel. The deep ocean floor is thought to be fairly flat with occasional deeps, abyssal plains, trenches, seamounts, basins, plateaus, canyons, and some guyots. Various shelves along the margins of the continents constitute about 11% of the bottom topography with few deep channels cut across the continental rise.
The mean depth between 60°N and 60°S is , or close to the average for the global ocean, with a modal depth between .
In the South Atlantic the Walvis Ridge and Rio Grande Rise form barriers to ocean currents.
The Laurentian Abyss is found off the eastern coast of Canada.
Water characteristics.
Surface water temperatures, which vary with latitude, current systems, and season and reflect the latitudinal distribution of solar energy, range from below to over . Maximum temperatures occur north of the equator, and minimum values are found in the polar regions. In the middle latitudes, the area of maximum temperature variations, values may vary by .
From October to June the surface is usually covered with sea ice in the Labrador Sea, Denmark Strait, and Baltic Sea.
The Coriolis effect circulates North Atlantic water in a clockwise direction, whereas South Atlantic water circulates counter-clockwise. The south tides in the Atlantic Ocean are semi-diurnal; that is, two high tides occur every 24 lunar hours. In latitudes above 40° North some east–west oscillation, known as the North Atlantic oscillation, occurs.
Salinity.
On average, the Atlantic is the saltiest major ocean; surface water salinity in the open ocean ranges from 33 to 37 parts per thousand (3.3–3.7%) by mass and varies with latitude and season. Evaporation, precipitation, river inflow and sea ice melting influence surface salinity values. Although the lowest salinity values are just north of the equator (because of heavy tropical rainfall), in general, the lowest values are in the high latitudes and along coasts where large rivers enter. Maximum salinity values occur at about 25° north and south, in subtropical regions with low rainfall and high evaporation.
The high surface salinity in the Atlantic, on which the Atlantic thermohaline circulation is dependent, is maintained by two processes: the Agulhas Leakage/Rings, which brings salty Indian Ocean waters into the South Atlantic, and the "Atmospheric Bridge", which evaporates subtropical Atlantic waters and exports it to the Pacific.
Water masses.
The Atlantic Ocean consists of four major, upper water masses with distinct temperature and salinity. The Atlantic subarctic upper water in the northernmost North Atlantic is the source for subarctic intermediate water and North Atlantic intermediate water. North Atlantic central water can be divided into the eastern and western North Atlantic central water since the western part is strongly affected by the Gulf Stream and therefore the upper layer is closer to underlying fresher subpolar intermediate water. The eastern water is saltier because of its proximity to Mediterranean water. North Atlantic central water flows into South Atlantic central water at 15°N.
There are five intermediate waters: four low-salinity waters formed at subpolar latitudes and one high-salinity formed through evaporation. Arctic intermediate water, flows from the north to become the source for North Atlantic deep water, south of the Greenland-Scotland sill. These two intermediate waters have different salinity in the western and eastern basins. The wide range of salinities in the North Atlantic is caused by the asymmetry of the northern subtropical gyre and the large number of contributions from a wide range of sources: Labrador Sea, Norwegian-Greenland Sea, Mediterranean, and South Atlantic Intermediate Water.
The North Atlantic deep water (NADW) is a complex of four water masses, two that form by deep convection in the open oceanclassical and upper Labrador sea waterand two that form from the inflow of dense water across the Greenland-Iceland-Scotland sillDenmark Strait and Iceland-Scotland overflow water. Along its path across Earth the composition of the NADW is affected by other water masses, especially Antarctic bottom water and Mediterranean overflow water.
The NADW is fed by a flow of warm shallow water into the northern North Atlantic which is responsible for the anomalous warm climate in Europe. Changes in the formation of NADW have been linked to global climate changes in the past. Since human-made substances were introduced into the environment, the path of the NADW can be traced throughout its course by measuring tritium and radiocarbon from nuclear weapon tests in the 1960s and CFCs.
Gyres.
The clockwise warm-water North Atlantic Gyre occupies the northern Atlantic, and the counter-clockwise warm-water South Atlantic Gyre appears in the southern Atlantic.
In the North Atlantic, surface circulation is dominated by three inter-connected currents: the Gulf Stream which flows north-east from the North American coast at Cape Hatteras; the North Atlantic Current, a branch of the Gulf Stream which flows northward from the Grand Banks; and the Subpolar Front, an extension of the North Atlantic Current, a wide, vaguely defined region separating the subtropical gyre from the subpolar gyre. This system of currents transport warm water into the North Atlantic, without which temperatures in the North Atlantic and Europe would plunge dramatically.
North of the North Atlantic Gyre, the cyclonic North Atlantic Subpolar Gyre plays a key role in climate variability. It is governed by ocean currents from marginal seas and regional topography, rather than being steered by wind, both in the deep ocean and at sea level.
The subpolar gyre forms an important part of the global thermohaline circulation. Its eastern portion includes eddying branches of the North Atlantic Current which transport warm, saline waters from the subtropics to the northeastern Atlantic. There this water is cooled during winter and forms return currents that merge along the eastern continental slope of Greenland where they form an intense (40–50 Sv) current which flows around the continental margins of the Labrador Sea. A third of this water becomes part of the deep portion of the North Atlantic Deep Water (NADW). The NADW, in its turn, feeds the meridional overturning circulation (MOC), the northward heat transport of which is threatened by anthropogenic climate change. Large variations in the subpolar gyre on a decade-century scale, associated with the North Atlantic oscillation, are especially pronounced in Labrador Sea Water, the upper layers of the MOC.
The South Atlantic is dominated by the anti-cyclonic southern subtropical gyre. The South Atlantic central water originates in this gyre, while Antarctic Intermediate Water originates in the upper layers of the circumpolar region, near the Drake Passage and the Falkland Islands. Both these currents receive some contribution from the Indian Ocean. On the African east coast, the small cyclonic Angola Gyre lies embedded in the large subtropical gyre.
The southern subtropical gyre is partly masked by a wind-induced Ekman layer. The residence time of the gyre is 4.4–8.5 years. North Atlantic Deep Water flows southward below the thermocline of the subtropical gyre.
Sargasso Sea.
The Sargasso Sea in the western North Atlantic can be defined as the area where two species of "Sargassum" ("S. fluitans" and "natans") float, an area wide and encircled by the Gulf Stream, North Atlantic Drift, and North Equatorial Current. This population of seaweed probably originated from Tertiary ancestors on the European shores of the former Tethys Ocean and has, if so, maintained itself by vegetative growth, floating in the ocean for millions of years.
Other species endemic to the Sargasso Sea include the sargassum fish, a predator with algae-like appendages which hovers motionless among the "Sargassum". Fossils of similar fishes have been found in fossil bays of the former Tethys Ocean, in what is now the Carpathian region, that were similar to the Sargasso Sea. It is possible that the population in the Sargasso Sea migrated to the Atlantic as the Tethys closed at the end of the Miocene around 17 Ma. The origin of the Sargasso fauna and flora remained enigmatic for centuries. The fossils found in the Carpathians in the mid-20th century often called the "quasi-Sargasso assemblage", finally showed that this assemblage originated in the Carpathian Basin from where it migrated over Sicily to the central Atlantic where it evolved into modern species of the Sargasso Sea.
The location of the spawning ground for European eels remained unknown for decades. In the early 19th century it was discovered that the southern Sargasso Sea is the spawning ground for both the European and American eel and that the former migrate more than and the latter . Ocean currents such as the Gulf Stream transport eel larvae from the Sargasso Sea to foraging areas in North America, Europe, and northern Africa. Recent but disputed research suggests that eels possibly use Earth's magnetic field to navigate through the ocean both as larvae and as adults.
Climate.
Climate is influenced by the temperatures of the surface waters and water currents as well as winds. Because of the ocean's great capacity to store and release heat, maritime climates are more moderate and have less extreme seasonal variations than inland climates. Precipitation can be approximated from coastal weather data and air temperature from water temperatures.
The oceans are the major source of the atmospheric moisture that is obtained through evaporation. Climatic zones vary with latitude; the warmest zones stretch across the Atlantic north of the equator. The coldest zones are in high latitudes, with the coldest regions corresponding to the areas covered by sea ice. Ocean currents influence the climate by transporting warm and cold waters to other regions. The winds that are cooled or warmed when blowing over these currents influence adjacent land areas.
The Gulf Stream and its northern extension towards Europe, the North Atlantic Drift is thought to have at least some influence on climate. For example, the Gulf Stream helps moderate winter temperatures along the coastline of southeastern North America, keeping it warmer in winter along the coast than inland areas. The Gulf Stream also keeps extreme temperatures from occurring on the Florida Peninsula. In the higher latitudes, the North Atlantic Drift, warms the atmosphere over the oceans, keeping the British Isles and northwestern Europe mild and cloudy, and not severely cold in winter, like other locations at the same high latitude. The cold water currents contribute to heavy fog off the coast of eastern Canada (the Grand Banks of Newfoundland area) and Africa's northwestern coast. In general, winds transport moisture and air over land areas.
Natural hazards.
Every winter, the Icelandic Low produces frequent storms. Icebergs are common from early February to the end of July across the shipping lanes near the Grand Banks of Newfoundland. The ice season is longer in the polar regions, but there is little shipping in those areas.
Hurricanes are a hazard in the western parts of the North Atlantic during the summer and autumn. Due to a consistently strong wind shear and a weak Intertropical Convergence Zone, South Atlantic tropical cyclones are rare.
Geology and plate tectonics.
The Atlantic Ocean is underlain mostly by dense mafic oceanic crust made up of basalt and gabbro and overlain by fine clay, silt and siliceous ooze on the abyssal plain. The continental margins and continental shelf mark lower density, but greater thickness felsic continental rock that is often much older than that of the seafloor. The oldest oceanic crust in the Atlantic is up to 145 million years and situated off the west coast of Africa and east coast of North America, or on either side of the South Atlantic.
In many places, the continental shelf and continental slope are covered in thick sedimentary layers. For instance, on the North American side of the ocean, large carbonate deposits formed in warm shallow waters such as Florida and the Bahamas, while coarse river outwash sands and silt are common in shallow shelf areas like the Georges Bank. Coarse sand, boulders, and rocks were transported into some areas, such as off the coast of Nova Scotia or the Gulf of Maine during the Pleistocene ice ages.
Central Atlantic.
The break-up of Pangaea began in the central Atlantic, between North America and Northwest Africa, where rift basins opened during the Late Triassic and Early Jurassic. This period also saw the first stages of the uplift of the Atlas Mountains. The exact timing is controversial with estimates ranging from 200 to 170 Ma.
The opening of the Atlantic Ocean coincided with the initial break-up of the supercontinent Pangaea, both of which were initiated by the eruption of the Central Atlantic Magmatic Province (CAMP), one of the most extensive and voluminous large igneous provinces in Earth's history associated with the Triassic–Jurassic extinction event, one of Earth's major extinction events.
Theoliitic dikes, sills, and lava flows from the CAMP eruption at 200 Ma have been found in West Africa, eastern North America, and northern South America. The extent of the volcanism has been estimated to of which covered what is now northern and central Brazil.
The formation of the Central American Isthmus closed the Central American Seaway at the end of the Pliocene 2.8 Ma ago. The formation of the isthmus resulted in the migration and extinction of many land-living animals, known as the Great American Interchange, but the closure of the seaway resulted in a "Great American Schism" as it affected ocean currents, salinity, and temperatures in both the Atlantic and Pacific. Marine organisms on both sides of the isthmus became isolated and either diverged or went extinct.
North Atlantic.
Geologically, the northern Atlantic is the area delimited to the south by two conjugate margins, Newfoundland and Iberia, and to the north by the Arctic Eurasian Basin. The opening of the northern Atlantic closely followed the margins of its predecessor, the Iapetus Ocean, and spread from the central Atlantic in six stages: Iberia–Newfoundland, Porcupine–North America, Eurasia–Greenland, Eurasia–North America. Active and inactive spreading systems in this area are marked by the interaction with the Iceland hotspot.
Seafloor spreading led to the extension of the crust and formations of troughs and sedimentary basins. The Rockall Trough opened between 105 and 84 million years ago although along the rift failed along with one leading into the Bay of Biscay. 
Spreading began opening the Labrador Sea around 61 million years ago, continuing until 36 million years ago. Geologists distinguish two magmatic phases. One from 62 to 58 million years ago predates the separation of Greenland from northern Europe while the second from 56 to 52 million years ago happened as the separation occurred.
Iceland began to form 62 million years ago due to a particularly concentrated mantle plume. Large quantities of basalt erupted at this time period are found on Baffin Island, Greenland, the Faroe Islands, and Scotland, with ash falls in Western Europe acting as a stratigraphic marker. The opening of the North Atlantic caused significant uplift of continental crust along the coast. For instance, in spite of 7 km thick basalt, Gunnbjorn Field in East Greenland is the highest point on the island, elevated enough that it exposes older Mesozoic sedimentary rocks at its base, similar to old lava fields above sedimentary rocks in the uplifted Hebrides of western Scotland. 
The North Atlantic Ocean contains about 810 seamounts, most of them situated along the Mid-Atlantic Ridge. The OSPAR database (Convention for the Protection of the Marine Environment of the North-East Atlantic) mentions 104 seamounts: 74 within national exclusive economic zones. Of these seamounts, 46 are located close to the Iberian Peninsula.
South Atlantic.
West Gondwana (South America and Africa) broke up in the Early Cretaceous to form the South Atlantic. The apparent fit between the coastlines of the two continents was noted on the first maps that included the South Atlantic and it was also the subject of the first computer-assisted plate tectonic reconstructions in 1965. This magnificent fit, however, has since then proven problematic and later reconstructions have introduced various deformation zones along the shorelines to accommodate the northward-propagating break-up. Intra-continental rifts and deformations have also been introduced to subdivide both continental plates into sub-plates.
Geologically the South Atlantic can be divided into four segments: equatorial segment, from 10°N to the Romanche fracture zone (RFZ); central segment, from RFZ to Florianopolis fracture zone (FFZ, north of Walvis Ridge and Rio Grande Rise); southern segment, from FFZ to the Agulhas-Falkland fracture zone (AFFZ); and Falkland segment, south of AFFZ.
In the southern segment the Early Cretaceous (133–130 Ma) intensive magmatism of the Paraná–Etendeka Large Igneous Province produced by the Tristan hotspot resulted in an estimated volume of . It covered an area of in Brazil, Paraguay, and Uruguay and in Africa. Dyke swarms in Brazil, Angola, eastern Paraguay, and Namibia, however, suggest the LIP originally covered a much larger area and also indicate failed rifts in all these areas. Associated offshore basaltic flows reach as far south as the Falkland Islands and South Africa. Traces of magmatism in both offshore and onshore basins in the central and southern segments have been dated to 147–49 Ma with two peaks between 143 and 121 Ma and 90–60 Ma.
In the Falkland segment rifting began with dextral movements between the Patagonia and Colorado sub-plates between the Early Jurassic (190 Ma) and the Early Cretaceous (126.7 Ma). Around 150 Ma sea-floor spreading propagated northward into the southern segment. No later than 130 Ma rifting had reached the Walvis Ridge–Rio Grande Rise.
In the central segment rifting started to break Africa in two by opening the Benue Trough around 118 Ma. Rifting in the central segment, however, coincided with the Cretaceous Normal Superchron (also known as the Cretaceous quiet period), a 40 Ma period without magnetic reversals, which makes it difficult to date sea-floor spreading in this segment.
The equatorial segment is the last phase of the break-up, but, because it is located on the Equator, magnetic anomalies cannot be used for dating. Various estimates date the propagation of seafloor spreading in this segment and consequent opening of the Equatorial Atlantic Gateway (EAG) to the period 120–96 Ma. This final stage, nevertheless, coincided with or resulted in the end of continental extension in Africa.
About 50 Ma the opening of the Drake Passage resulted from a change in the motions and separation rate of the South American and Antarctic plates. First, small ocean basins opened and a shallow gateway appeared during the Middle Eocene. 34–30 Ma a deeper seaway developed, followed by an Eocene–Oligocene climatic deterioration and the growth of the Antarctic ice sheet.
Closure of the Atlantic.
An embryonic subduction margin is potentially developing west of Gibraltar. The Gibraltar Arc in the western Mediterranean is migrating westward into the central Atlantic where it joins the converging African and Eurasian plates. Together these three tectonic forces are slowly developing into a new subduction system in the eastern Atlantic Basin. Meanwhile, the Scotia Arc and Caribbean Plate in the western Atlantic Basin are eastward-propagating subduction systems that might, together with the Gibraltar system, represent the beginning of the closure of the Atlantic Ocean and the final stage of the Atlantic Wilson cycle.
History.
Human origin.
Humans evolved in Africa; around 7 mya; then developing stone tools around 2.6 mya; to finally evolve as modern humans around 200 kya. The earliest evidence for the complex behavior associated with this behavioral modernity has been found in the Greater Cape Floristic Region (GCFR) along the coast of South Africa. During the latest glacial stages, the now-submerged plains of the Agulhas Bank were exposed above sea level, extending the South African coastline farther south by hundreds of kilometers. A small population of modern humansprobably fewer than a thousand reproducing individualssurvived glacial maxima by exploring the high diversity offered by these Palaeo-Agulhas plains. The GCFR is delimited to the north by the Cape Fold Belt and the limited space south of it resulted in the development of social networks out of which complex Stone Age technologies emerged. Human history thus begins on the coasts of South Africa where the Atlantic Benguela Upwelling and Indian Ocean Agulhas Current meet to produce an intertidal zone on which shellfish, fur seal, fish and sea birds provided the necessary protein sources.
The African origin of this modern behaviour is evidenced by 70,000 years-old engravings from Blombos Cave, South Africa.
Old World.
Mitochondrial DNA (mtDNA) studies indicate that 80–60,000 years ago a major demographic expansion within Africa, derived from a single, small population, coincided with the emergence of behavioral complexity and the rapid MIS 5–4 environmental changes. This group of people not only expanded over the whole of Africa, but also started to disperse out of Africa into Asia, Europe, and Australasia around 65,000 years ago and quickly replaced the archaic humans in these regions. During the Last Glacial Maximum (LGM) 20,000 years ago humans had to abandon their initial settlements along the European North Atlantic coast and retreat to the Mediterranean. Following rapid climate changes at the end of the LGM this region was repopulated by Magdalenian culture. Other hunter-gatherers followed in waves interrupted by large-scale hazards such as the Laacher See volcanic eruption, the inundation of Doggerland (now the North Sea), and the formation of the Baltic Sea. The European coasts of the North Atlantic were permanently populated about 9–8.5 thousand years ago.
This human dispersal left abundant traces along the coasts of the Atlantic Ocean. 50 kya-old, deeply stratified shell middens found in Ysterfontein on the western coast of South Africa are associated with the Middle Stone Age (MSA). The MSA population was small and dispersed and the rate of their reproduction and exploitation was less intense than those of later generations. While their middens resemble 12–11 kya-old Late Stone Age (LSA) middens found on every inhabited continent, the 50–45 kya-old Enkapune Ya Muto in Kenya probably represents the oldest traces of the first modern humans to disperse out of Africa.
The same development can be seen in Europe. In La Riera Cave (23–13 kya) in Asturias, Spain, only some 26,600 molluscs were deposited over 10 kya. In contrast, 8–7 kya-old shell middens in Portugal, Denmark, and Brazil generated thousands of tons of debris and artefacts. The Ertebølle middens in Denmark, for example, accumulated of shell deposits representing some 50 million molluscs over only a thousand years. This intensification in the exploitation of marine resources has been described as accompanied by new technologiessuch as boats, harpoons, and fish-hooksbecause many caves found in the Mediterranean and on the European Atlantic coast have increased quantities of marine shells in their upper levels and reduced quantities in their lower. The earliest exploitation, however, took place on the now submerged shelves, and most settlements now excavated were then located several kilometers from these shelves. The reduced quantities of shells in the lower levels can represent the few shells that were exported inland.
New World.
During the LGM the Laurentide Ice Sheet covered most of northern North America while Beringia connected Siberia to Alaska. In 1973, late American geoscientist Paul S. Martin proposed a "blitzkrieg" colonization of the Americas by which Clovis hunters migrated into North America around 13,000 years ago in a single wave through an ice-free corridor in the ice sheet and "spread southward explosively, briefly attaining a density sufficiently large to overkill much of their prey." Others later proposed a "three-wave" migration over the Bering Land Bridge. These hypotheses remained the long-held view regarding the settlement of the Americas, a view challenged by more recent archaeological discoveries: the oldest archaeological sites in the Americas have been found in South America; sites in northeast Siberia report virtually no human presence there during the LGM; and most Clovis artefacts have been found in eastern North America along the Atlantic coast. Furthermore, colonisation models based on mtDNA, yDNA, and atDNA data respectively support neither the "blitzkrieg" nor the "three-wave" hypotheses but they also deliver mutually ambiguous results. Contradictory data from archaeology and genetics will most likely deliver future hypotheses that will, eventually, confirm each other. A proposed route across the Pacific to South America could explain early South American finds and another hypothesis proposes a northern path, through the Canadian Arctic and down the North American Atlantic coast.
Early settlements across the Atlantic have been suggested by alternative theories, ranging from purely hypothetical to mostly disputed, including the Solutrean hypothesis and some of the Pre-Columbian trans-oceanic contact theories.
The Norse settlement of the Faroe Islands and Iceland began during the 9th and 10th centuries. A settlement on Greenland was established before 1000 CE, but contact with it was lost in 1409 and it was finally abandoned during the early Little Ice Age. This setback was caused by a range of factors: an unsustainable economy resulted in erosion and denudation, while conflicts with the local Inuit resulted in the failure to adapt their Arctic technologies; a colder climate resulted in starvation, and the colony got economically marginalized as the Great Plague harvested its victims on Iceland in the 15th century.
Iceland was initially settled 865–930 CE following a warm period when winter temperatures hovered around which made farming favorable at high latitudes. This did not last, however, and temperatures quickly dropped; at 1080 CE summer temperatures had reached a maximum of . The ("Book of Settlement") records disastrous famines during the first century of settlement"men ate foxes and ravens" and "the old and helpless were killed and thrown over cliffs"and by the early 1200s hay had to be abandoned for short-season crops such as barley.
Atlantic World.
Christopher Columbus reached the Americas in 1492, sailing under the Spanish flag. Six years later Vasco da Gama reached India under the Portuguese flag, by navigating south around the Cape of Good Hope, thus proving that the Atlantic and Indian Oceans are connected. In 1500, in his voyage to India following Vasco da Gama, Pedro Álvares Cabral reached Brazil, taken by the currents of the South Atlantic Gyre. Following these explorations, Spain and Portugal quickly conquered and colonized large territories in the New World and forced the Amerindian population into slavery in order to exploit the vast quantities of silver and gold they found. Spain and Portugal monopolized this trade in order to keep other European nations out, but conflicting interests nevertheless led to a series of Spanish-Portuguese wars. A peace treaty mediated by the Pope divided the conquered territories into Spanish and Portuguese sectors while keeping other colonial powers away. England, France, and the Dutch Republic enviously watched the Spanish and Portuguese wealth grow and allied themselves with pirates such as Henry Mainwaring and Alexandre Exquemelin. They could explore the convoys leaving the Americas because prevailing winds and currents made the transport of heavy metals slow and predictable.
In the colonies of the Americas, depredation, smallpox and others diseases, and slavery quickly reduced the indigenous population of the Americas to the extent that the Atlantic slave trade had to be introduced to replace thema trade that became the norm and an integral part of the colonization. Between the 15th century and 1888, when Brazil became the last part of the Americas to end the slave trade, an estimated ten million Africans were exported as slaves, most of them destined for agricultural labour. The slave trade was officially abolished in the British Empire and the United States in 1808, and slavery itself was abolished in the British Empire in 1838 and in the United States in 1865 after the Civil War.
From Columbus to the Industrial Revolution Trans-Atlantic trade, including colonialism and slavery, became crucial for Western Europe. For European countries with direct access to the Atlantic (including Britain, France, the Netherlands, Portugal, and Spain) 1500–1800 was a period of sustained growth during which these countries grew richer than those in Eastern Europe and Asia. Colonialism evolved as part of the Trans-Atlantic trade, but this trade also strengthened the position of merchant groups at the expense of monarchs. Growth was more rapid in non-absolutist countries, such as Britain and the Netherlands, and more limited in absolutist monarchies, such as Portugal, Spain, and France, where profit mostly or exclusively benefited the monarchy and its allies.
Trans-Atlantic trade also resulted in increasing urbanization: in European countries facing the Atlantic, urbanization grew from 8% in 1300, 10.1% in 1500, to 24.5% in 1850; in other European countries from 10% in 1300, 11.4% in 1500, to 17% in 1850. Likewise, GDP doubled in Atlantic countries but rose by only 30% in the rest of Europe. By end of the 17th century, the volume of the Trans-Atlantic trade had surpassed that of the Mediterranean trade.
Economy.
The Atlantic has contributed significantly to the development and economy of surrounding countries. Besides major transatlantic transportation and communication routes, the Atlantic offers abundant petroleum deposits in the sedimentary rocks of the continental shelves.
The Atlantic harbors petroleum and gas fields, fish, marine mammals (seals and whales), sand and gravel aggregates, placer deposits, polymetallic nodules, and precious stones.
Gold deposits are a mile or two under water on the ocean floor, however, the deposits are also encased in rock that must be mined through. Currently, there is no cost-effective way to mine or extract gold from the ocean to make a profit.
Various international treaties attempt to reduce pollution caused by environmental threats such as oil spills, marine debris, and the incineration of toxic wastes at sea.
Fisheries.
The shelves of the Atlantic hosts one of the world's richest fishing resources. The most productive areas include the Grand Banks of Newfoundland, the Scotian Shelf, Georges Bank off Cape Cod, the Bahama Banks, the waters around Iceland, the Irish Sea, the Bay of Fundy, the Dogger Bank of the North Sea, and the Falkland Banks.
Fisheries have, however, undergone significant changes since the 1950s and global catches can now be divided into three groups of which only two are observed in the Atlantic: fisheries in the eastern-central and southwest Atlantic oscillate around a globally stable value, the rest of the Atlantic is in overall decline following historical peaks. The third group, "continuously increasing trend since 1950", is only found in the Indian Ocean and western Pacific.
UN FAO partitioned Atlantic in major fishing areas:
Northeast Atlantic is schematically limited to the 40°00' west longitude (except around Greenland), south to the 36°00' north latitude, and to the 68°30' east longitude, with both the west and east longitude limits reaching to the north pole. The Atlantic's subareas include: Barents Sea; Norwegian Sea, Spitzbergen, and Bear Island; Skagerrak, Kattegat, Sound, Belt Sea, and Baltic Sea; North Sea; Iceland and Faroes Grounds; Rockall, Northwest Coast of Scotland, and North Ireland; Irish Sea, West of Ireland, Porcupine Bank, and eastern and western English Channel; Bay of Biscay; Portuguese Waters; Azores Grounds and Northeast Atlantic South; North of Azores; and East Greenland. There are also two defunct subareas.
In 1497, John Cabot became the first Western European since the Vikings to explore mainland North America and one of his major discoveries was the abundant resources of Atlantic cod off Newfoundland. Referred to as "Newfoundland Currency" this discovery yielded some 200 million tons of fish over five centuries. In the late 19th and early 20th centuries, new fisheries started to exploit haddock, mackerel, and lobster. From the 1950s to the 1970s, the introduction of European and Asian distant-water fleets in the area dramatically increased the fishing capacity and the number of exploited species. It also expanded the exploited areas from near-shore to the open sea and to great depths to include deep-water species such as redfish, Greenland halibut, witch flounder, and grenadiers. Overfishing in the area was recognized as early as the 1960s but, because this was occurring on international waters, it took until the late 1970s before any attempts to regulate was made. In the early 1990s, this finally resulted in the collapse of the Atlantic northwest cod fishery. The population of a number of deep-sea fishes also collapsed in the process, including American plaice, redfish, and Greenland halibut, together with flounder and grenadier.
Environmental issues.
Endangered species.
Endangered marine species include the manatee, seals, sea lions, turtles, and whales. Drift net fishing can kill dolphins, albatrosses and other seabirds (petrels, auks), hastening the fish stock decline and contributing to international disputes.
Waste and pollution.
Marine pollution is a generic term for the entry into the ocean of potentially hazardous chemicals or particles. The biggest culprits are rivers and with them many agriculture fertilizer chemicals as well as livestock and human waste. The excess of oxygen-depleting chemicals leads to hypoxia and the creation of a dead zone.
Marine debris, which is also known as marine litter, describes human-created waste floating in a body of water. Oceanic debris tends to accumulate at the center of gyres and coastlines, frequently washing aground where it is known as beach litter. The North Atlantic garbage patch is estimated to be hundreds of kilometers across in size.
Other pollution concerns include agricultural and municipal waste. Municipal pollution comes from the eastern United States, southern Brazil, and eastern Argentina; oil pollution in the Caribbean Sea, Gulf of Mexico, Lake Maracaibo, Mediterranean Sea, and North Sea; and industrial waste and municipal sewage pollution in the Baltic Sea, North Sea, and Mediterranean Sea.
A USAF C-124 aircraft from Dover Air Force Base, Delaware was carrying three nuclear bombs over the Atlantic Ocean when it experienced a loss of power. For their own safety, the crew jettisoned two nuclear bombs, which were never recovered.
Climate change.
North Atlantic hurricane activity has increased over past decades because of increased sea surface temperature (SST) at tropical latitudes, changes that can be attributed to either the natural Atlantic Multidecadal Oscillation (AMO) or to anthropogenic climate change.
A 2005 report indicated that the Atlantic meridional overturning circulation (AMOC) slowed down by 30% between 1957 and 2004. In 2024, research highlighted a significant weakening of the AMOC by approximately 12% over the past two decades. If the AMO were responsible for SST variability, the AMOC would have increased in strength, which is apparently not the case. Furthermore, it is clear from statistical analyses of annual tropical cyclones that these changes do not display multidecadal cyclicity. Therefore, these changes in SST must be caused by human activities.
The ocean mixed layer plays an important role in heat storage over seasonal and decadal time-scales, whereas deeper layers are affected over millennia and have a heat capacity about 50 times that of the mixed layer. This heat uptake provides a time-lag for climate change but it also results in thermal expansion of the oceans which contributes to sea level rise. 21st-century global warming will probably result in an equilibrium sea-level rise five times greater than today, whilst melting of glaciers, including that of the Greenland ice-sheet, expected to have virtually no effect during the 21st century, will likely result in a sea-level rise of over a millennium.
Theories of natural delimitation between the Atlantic and Pacific oceans.
Scientific researchers have proposed delimiting the boundary between the Atlantic and Pacific oceans by two different natural boundaries, by the Shackleton Fracture Zone and by the Scotia Arc the former being more current than the latter.

</doc>
<doc id="700" url="?curid=700" title="Arthur Schopenhauer">
Arthur Schopenhauer

Arthur Schopenhauer ( , ; 22 February 1788 – 21 September 1860) was a German philosopher. He is known for his 1818 work "The World as Will and Representation" (expanded in 1844), which characterizes the phenomenal world as the manifestation of a blind and irrational noumenal will. Building on the transcendental idealism of Immanuel Kant (1724–1804), Schopenhauer developed an atheistic metaphysical and ethical system that rejected the contemporaneous ideas of German idealism.
Schopenhauer was among the first thinkers in Western philosophy to share and affirm significant tenets of Indian philosophy, such as asceticism, denial of the self, and the notion of the world-as-appearance. His work has been described as an exemplary manifestation of philosophical pessimism. Though his work failed to garner substantial attention during his lifetime, he had a posthumous impact across various disciplines, including philosophy, literature, and science. His writing on aesthetics, morality, and psychology have influenced many thinkers and artists.
Life.
Early life.
Arthur Schopenhauer was born on 22 February 1788, in Gdańsk (then part of the Polish–Lithuanian Commonwealth; later in the Kingdom of Prussia "Danzig") on Św. Ducha 47 (in Prussia Heiliggeistgasse), the son of Johanna Schopenhauer (née Trosiener; 1766–1838) and Heinrich Floris Schopenhauer (1747–1805), both descendants of wealthy German patrician families. While they came from a Protestant background, neither of them was very religious; both supported the French Revolution, were republicans, cosmopolitans and Anglophiles. When Gdańsk became part of Prussia in 1793, Heinrich moved to Hamburg—a free city with a republican constitution. His firm continued trading in Danzig where most of their extended families remained. Adele, Arthur's only sibling, was born on 12 July 1797.
In 1797, Arthur was sent to Le Havre to live with the family of his father's business associate, Grégoire de Blésimaire. He seemed to enjoy his two-year stay there, learning to speak French and fostering a life-long friendship with Jean Anthime Grégoire de Blésimaire. As early as 1799, Arthur started playing the flute.
In 1803, he accompanied his parents on a European tour of Holland, Britain, France, Switzerland, Austria and Prussia. Viewed as primarily a pleasure tour, Heinrich used the opportunity to visit some of his business associates abroad.
Heinrich presented Arthur with a choice: he could either stay at home to begin preparations for university or travel with them to further his merchant education. Arthur chose to travel with them. He deeply regretted his choice later because the merchant training was very tedious. He spent twelve weeks of the tour attending school in Wimbledon, where he was confused by strict and intellectual Anglicans who he'd described as shallow. He continued to sharply criticize Anglican religiosity later in life despite his general Anglophilia. He was also under pressure from his father, who became very critical of his educational results.
In 1805, Heinrich drowned in a canal near their home in Hamburg. Although it was possible that his death was accidental, his wife and son believed that it was suicide. He was prone to anxiety and depression, each becoming more pronounced later in his life. Heinrich had become so fussy, even his wife started to doubt his mental health. "There was, in the father's life, some dark and vague source of fear which later made him hurl himself to his death from the attic of his house in Hamburg."
Arthur showed similar moodiness during his youth and often acknowledged that he inherited it from his father. There were other instances of serious mental health problems on his father's side of the family. Despite his hardship, Schopenhauer liked his father and later referred to him in a positive light. Heinrich Schopenhauer left the family with a significant inheritance that was split in three among Johanna and the children. Arthur Schopenhauer was entitled to control of his part when he reached the age of majority. He invested it conservatively in government bonds and earned annual interest that was more than double the salary of a university professor. After quitting his merchant apprenticeship, with some encouragement from his mother, he dedicated himself to studies at the Ernestine Gymnasium, Gotha, in Saxe-Gotha-Altenburg. While there, he also enjoyed social life among the local nobility, spending large amounts of money, which deeply concerned his frugal mother. He left the Gymnasium after writing a satirical poem about one of the schoolmasters. Although Arthur claimed that he left voluntarily, his mother's letter indicates that he may have been expelled.
Arthur spent two years as a merchant in honor of his dead father. During this time, he had doubts about being able to start a new life as a scholar. Most of his prior education was as a practical merchant and he had trouble learning Latin; a prerequisite for an academic career.
His mother moved away, with her daughter Adele, to Weimar—then the centre of German literature—to enjoy social life among writers and artists. Arthur and his mother did not part on good terms. In one letter, she wrote: "You are unbearable and burdensome, and very hard to live with; all your good qualities are overshadowed by your conceit, and made useless to the world simply because you cannot restrain your propensity to pick holes in other people." His mother, Johanna, was generally described as vivacious and sociable. She died 24 years later. Some of Arthur's negative opinions about women may be rooted in his troubled relationship with his mother.
Arthur moved to Hamburg to live with his friend Jean Anthime, who was also studying to become a merchant.
Education.
He moved to Weimar but did not live with his mother, who even tried to discourage him from coming by explaining that they would not get along very well. Their relationship deteriorated even further due to their temperamental differences. He accused his mother of being financially irresponsible, flirtatious and seeking to remarry, which he considered an insult to his father's memory. His mother, while professing her love to him, criticized him sharply for being moody, tactless, and argumentative, and urged him to improve his behavior so that he would not alienate people. Arthur concentrated on his studies, which were now going very well, and he also enjoyed the usual social life such as balls, parties and theater. By that time Johanna's famous salon was well established among local intellectuals and dignitaries, the most celebrated of them being Goethe. Arthur attended her parties, usually when he knew that Goethe would be there—although the famous writer and statesman seemed not even to notice the young and unknown student. It is possible that Goethe kept a distance because Johanna warned him about her son's depressive and combative nature, or because Goethe was then on bad terms with Arthur's language instructor and roommate, Franz Passow. Schopenhauer was also captivated by the beautiful Karoline Jagemann, mistress of Karl August, Grand Duke of Saxe-Weimar-Eisenach, and he wrote to her his only known love poem. Despite his later celebration of asceticism and negative views of sexuality, Schopenhauer occasionally had sexual affairs—usually with women of lower social status, such as servants, actresses, and sometimes even paid prostitutes. In a letter to his friend Anthime he claims that such affairs continued even in his mature age and admits that he had two out-of-wedlock daughters (born in 1819 and 1836), both of whom died in infancy. In their youthful correspondence Arthur and Anthime were somewhat boastful and competitive about their sexual exploits—but Schopenhauer seemed aware that women usually did not find him very charming or physically attractive, and his desires often remained unfulfilled.
He left Weimar to become a student at the University of Göttingen in 1809. There are no written reasons about why Schopenhauer chose that university instead of the then more famous University of Jena, but Göttingen was known as more modern and scientifically oriented, with less attention given to theology. Law or medicine were usual choices for young men of Schopenhauer's status who also needed career and income; he chose medicine due to his scientific interests. Among his notable professors were Bernhard Friedrich Thibaut, Arnold Hermann Ludwig Heeren, Johann Friedrich Blumenbach, Friedrich Stromeyer, Heinrich Adolf Schrader, Johann Tobias Mayer and Konrad Johann Martin Langenbeck. He studied metaphysics, psychology and logic under Gottlob Ernst Schulze, the author of "Aenesidemus", who made a strong impression and advised him to concentrate on Plato and Immanuel Kant. He decided to switch from medicine to philosophy around 1810–11 and he left Göttingen, which did not have a strong philosophy program: besides Schulze, the only other philosophy professor was Friedrich Bouterwek, whom Schopenhauer disliked. He did not regret his medicinal and scientific studies; he claimed that they were necessary for a philosopher, and even in Berlin he attended more lectures in sciences than in philosophy. During his days at Göttingen, he spent considerable time studying, but also continued his flute playing and social life. His friends included Friedrich Gotthilf Osann, Karl Witte, Christian Charles Josias von Bunsen, and William Backhouse Astor Sr.
He arrived at the newly founded University of Berlin for the winter semester of 1811–12. At the same time, his mother had just begun her literary career; she published her first book in 1810, a biography of her friend Karl Ludwig Fernow, which was a critical success. Arthur attended lectures by the prominent post-Kantian philosopher Johann Gottlieb Fichte, but quickly found many points of disagreement with his ; he also found Fichte's lectures tedious and hard to understand. He later mentioned Fichte only in critical, negative terms—seeing his philosophy as a lower-quality version of Kant's and considering it useful only because Fichte's poor arguments unintentionally highlighted some failings of Kantianism. He also attended the lectures of the famous Protestant theologian Friedrich Schleiermacher, whom he also quickly came to dislike. His notes and comments on Schleiermacher's lectures show that Schopenhauer was becoming very critical of religion and moving towards atheism. He learned by self-directed reading; besides Plato, Kant and Fichte he also read the works of Schelling, Fries, Jacobi, Bacon, Locke, and much current scientific literature. He attended philological courses by August Böckh and Friedrich August Wolf and continued his naturalistic interests with courses by Martin Heinrich Klaproth, Paul Erman, Johann Elert Bode, Ernst Gottfried Fischer, Johann Horkel, Friedrich Christian Rosenthal and Hinrich Lichtenstein (Lichtenstein was also a friend whom he met at one of his mother's parties in Weimar).
Early work.
Schopenhauer left Berlin in a rush in 1813, fearing that the city could be attacked and that he could be pressed into military service as Prussia had just joined the war against France. He returned to Weimar but left after less than a month, disgusted by the fact that his mother was now living with her supposed lover, 
(1778–1838), a civil servant twelve years younger than her; he considered the relationship an act of infidelity to his father's memory. He settled for a while in Rudolstadt, hoping that no army would pass through the small town. He spent his time in solitude, hiking in the mountains and the Thuringian Forest and writing his dissertation, "On the Fourfold Root of the Principle of Sufficient Reason".
Schopenhauer completed his dissertation at about the same time as the French army was defeated at the Battle of Leipzig. He became irritated by the arrival of soldiers in the town and accepted his mother's invitation to visit her in Weimar. She tried to convince him that her relationship with Gerstenbergk was platonic and that she had no intention of remarrying. But Schopenhauer remained suspicious and often came in conflict with Gerstenbergk because he considered him untalented, pretentious, and nationalistic. His mother had just published her second book, "Reminiscences of a Journey in the Years 1803, 1804, and 1805", a description of their family tour of Europe, which quickly became a hit. She found his dissertation incomprehensible and said it was unlikely that anyone would ever buy a copy. In a fit of temper Arthur told her that people would read his work long after the "rubbish" she wrote was totally forgotten. In fact, although they considered her novels of dubious quality, the Brockhaus publishing firm held her in high esteem because they consistently sold well. Hans Brockhaus (1888–1965) later claimed that his predecessors "saw nothing in this manuscript, but wanted to please one of our best-selling authors by publishing her son's work. We published more and more of her son Arthur's work and today nobody remembers Johanna, but her son's works are in steady demand and contribute to Brockhaus' reputation." He kept large portraits of the pair in his office in Leipzig for the edification of his new editors.
Also contrary to his mother's prediction, Schopenhauer's dissertation made an impression on Goethe, to whom he sent it as a gift. Although it is doubtful that Goethe agreed with Schopenhauer's philosophical positions, he was impressed by his intellect and extensive scientific education. Their subsequent meetings and correspondence were a great honor to a young philosopher, who was finally acknowledged by his intellectual hero. They mostly discussed Goethe's newly published (and somewhat lukewarmly received) work on color theory. Schopenhauer soon started writing his own treatise on the subject, "On Vision and Colors", which in many points differed from his teacher's. Although they remained polite towards each other, their growing theoretical disagreements—and especially Schopenhauer's extreme self-confidence and tactless criticisms—soon made Goethe become distant again and after 1816 their correspondence became less frequent. Schopenhauer later admitted that he was greatly hurt by this rejection, but he continued to praise Goethe, and considered his color theory a great introduction to his own.
Another important experience during his stay in Weimar was his acquaintance with Friedrich Majer—a historian of religion, orientalist and disciple of Herder—who introduced him to Eastern philosophy (see also Indology). Schopenhauer was immediately impressed by the "Upanishads" (he called them "the production of the highest human wisdom", and believed that they contained superhuman concepts) and the Buddha, and put them on a par with Plato and Kant. He continued his studies by reading the "Bhagavad Gita", an amateurish German journal "Asiatisches Magazin" and "Asiatick Researches" by the Asiatic Society. Schopenhauer held a profound respect for Indian philosophy; although he loved Hindu texts, he never revered a Buddhist text but regarded Buddhism as the most distinguished religion. His studies on Hindu and Buddhist texts were constrained by the lack of adequate literature, and the latter were mostly restricted to Theravada Buddhism. He also claimed that he formulated most of his ideas independently, and only later realized the similarities with Buddhism.
Schopenhauer read the Latin translation and praised the Upanishads in his main work, "The World as Will and Representation" (1819), as well as in his "Parerga and Paralipomena" (1851), and commented
In the whole world there is no study so beneficial and so elevating as that of the Upanishads. It has been the solace of my life, it will be the solace of my death.
As the relationship with his mother fell to a new low, in May 1814 he left Weimar and moved to Dresden. He continued his philosophical studies, enjoyed the cultural life, socialized with intellectuals and engaged in sexual affairs. His friends in Dresden were Johann Gottlob von Quandt, Friedrich Laun, Karl Christian Friedrich Krause and Ludwig Sigismund Ruhl, a young painter who made a romanticized portrait of him in which he improved some of Schopenhauer's unattractive physical features. His criticisms of local artists occasionally caused public quarrels when he ran into them in public. Schopenhauer's main occupation during his stay in Dresden was his seminal philosophical work, "The World as Will and Representation", which he started writing in 1814 and finished in 1818. He was recommended to the publisher Friedrich Arnold Brockhaus by Baron Ferdinand von Biedenfeld, an acquaintance of his mother. Although Brockhaus accepted his manuscript, Schopenhauer made a poor impression because of his quarrelsome and fussy attitude, as well as very poor sales of the book after it was published in December 1818.
In September 1818, while waiting for his book to be published and conveniently escaping an affair with a maid that caused an unwanted pregnancy, Schopenhauer left Dresden for a year-long vacation in Italy. He visited Venice, Bologna, Florence, Naples and Milan, travelling alone or accompanied by mostly English tourists he met. He spent the winter months in Rome, where he accidentally met his acquaintance Karl Witte and engaged in numerous quarrels with German tourists in the Caffè Greco, among them Johann Friedrich Böhmer, who also mentioned his insulting remarks and unpleasant character. He enjoyed art, architecture, and ancient ruins, attended plays and operas, and continued his philosophical contemplation and love affairs. One of his affairs supposedly became serious, and for a while he contemplated marriage to a rich Italian noblewoman—but, despite his mentioning this several times, no details are known and it may have been Schopenhauer exaggerating. He corresponded regularly with his sister Adele and became close to her as her relationship with Johanna and Gerstenbergk also deteriorated. She informed him about their financial troubles as the banking house of A. L. Muhl in Danzig—in which her mother invested their whole savings and Arthur a third of his—was near bankruptcy. Arthur offered to share his assets, but his mother refused and became further enraged by his insulting comments. The women managed to receive only thirty percent of their savings while Arthur, using his business knowledge, took a suspicious and aggressive stance towards the banker and eventually received his part in full. The affair additionally worsened the relationships among all three members of the Schopenhauer family.
He shortened his stay in Italy because of the trouble with Muhl and returned to Dresden. Disturbed by the financial risk and the lack of responses to his book he decided to take an academic position since it provided him with both income and an opportunity to promote his views. He contacted his friends at universities in Heidelberg, Göttingen and Berlin and found Berlin most attractive. He scheduled his lectures to coincide with those of the famous philosopher G. W. F. Hegel, whom Schopenhauer described as a "clumsy charlatan". He was especially appalled by Hegel's supposedly poor knowledge of natural sciences and tried to engage him in a quarrel about it already at his test lecture in March 1820. Hegel was also facing political suspicions at the time, when many progressive professors were fired, while Schopenhauer carefully mentioned in his application that he had no interest in politics. Despite their differences and the arrogant request to schedule lectures at the same time as his own, Hegel still voted to accept Schopenhauer to the university. Only five students turned up to Schopenhauer's lectures, and he dropped out of academia. A late essay, "On University Philosophy", expressed his resentment towards the work conducted in academies.
Later life.
After his trying in academia, he continued to travel extensively, visiting Leipzig, Nuremberg, Stuttgart, Schaffhausen, Vevey, Milan and spending eight months in Florence. Before he left for his three-year travel, Schopenhauer had an incident with his Berlin neighbor, 47-year-old seamstress Caroline Louise Marquet. The details of the August 1821 incident are unknown. He claimed that he had just pushed her from his entrance after she had rudely refused to leave, and that she had purposely fallen to the ground so that she could sue him. She claimed that he had attacked her so violently that she had become paralyzed on her right side and unable to work. She immediately sued him, and the process lasted until May 1827, when a court found Schopenhauer guilty and forced him to pay her an annual pension until her death in 1842.
Schopenhauer enjoyed Italy, where he studied art and socialized with Italian and English nobles. It was his last visit to the country. He left for Munich and stayed there for a year, mostly recuperating from various health issues, some of them possibly caused by venereal diseases (the treatment his doctor used suggests syphilis). He contacted publishers, offering to translate Hume into German and Kant into English, but his proposals were declined. Returning to Berlin, he began to study Spanish so he could read some of his favorite authors in their original language. He liked Pedro Calderón de la Barca, Lope de Vega, Miguel de Cervantes, and especially Baltasar Gracián. He also made failed attempts to publish his translations of their works. A few attempts to revive his lectures—again scheduled at the same time as Hegel's—also failed, as did his inquiries about relocating to other universities.
During his Berlin years, Schopenhauer occasionally mentioned his desire to marry and have a family. For a while he was unsuccessfully courting 17-year-old Flora Weiss, who was 22 years younger than himself. His unpublished writings from that time show that he was already very critical of monogamy but still not advocating polygyny—instead musing about a polyamorous relationship that he called "tetragamy". He had an on-and-off relationship with a young dancer, Caroline Richter (she also used the surname Medon after one of her ex-lovers). They met when he was 33 and she was 19 and working at the Berlin Opera. She had already had numerous lovers and a son out of wedlock, and later gave birth to another son, this time to an unnamed foreign diplomat (she soon had another pregnancy but the child was stillborn). As Schopenhauer was preparing to escape from Berlin in 1831, due to a cholera epidemic, he offered to take her with him on the condition that she left her young son behind. She refused and he went alone; in his will he left her a significant sum of money, but insisted that it should not be spent in any way on her second son.
Schopenhauer claimed that, in his last year in Berlin, he had a prophetic dream that urged him to escape from the city. As he arrived in his new home in Frankfurt, he supposedly had another supernatural experience, an apparition of his dead father and his mother, who was still alive. This experience led him to spend some time investigating paranormal phenomena and magic. He was quite critical of the available studies and claimed that they were mostly ignorant or fraudulent, but he did believe that there are authentic cases of such phenomena and tried to explain them through his metaphysics as manifestations of the will.
Upon his arrival in Frankfurt, he experienced a period of depression and declining health. He renewed his correspondence with his mother, and she seemed concerned that he might commit suicide like his father. By now Johanna and Adele were living very modestly. Johanna's writing did not bring her much income, and her popularity was waning. Their correspondence remained reserved, and Arthur seemed undisturbed by her death in 1838. His relationship with his sister grew closer and he corresponded with her until she died in 1849.
In July 1832, Schopenhauer left Frankfurt for Mannheim but returned in July 1833 to remain there for the rest of his life, except for a few short journeys. He lived alone except for a succession of pet poodles named Atman and Butz. In 1836, he published "On the Will in Nature". In 1838, he sent his essay "On the Freedom of the Will" to the contest of the Royal Norwegian Society of Sciences in 1838 and won the prize in 1839. He sent another essay, "On the Basis of Morality", to the Royal Danish Society of Sciences in 1839, but did not win the (1840) prize despite being the only contestant. The Society was appalled that several distinguished contemporary philosophers were mentioned in a very offensive manner, and claimed that the essay missed the point of the set topic and that the arguments were inadequate. Schopenhauer, who had been very confident that he would win, was enraged by this rejection. He published both essays as "The Two Basic Problems of Ethics". The first edition, published September 1840 but with an 1841 date, again failed to draw attention to his philosophy. In the preface to the second edition, in 1860, he was still pouring insults on the Royal Danish Society. Two years later, after some negotiations, he managed to convince his publisher, Brockhaus, to print the second, updated edition of "The World as Will and Representation". That book was again mostly ignored and the few reviews were mixed or negative.
Schopenhauer began to attract some followers, mostly outside academia, among practical professionals (several of them were lawyers) who pursued private philosophical studies. He jokingly referred to them as "evangelists" and "apostles". One of the most active early followers was Julius Frauenstädt, who wrote numerous articles promoting Schopenhauer's philosophy. He was also instrumental in finding another publisher after Brockhaus declined to publish "Parerga and Paralipomena", believing that it would be another failure. Though Schopenhauer later stopped corresponding with him, claiming that he did not adhere closely enough to his ideas, Frauenstädt continued to promote Schopenhauer's work. They renewed their communication in 1859 and Schopenhauer named him heir for his literary estate. Frauenstädt also became the editor of the first collected works of Schopenhauer.
In 1848, Schopenhauer witnessed violent upheaval in Frankfurt after General Hans Adolf Erdmann von Auerswald and Prince Felix Lichnowsky were murdered. He became worried for his own safety and property. Even earlier in life he had had such worries and kept a sword and loaded pistols near his bed to defend himself from thieves. He gave a friendly welcome to Austrian soldiers who wanted to shoot revolutionaries from his window and as they were leaving he gave one of the officers his opera glasses to help him monitor rebels. The rebellion passed without any loss to Schopenhauer and he later praised Alfred I, Prince of Windisch-Grätz for restoring order. He even modified his will, leaving a large part of his property to a Prussian fund that helped soldiers who became invalids while fighting rebellion in 1848 or the families of soldiers who died in battle. As Young Hegelians were advocating change and progress, Schopenhauer claimed that misery is natural for humans and that, even if some utopian society were established, people would still fight each other out of boredom, or would starve due to overpopulation.
In 1851, Schopenhauer published "Parerga and Paralipomena", which contains essays that are supplementary to his main work. It was his first successful, widely read book, partly due to the work of his disciples who wrote praising reviews. The essays that proved most popular were the ones that actually did not contain the basic philosophical ideas of his system. Many academic philosophers considered him a great stylist and cultural critic but did not take his philosophy seriously. His early critics liked to point out similarities of his ideas to those of Fichte and Schelling, or to claim that there were numerous contradictions in his philosophy. Both criticisms enraged Schopenhauer. He was becoming less interested in intellectual fights, but encouraged his disciples to do so. His private notes and correspondence show that he acknowledged some of the criticisms regarding contradictions, inconsistencies, and vagueness in his philosophy, but claimed that he was not concerned about harmony and agreement in his propositions and that some of his ideas should not be taken literally but instead as metaphors.
Academic philosophers were also starting to notice his work. In 1856, the University of Leipzig sponsored an essay contest about Schopenhauer's philosophy, which was won by Rudolf Seydel's very critical essay. Schopenhauer's friend Jules Lunteschütz made the first of his four portraits of him—which Schopenhauer did not particularly like—which was soon sold to a wealthy landowner, Carl Ferdinand Wiesike, who built a house to display it. Schopenhauer seemed flattered and amused by this, and would claim that it was his first chapel. As his fame increased, copies of paintings and photographs of him were being sold and admirers were visiting the places where he had lived and written his works. People visited Frankfurt's "Englischer Hof" to observe him dining. Admirers gave him gifts and asked for autographs. He complained that he still felt isolated due to his not very social nature and the fact that many of his good friends had already died from old age.
He remained healthy in his own old age, which he attributed to regular walks no matter the weather and always getting enough sleep. He had a great appetite and could read without glasses, but his hearing had been declining since his youth and he developed problems with rheumatism. He remained active and lucid, continued his reading, writing and correspondence until his death. The numerous notes that he made during these years, amongst others on aging, were published posthumously under the title "Senilia". In the spring of 1860 his health began to decline, and he experienced shortness of breath and heart palpitations; in September he suffered inflammation of the lungs and, although he was starting to recover, he remained very weak. The last friend to visit him was Wilhelm Gwinner; according to him, Schopenhauer was concerned that he would not be able to finish his planned additions to "Parerga and Paralipomena" but was at peace with dying. He died of pulmonary-respiratory failure on 21 September 1860 while sitting at home on his couch. He died at the age of 72 and had a funeral conducted by a Lutheran minister.
Philosophy.
Theory of perception.
In November 1813 Goethe invited Schopenhauer to help him on his Theory of Colours. Although Schopenhauer considered colour theory a minor matter, he accepted the invitation out of admiration for Goethe. Nevertheless, these investigations led him to his most important discovery in epistemology: finding a demonstration for the "a priori" nature of causality.
Kant openly admitted that it was Hume's skeptical assault on causality that motivated the critical investigations in "Critique of Pure Reason" and gave an elaborate proof to show that causality is "a priori". After G. E. Schulze had made it plausible that Kant had not disproven Hume's skepticism, it was up to those loyal to Kant's project to prove this important matter.
The difference between the approaches of Kant and Schopenhauer was this: Kant simply declared that the empirical content of perception is "given" to us from outside, an expression with which Schopenhauer often expressed his dissatisfaction. He, on the other hand, was occupied with the questions: how do we get this empirical content of perception; how is it possible to comprehend subjective sensations "limited to my skin" as the objective perception of things that lie "outside" of me?
Causality is therefore not an empirical concept drawn from objective perceptions, as Hume had maintained; instead, as Kant had said, objective perception presupposes knowledge of causality.
By this intellectual operation, comprehending every effect in our sensory organs as having an external cause, the external world arises. With vision, finding the cause is essentially simplified due to light acting in straight lines. We are seldom conscious of the process that interprets the double sensation in both eyes as coming from one object, that inverts the impressions on the retinas, and that uses the change in the apparent position of an object relative to more distant objects provided by binocular vision to perceive depth and distance.
Schopenhauer stresses the importance of the intellectual nature of perception; the senses furnish the raw material by which the intellect produces the world as representation. He set out his theory of perception for the first time in "On Vision and Colors", and, in the subsequent editions of "Fourfold Root", an extensive exposition is given in § 21.
The world as representation.
Schopenhauer saw his philosophy as an extension of Kant's, and used the results of Kantian epistemological investigation (transcendental idealism) as starting point for his own. Kant had argued that the empirical world is merely a complex of appearances whose existence and connection occur only in our mental representations. Schopenhauer did not deny that the external world existed empirically but followed Kant in claiming that our knowledge and experience of the world is always indirect. Schopenhauer reiterates this in the first sentence of his main work: "The world is my representation ("Die Welt ist meine Vorstellung")". Everything that there is for cognition (the entire world) exists simply as an object in relation to a subject—a 'representation' to a subject. Everything that belongs to the world is, therefore, 'subject-dependent'. In Book One of "The World as Will and Representation", Schopenhauer considers the world from this angle—that is, insofar as it is representation.
Kant had previously argued that we perceive reality as something spatial and temporal not because reality is inherently spatial and temporal, but because that is how our minds operate in perceiving an object. Therefore, understanding objects in space and time represents our 'contribution' to an experience. For Schopenhauer, Kant's 'greatest service' lay in the 'differentiation between phenomena and the thing-in-itself (noumena), based on the proof that between everything and us there is always a perceiving mind.' In other words, Kant's primary achievement is to demonstrate that instead of being a blank slate where reality merely reveals its character, the mind, with sensory support, actively participates in constructing reality. Thus, Schopenhauer believed that Kant had shown that the everyday world of experience, and indeed the entire material world related to space and time, is merely 'appearance' or 'phenomena,' entirely distinct from the thing-in-itself.'
The world as will.
In Book Two of "The World as Will and Representation", Schopenhauer considers what the world is beyond the aspect of it that appears to us—that is, the aspect of the world beyond representation, the world considered "in-itself" or "noumena", its inner essence. The very being in-itself of all things, Schopenhauer argues, is will ("Wille"). The empirical world that appears to us as representation has plurality and is ordered in a spatio-temporal framework. The world as thing in-itself must exist outside the subjective forms of space and time. Although the world manifests itself to our experience as a multiplicity of objects (the "objectivation" of the will), each element of this multiplicity has the same blind essence striving towards existence and life. Human rationality is merely a secondary phenomenon that does not distinguish humanity from the rest of nature at the fundamental, essential level. The advanced cognitive abilities of human beings, Schopenhauer argues, serve the ends of willing—an illogical, directionless, ceaseless striving that condemns the human individual to a life of suffering unredeemed by any final purpose. Schopenhauer's philosophy of the will as the essential reality behind the world as representation is often called metaphysical voluntarism.
For Schopenhauer, understanding the world as will leads to ethical concerns (see the ethics section below for further detail), which he explores in the Fourth Book of "The World as Will and Representation" and again in his two prize essays on ethics, "On the Freedom of the Will" and "On the Basis of Morality". No individual human actions are free, Schopenhauer argues, because they are events in the world of appearance and thus are subject to the principle of sufficient reason: a person's actions are a necessary consequence of motives and the given character of the individual human. Necessity extends to the actions of human beings just as it does to every other appearance, and thus we cannot speak of freedom of individual willing. Albert Einstein quoted the Schopenhauerian idea that "a man can "do" as he will, but not "will" as he will." Yet the will as thing in-itself is free, as it exists beyond the realm of representation and thus is not constrained by any of the forms of necessity that are part of the principle of sufficient reason.
According to Schopenhauer, salvation from our miserable existence can come through the will's being "tranquillized" by the metaphysical insight that reveals individuality to be merely an illusion. The saint or 'great soul' intuitively "recognizes the whole, comprehends its essence, and finds that it is constantly passing away, caught up in vain strivings, inner conflict, and perpetual suffering". The negation of the will, in other words, stems from the insight that the world in-itself (free from the forms of space and time) is one. Ascetic practices, Schopenhauer remarks, are used to aid the will's "self-abolition", which brings about a blissful, redemptive "will-less" state of emptiness that is free from striving or suffering.
Art and aesthetics.
For Schopenhauer, human "willing"—desiring, craving, etc.—is at the root of suffering. A temporary way to escape this pain is through aesthetic contemplation. Here one moves away from ordinary cognizance of individual things to cognizance of eternal Platonic "Ideas"—in other words, cognizance that is free from the service of will. In aesthetic contemplation, one no longer perceives an object of perception as something from which one is separated; rather "it is as if the object alone existed without anyone perceiving it, and one can thus no longer separate the perceiver from the perception, but the two have become one, the entirety of consciousness entirely filled and occupied by a single perceptual image". Subject and object are no longer distinguishable, and the "Idea" comes to the fore.
From this aesthetic immersion, one is no longer an individual who suffers as a result of servitude to one's individual will but, rather, becomes a "pure, will-less, painless, timeless, subject of cognition". The pure, will-less subject of cognition is cognizant only of Ideas, not individual things: this is a kind of cognition that is unconcerned with relations between objects according to the Principle of Sufficient Reason (time, space, cause and effect) and instead involves complete absorption in the object.
Art is the practical consequence of this brief aesthetic contemplation, since it attempts to depict the essence/pure Ideas of the world. Music, for Schopenhauer, is the purest form of art because it is the one that depicts the will itself without it appearing as subject to the Principle of Sufficient Reason, therefore as an individual object. According to Daniel Albright, "Schopenhauer thought that music was the only art that did not merely copy ideas, but actually embodied the will itself". He deemed music a timeless, universal language comprehended everywhere, that can imbue global enthusiasm, if in possession of a significant melody.
Mathematics.
Schopenhauer's realist views on mathematics are evident in his criticism of contemporaneous attempts to prove the parallel postulate in Euclidean geometry. Writing shortly before the discovery of hyperbolic geometry demonstrated the logical independence of the axiom—and long before the general theory of relativity revealed that it does not necessarily express a property of physical space—Schopenhauer criticized mathematicians for trying to use indirect concepts to prove what he held was directly evident from intuitive perception.
Throughout his writings, Schopenhauer criticized the logical derivation of philosophies and mathematics from mere concepts, instead of from intuitive perceptions.
Although Schopenhauer could see no justification for trying to prove Euclid's parallel postulate, he did see a reason for examining another of Euclid's axioms.
This follows Kant's reasoning.
Ethics.
Schopenhauer asserts that the task of ethics is not to prescribe moral actions that ought to be done, but to investigate moral actions. As such, he states that philosophy is always theoretical: its task to explain what is given.
According to Kant's transcendental idealism, space and time are forms of our sensibility in which phenomena appear in multiplicity. Reality in itself is free from multiplicity, not in the sense that an object is one, but that it is outside the "possibility" of multiplicity. Two individuals, though they appear distinct, are in-themselves not distinct.
Appearances are entirely subordinated to the principle of sufficient reason. The egoistic individual who focuses his aims on his own interests has to deal with empirical laws as well as he can.
What is relevant for ethics are individuals who can act against their own self-interest. If we take a man who suffers when he sees his fellow men living in poverty and consequently uses a significant part of his income to support "their" needs instead of his "own" pleasures, then the simplest way to describe this is that he makes "less distinction between himself" and others than is usually made.
Regarding how things "appear" to us, the egoist asserts a gap between two individuals, but the altruist experiences the sufferings of others as his own. In the same way a compassionate man cannot hurt animals, though they appear as distinct from himself.
What motivates the altruist is compassion. The suffering of others is for him not a cold matter to which he is indifferent, but he feels connectiveness to all beings. Compassion is thus the basis of morality.
Eternal justice.
Schopenhauer calls the principle through which multiplicity appears the "principium individuationis". When we behold nature we see that it is a cruel battle for existence. Individual manifestations of the will can maintain themselves only at the expense of others—the will, as the only thing that exists, has no other option but to devour itself to experience pleasure. This is a fundamental characteristic of the will, and cannot be circumvented.
Unlike temporal or human justice, which requires time to repay an evil deed and "has its seat in the state, as requiting and punishing", eternal justice "rules not the state but the world, is not dependent upon human institutions, is not subject to chance and deception, is not uncertain, wavering, and erring, but infallible, fixed, and sure". Eternal justice is not retributive, because retribution requires time. There are no delays or reprieves. Instead, punishment is tied to the offence, "to the point where the two become one. ... Tormenter and tormented are one. The [Tormenter] errs in that he believes he is not a partaker in the suffering; the [tormented], in that he believes he is not a partaker in the guilt."
Suffering is the moral outcome of our attachment to pleasure. Schopenhauer deemed that this truth was expressed by the Christian dogma of original sin and, in Eastern religions, by the dogma of rebirth.
Quietism.
He who sees through the "principium individuationis" and comprehends suffering "in general" as his own will see suffering everywhere and, instead of fighting for the happiness of his individual manifestation, will abhor life itself since he knows that it is inseparably connected with suffering. For him, a happy individual life in a world of suffering is like a beggar who dreams one night that he is a king.
Those who have experienced this intuitive knowledge cannot affirm life, but exhibit asceticism and quietism, meaning that they are no longer sensitive to motives, are not concerned about their individual welfare, and accept without resistance the evil that others inflict on them. They welcome poverty and neither seek nor flee death. Schopenhauer referred to asceticism as the denial of the will to live.
Human life is a ceaseless struggle for satisfaction and, instead of continuing their struggle, ascetics break it. It does not matter if these ascetics adhere to the dogmata of Christianity or to Dharmic religions, since their way of living is the result of intuitive knowledge.
Psychology.
Philosophers have not traditionally been impressed by the necessity of sex, but Schopenhauer addressed sex and related concepts forthrightly:
He named a force within man that he felt took invariable precedence over reason: the Will to Live or Will to Life ("Wille zum Leben"), defined as an inherent drive within human beings, and all creatures, to stay alive; a force that inveigles us into reproducing.
Schopenhauer refused to conceive of love as either trifling or accidental, but rather understood it as an immensely powerful force that lay unseen within man's psyche, guaranteeing the quality of the human race:
It has often been argued that Schopenhauer's thoughts on sexuality foreshadowed the theory of evolution, a claim met with satisfaction by Darwin as he included a quotation from Schopenhauer in his "Descent of Man". This has also been noted about Freud's concepts of the libido and the unconscious mind, and evolutionary psychology in general.
Political and social thought.
Politics.
Schopenhauer's politics were an echo of his system of ethics, which he elucidated in detail in his "Die beiden Grundprobleme der Ethik" (the two essays "On the Freedom of the Will" and "On the Basis of Morality").
In occasional political comments in his "Parerga and Paralipomena" and "Manuscript Remains", Schopenhauer described himself as a proponent of limited government. Schopenhauer shared the view of Thomas Hobbes on the necessity of the state and state action to check the innate destructive tendencies of our species. He also defended the independence of the legislative, judicial and executive branches of power, and a monarch as an impartial element able to practise justice (in a practical and everyday sense, not a cosmological one).
He declared that monarchy is "natural to man in almost the same way as it is to bees and ants, to cranes in flight, to wandering elephants, to wolves in a pack in search of prey, and to other animals". Intellect in monarchies, he writes, always has "much better chances against stupidity, its implacable and ever-present foe, than it has in republics; but this is a great advantage." On the other hand, Schopenhauer disparaged republicanism as being "as unnatural to man as it is unfavorable to higher intellectual life and thus to the arts and sciences".
By his own admission, Schopenhauer did not give much thought to politics, and several times he wrote proudly of how little attention he paid "to political affairs of [his] day". In a life that spanned several revolutions in French and German government, and a few continent-shaking wars, he maintained his position of "minding not the times but the eternities". He wrote many disparaging remarks about Germany and the Germans. A typical example is: "For a German it is even good to have somewhat lengthy words in his mouth, for he thinks slowly, and they give him time to reflect."
Punishment.
The State, Schopenhauer claimed, punishes criminals to prevent future crimes. It places "beside every possible motive for committing a wrong a more powerful motive for leaving it undone, in the inescapable punishment. Accordingly, the criminal code is as complete a register as possible of counter-motives to all criminal actions that can possibly be imagined ..." He claimed that this doctrine was not original to him but had appeared in the writings of Plato, Seneca, Hobbes, Pufendorf, and Anselm Feuerbach.
Races and religions.
Schopenhauer attributed civilizational primacy to the northern "white races" due to their sensitivity and creativity (except for the ancient Egyptians and Hindus, whom he saw as equal):
The highest civilization and culture, apart from the ancient Hindus and Egyptians, are found exclusively among the white races; and even with many dark peoples, the ruling caste or race is fairer in colour than the rest and has, therefore, evidently immigrated, for example, the Brahmans, the Incas, and the rulers of the South Sea Islands. All this is due to the fact that necessity is the mother of invention because those tribes that emigrated early to the north, and there gradually became white, had to develop all their intellectual powers and invent and perfect all the arts in their struggle with need, want and misery, which in their many forms were brought about by the climate. This they had to do in order to make up for the parsimony of nature and out of it all came their high civilization.
Schopenhauer was fervently opposed to slavery. Speaking of the treatment of slaves in the slave-holding states of the United States, he condemned "those devils in human form, those bigoted, church-going, strict sabbath-observing scoundrels, especially the Anglican parsons among them" for how they "treat their innocent black brothers who through violence and injustice have fallen into their devil's claws". The slave-holding states of North America, Schopenhauer writes, are a "disgrace to the whole of humanity".
Schopenhauer also maintained a marked metaphysical and political anti-Judaism. He argued that Christianity constituted a revolt against what he styled the materialistic basis of Judaism, exhibiting an Indian-influenced ethics reflecting the Aryan-Vedic theme of spiritual self-conquest. He saw this as opposed to the ignorant drive toward earthly utopianism and superficiality of a worldly "Jewish" spirit:
[Judaism] is, therefore, the crudest and poorest of all religions and consists merely in an absurd and revolting theism. It amounts to this that the "κύριος" ['Lord'], who has created the world, desires to be worshipped and adored; and so above all he is jealous, is envious of his colleagues, of all the other gods; if sacrifices are made to them he is furious and his Jews have a bad time ... It is most deplorable that this religion has become the basis of the prevailing religion of Europe; for it is a religion without any metaphysical tendency. While all other religions endeavor to explain to the people by symbols the metaphysical significance of life, the religion of the Jews is entirely immanent and furnishes nothing but a mere war-cry in the struggle with other nations.
Women.
In his 1851 essay "On Women", Schopenhauer expressed opposition to what he called "Teutonico-Christian stupidity" of "reflexive, unexamined reverence for the female ("abgeschmackten Weiberveneration")". He wrote: "Women are directly fitted for acting as the nurses and teachers of our early childhood by the fact that they are themselves childish, frivolous and short-sighted." He opined that women are deficient in artistic faculties and sense of justice, and expressed his opposition to monogamy. He claimed that "woman is by nature meant to obey". The essay does give some compliments: "women are decidedly more sober in their judgment than [men] are", and are more sympathetic to the suffering of others.
Schopenhauer's writings influenced many, from Friedrich Nietzsche to nineteenth-century feminists. His biological analysis of the difference between the sexes, and their separate roles in the struggle for survival and reproduction, anticipates some of the claims that were later ventured by sociobiologists and evolutionary psychologists.
When the elderly Schopenhauer sat for a sculpture portrait by the Prussian sculptor Elisabet Ney in 1859, he was much impressed by the young woman's wit and independence, as well as by her skill as a visual artist. After his time with Ney, he told Richard Wagner's friend Malwida von Meysenbug: "I have not yet spoken my last word about women. I believe that if a woman succeeds in withdrawing from the mass, or rather raising herself above the mass, she grows ceaselessly and more than a man."
Pederasty.
In the third, expanded edition of "The World as Will and Representation" (1859), Schopenhauer added an appendix to his chapter on the "Metaphysics of Sexual Love". He wrote that pederasty has the benefit of preventing ill-begotten children. Concerning this, he stated that "the vice we are considering appears to work directly against the aims and ends of nature, and that in a matter that is all important and of the greatest concern to her it must in fact serve these very aims, although only indirectly, as a means for preventing greater evils."
Schopenhauer ends the appendix with the statement that "by expounding these paradoxical ideas, I wanted to grant to the professors of philosophy a small favour. I have done so by giving them the opportunity of slandering me by saying that I defend and commend pederasty."
Heredity and eugenics.
Schopenhauer viewed personality and intellect as inherited. He quotes Horace's saying, "From the brave and good are the brave descended" ("Odes", iv, 4, 29) and Shakespeare's line from "Cymbeline", "Cowards father cowards, and base things sire base" (IV, 2) to reinforce his hereditarian argument.
Mechanistically, Schopenhauer believed that a person inherits his intellect through his mother, and personal character through the father. This belief in heritability of traits informed Schopenhauer's view of love—placing it at the highest level of importance. For Schopenhauer the "final aim of all love intrigues, be they comic or tragic, is really of more importance than all other ends in human life. What it all turns upon is nothing less than the composition of the next generation. ... It is not the weal or woe of any one individual, but that of the human race to come, which is here at stake." This view of the importance for the species of whom we choose to love was reflected in his views on eugenics or good breeding. Here Schopenhauer wrote:
With our knowledge of the complete unalterability both of character and of mental faculties, we are led to the view that a real and thorough improvement of the human race might be reached not so much from outside as from within, not so much by theory and instruction as rather by the path of generation. Plato had something of the kind in mind when, in the fifth book of his "Republic", he explained his plan for increasing and improving his warrior caste. If we could castrate all scoundrels and stick all stupid geese in a convent, and give men of noble character a whole harem, and procure men, and indeed thorough men, for all girls of intellect and understanding, then a generation would soon arise which would produce a better age than that of Pericles.
In another context, Schopenhauer reiterated his eugenic thesis: "If you want Utopian plans, I would say: the only solution to the problem is the despotism of the wise and noble members of a genuine aristocracy, a genuine nobility, achieved by mating the most magnanimous men with the cleverest and most gifted women. This proposal constitutes my Utopia and my Platonic Republic." Analysts (e.g., Keith Ansell-Pearson) have suggested that Schopenhauer's anti-egalitarianist sentiment and his support for eugenics influenced the neo-aristocratic philosophy of Friedrich Nietzsche, who initially considered Schopenhauer his mentor.
Animal rights.
As a consequence of his monistic philosophy, Schopenhauer was very concerned about animal welfare and rights. For him, all individual animals, including humans, are essentially phenomenal manifestations of the one underlying Will. For him the word "will" designates force, power, impulse, energy, and desire; it is the closest word we have that can signify both the essence of all external things and our own direct, inner experience. Since every living thing possesses will, humans and animals are fundamentally the same and can recognize themselves in each other. For this reason, he claimed that a good person would have sympathy for animals, who are our fellow sufferers.
In 1841, he praised the establishment in London of the Society for the Prevention of Cruelty to Animals, and in Philadelphia of the Animals' Friends Society. Schopenhauer went so far as to protest using the pronoun "it" in reference to animals because that led to treatment of them as though they were inanimate things. To reinforce his points, Schopenhauer referred to anecdotal reports of the look in the eyes of a monkey who had been shot and also the grief of a baby elephant whose mother had been killed by a hunter.
Schopenhauer was very attached to his succession of pet poodles. He criticized Spinoza's belief that animals are a mere means for the satisfaction of humans. Tim Madigan wrote that despite all of his bombast, Schopenhauer was a sympathetic character who had concerns for the suffering of animals.
Intellectual interests and affinities.
Indology.
Schopenhauer read the Latin translation of the ancient Hindu texts, the "Upanishads", translated by French writer Anquetil du Perron from the Persian translation of Prince Dara Shukoh entitled "Sirre-Akbar" ("The Great Secret"). He was so impressed by its philosophy that he called it "the production of the highest human wisdom", and believed it contained superhuman concepts. Schopenhauer considered India as "the land of the most ancient and most pristine wisdom, the place from which Europeans could trace their descent and the tradition by which they had been influenced in so many decisive ways", and regarded the "Upanishads" as "the most profitable and elevating reading which [...] is possible in the world. It has been the solace of my life, and will be the solace of my death."
Schopenhauer was first introduced to Anquetil du Perron's translation by Friedrich Majer in 1814. They met during the winter of 1813–1814 in Weimar at the home of Schopenhauer's mother, according to the biographer Safranski. Majer was a follower of Herder, and an early Indologist. Schopenhauer did not begin serious study of the Indic texts until the summer of 1814. Safranski maintains that, between 1815 and 1817, Schopenhauer had another important cross-pollination with Indian thought in Dresden. This was through his neighbor of two years, Karl Christian Friedrich Krause. Krause was then a minor and rather unorthodox philosopher who attempted to mix his own ideas with ancient Indian wisdom. Krause had also mastered Sanskrit, unlike Schopenhauer, and they developed a professional relationship. It was from Krause that Schopenhauer learned meditation and received the closest thing to expert advice concerning Indian thought.
For Schopenhauer, will had ontological primacy over the intellect; desire is prior to thought. Schopenhauer felt this was similar to notions of puruṣārtha or goals of life in Vedānta Hinduism.
In Schopenhauer's philosophy, denial of the will is attained by:
The book "Oupnekhat" (Upanishad) always lay open on his table, and he invariably studied it before going to bed. He called the opening up of Sanskrit literature "the greatest gift of our century", and predicted that the philosophy and knowledge of the Upanishads would become the cherished faith of the West. Most noticeable, in the case of Schopenhauer's work, was the significance of the "Chandogya Upanishad", whose Mahāvākya, Tat Tvam Asi, is mentioned throughout "The World as Will and Representation".
Buddhism.
Schopenhauer noted a correspondence between his doctrines and the Four Noble Truths of Buddhism. Similarities centered on the principles that life involves suffering, that suffering is caused by desire (taṇhā), and that the extinction of desire leads to liberation. Thus three of the four "truths of the Buddha" correspond to Schopenhauer's doctrine of the will. In Buddhism, while greed and lust are always unskillful, desire is ethically variable – it can be skillful, unskillful, or neutral.
Buddhist nirvāṇa is not equivalent to the condition that Schopenhauer described as denial of the will. Nirvāṇa is not the extinguishing of the "person" as some Western scholars have thought, but only the "extinguishing" (the literal meaning of nirvana) of the flames of greed, hatred, and delusion that assail a person's character. Schopenhauer made the following statement in his discussion of religions:
If I wished to take the results of my philosophy as the standard of truth, I should have to concede to Buddhism pre-eminence over the others. In any case, it must be a pleasure to me to see my doctrine in such close agreement with a religion that the majority of men on earth hold as their own, for this numbers far more followers than any other. And this agreement must be yet the more pleasing to me, inasmuch as "in my philosophizing I have certainly not been under its influence" [emphasis added]. For up till 1818, when my work appeared, there was to be found in Europe only a very few accounts of Buddhism.
Buddhist philosopher Keiji Nishitani sought to distance Buddhism from Schopenhauer. While Schopenhauer's philosophy may sound rather mystical in such a summary, his methodology was resolutely empirical, rather than speculative or transcendental:
Philosophy ... is a science, and as such has no articles of faith; accordingly, in it nothing can be assumed as existing except what is either positively given empirically, or demonstrated through indubitable conclusions.
Also note:
This actual world of what is knowable, in which we are and which is in us, remains both the material and the limit of our consideration.
The argument that Buddhism affected Schopenhauer's philosophy more than any other Dharmic faith loses credence since he did not begin a serious study of Buddhism until after the publication of "The World as Will and Representation" in 1818. Scholars have started to revise earlier views about Schopenhauer's discovery of Buddhism. Proof of early interest and influence appears in Schopenhauer's 1815–16 notes (transcribed and translated by Urs App) about Buddhism. They are included in a recent case study that traces Schopenhauer's interest in Buddhism and documents its influence. Other scholarly work questions how similar Schopenhauer's philosophy actually is to Buddhism.
Magic and occultism.
Some traditions in Western esotericism and parapsychology interested Schopenhauer and influenced his philosophical theories. He praised animal magnetism as evidence for the reality of magic in his "On the Will in Nature", and went so far as to accept the division of magic into left-hand and right-hand magic, although he doubted the existence of demons.
Schopenhauer grounded magic in the Will and claimed all forms of magical transformation depended on the human Will, not on ritual. This theory notably parallels Aleister Crowley's system of magic and its emphasis on human will. Given the importance of the Will to Schopenhauer's overarching system, this amounts to "suggesting his whole philosophical system had magical powers." Schopenhauer rejected the theory of disenchantment and claimed philosophy should synthesize itself with magic, which he believed amount to "practical metaphysics".
Neoplatonism, including the traditions of Plotinus and to a lesser extent Marsilio Ficino, has also been cited as an influence on Schopenhauer.
Interests.
Schopenhauer had a wide range of interests, from science and opera to occultism and literature.
In his student years, Schopenhauer went more often to lectures in the sciences than philosophy. He kept a strong interest as his personal library contained near to 200 books of scientific literature at his death, and his works refer to scientific titles not found in the library.
Many evenings were spent in the theatre, opera and ballet; Schopenhauer especially liked the operas of Mozart, Rossini and Bellini. Schopenhauer considered music the highest art, and played the flute during his whole life.
As a polyglot, he knew German, Italian, Spanish, French, English, Latin and ancient Greek, and was an avid reader of poetry and literature. He particularly revered Goethe, Petrarch, Calderón and Shakespeare.
If Goethe had not been sent into the world simultaneously with Kant in order to counterbalance him, so to speak, in the spirit of the age, the latter would have been haunted like a nightmare many an aspiring mind and would have oppressed it with great affliction. But now the two have an infinitely wholesome effect from opposite directions and will probably raise the German spirit to a height surpassing even that of antiquity.
In philosophy, his most important influences were, according to himself, Kant, Plato and the Upanishads. Concerning the Upanishads and Vedas, he writes in "The World as Will and Representation":
If the reader has also received the benefit of the Vedas, the access to which by means of the Upanishads is in my eyes the greatest privilege which this still young century (1818) may claim before all previous centuries, if then the reader, I say, has received his initiation in primeval Indian wisdom, and received it with an open heart, he will be prepared in the very best way for hearing what I have to tell him. It will not sound to him strange, as to many others, much less disagreeable; for I might, if it did not sound conceited, contend that every one of the detached statements which constitute the Upanishads, may be deduced as a necessary result from the fundamental thoughts which I have to enunciate, though those deductions themselves are by no means to be found there.
Thoughts on other philosophers.
Giordano Bruno and Spinoza.
Schopenhauer saw Bruno and Spinoza as philosophers not bound to their age or nation. "Both were fulfilled by the thought, that as manifold the appearances of the world may be, it is still "one" being, that appears in all of them. ... Consequently, there is no place for God as creator of the world in their philosophy, but God is the world itself."
Schopenhauer expressed regret that Spinoza stuck, for the presentation of his philosophy, with the concepts of scholasticism and Cartesian philosophy, and tried to use geometrical proofs that do not hold because of vague and overly broad definitions. Bruno on the other hand, who knew much about nature and ancient literature, presented his ideas with Italian vividness, and is amongst philosophers the only one who comes near Plato's poetic and dramatic power of exposition.
Schopenhauer noted that their philosophies do not provide any ethics, and it is therefore very remarkable that Spinoza called his main work "Ethics". In fact, it could be considered complete from the standpoint of life-affirmation, if one completely ignores morality and self-denial. It is yet even more remarkable that Schopenhauer mentions Spinoza as an example of the denial of the will, if one uses the French biography by Jean Maximilien Lucas as the key to "Tractatus de Intellectus Emendatione".
Immanuel Kant.
The importance of Kant for Schopenhauer, in philosophy as well as on a personal level, cannot be overstated. Kant's philosophy was the foundation of Schopenhauer's, and he had high praise for the Transcendental Aesthetic section of Kant's "Critique of Pure Reason". Schopenhauer maintained that Kant stands in the same relation to philosophers such as Berkeley and Plato, as Copernicus to Hicetas, Philolaus, and Aristarchus: Kant succeeded in demonstrating what previous philosophers merely asserted.
Schopenhauer writes about Kant's influence on his work in the preface to the second edition of "The World as Will and Representation":
In his study room, one bust was of Buddha, the other was of Kant. The bond which Schopenhauer felt with the philosopher of Königsberg is demonstrated in an unfinished poem he dedicated to Kant (included in volume 2 of the "Parerga"):
Schopenhauer dedicated one fifth of his main work, "The World as Will and Representation", to a detailed criticism of the Kantian philosophy.
Schopenhauer praised Kant for his distinction between appearance and the thing-in-itself, whereas the general consensus in German idealism was that this was the weakest spot of Kant's theory, since, according to Kant, causality can find application on objects of experience only, and consequently, things-in-themselves cannot be the cause of appearances. The inadmissibility of this reasoning was also acknowledged by Schopenhauer. He insisted that this was a true conclusion, drawn from false premises.
Post-Kantian school.
The leading figures of post-Kantian philosophy—Johann Gottlieb Fichte, F. W. J. Schelling and G. W. F. Hegel—were not respected by Schopenhauer. He argued that they were not philosophers at all, for they lacked "the first requirement of a philosopher, namely a seriousness and honesty of inquiry." Rather, they were merely sophists who, excelling in the art of beguiling the public, pursued their own selfish interests (such as professional advancement within the university system). Diatribes against the alleged vacuity, dishonesty, pomposity, and self-interest of these contemporaries are to be found throughout Schopenhauer's published writings. The following passage is an example:
Schopenhauer deemed Schelling the most talented of the three and wrote that he would recommend his "elucidatory paraphrase of the highly important doctrine of Kant" concerning the intelligible character, if he had been honest enough to admit he was parroting Kant, instead of hiding this relation in a cunning manner.
Schopenhauer reserved his most unqualified damning condemnation for Hegel, whom he considered less worthy than Fichte or Schelling. Whereas Fichte was merely a windbag ("Windbeutel"), Hegel was a "commonplace, inane, loathsome, repulsive, and ignorant charlatan." The philosophers Karl Popper and Mario Bunge agreed with this distinction. Hegel, Schopenhauer wrote in the preface to his "Two Fundamental Problems of Ethics", not only "performed no service to philosophy, but he has had a detrimental influence on philosophy, and thereby on German literature in general, really a downright stupefying, or we could even say a pestilential influence, which it is therefore the duty of everyone capable of thinking for himself and judging for himself to counteract in the most express terms at every opportunity."
Influence and legacy.
Schopenhauer remained the most influential German philosopher until the First World War. His philosophy was a starting point for a new generation of philosophers including Julius Bahnsen, Paul Deussen, Lazar von Hellenbach, Karl Robert Eduard von Hartmann, Ernst Otto Lindner, Philipp Mainländer, Friedrich Nietzsche, Olga Plümacher and Agnes Taubert. His legacy shaped the intellectual debate, and forced movements that were utterly opposed to him, neo-Kantianism and positivism, to address issues they would otherwise have completely ignored, and in doing so he changed them markedly. The French writer Maupassant commented that "to-day even those who execrate him seem to carry in their own souls particles of his thought". Other philosophers of the 19th century who cited his influence include Hans Vaihinger, Volkelt, Solovyov and Weininger.
Schopenhauer was well read by physicists, most notably Einstein, Schrödinger, Wolfgang Pauli, and Majorana. Einstein described Schopenhauer's thoughts as a "continual consolation" and called him a genius. In his Berlin study three figures hung on the wall: Faraday, Maxwell, Schopenhauer. Konrad Wachsmann recalled: "He often sat with one of the well-worn Schopenhauer volumes, and as he sat there, he seemed so pleased, as if he were engaged with a serene and cheerful work."
When Erwin Schrödinger discovered Schopenhauer ("the greatest savant of the West") he considered switching his study of physics to philosophy. He maintained the idealistic views during the rest of his life. Wolfgang Pauli accepted the main tenet of Schopenhauer's metaphysics, that the thing-in-itself is will.
But most of all Schopenhauer is famous for his influence on artists. Richard Wagner became one of the earliest and most famous adherents of the Schopenhauerian philosophy. The admiration was not mutual, and Schopenhauer proclaimed: "I remain faithful to Rossini and Mozart!" So he has been nicknamed "the artist's philosopher". See also Influence of Schopenhauer on "Tristan und Isolde".
Under the influence of Schopenhauer, Leo Tolstoy became convinced that the truth of all religions lies in self-renunciation. When he read Schopenhauer's philosophy, Tolstoy exclaimed "at present I am convinced that Schopenhauer is the greatest genius among men. ... It is the whole world in an incomparably beautiful and clear reflection." He said that what he has written in "War and Peace" is also said by Schopenhauer in "The World as Will and Representation".
Jorge Luis Borges remarked that the reason he had never attempted to write a systematic account of his world view, despite his penchant for philosophy and metaphysics in particular, was because Schopenhauer had already written it for him.
Other figures in literature who were strongly influenced by Schopenhauer were Thomas Mann, Thomas Hardy, Afanasy Fet, J.-K. Huysmans and George Santayana. In Herman Melville's final years, while he wrote "Billy Budd", he read Schopenhauer's essays and marked them heavily. Scholar Brian Yothers notes that Melville "marked numerous misanthropic and even suicidal remarks, suggesting an attraction to the most extreme sorts of solitude, but he also made note of Schopenhauer's reflection on the moral ambiguities of genius." Schopenhauer's attraction to and discussions of both Eastern and Western religions in conjunction with each other made an impression on Melville in his final years.
Sergei Prokofiev, although initially reluctant to engage with works noted for their pessimism, became fascinated with Schopenhauer after reading "Aphorisms on the Wisdom of Life" in "Parerga and Paralipomena". "With his truths Schopenhauer gave me a spiritual world and an awareness of happiness."
Friedrich Nietzsche owed the awakening of his philosophical interest to reading "The World as Will and Representation" and admitted that he was one of the few philosophers that he respected, dedicating to him his essay "Schopenhauer als Erzieher", one of his "Untimely Meditations".
Early in his career, Ludwig Wittgenstein adopted Schopenhauer's epistemological idealism, and some traits of Schopenhauer's influence (particularly Schopenhauerian transcendentalism) can be observed in the "Tractatus Logico-Philosophicus". Later on, Wittgenstein rejected epistemological transcendental idealism for Gottlob Frege's conceptual realism. In later years, Wittgenstein became highly dismissive of Schopenhauer, describing him as an ultimately shallow thinker. His friend Bertrand Russell had a low opinion on the philosopher, and even came to attack him in his "History of Western Philosophy" for hypocritically praising asceticism yet not acting upon it.
Opposite to Russell on the foundations of mathematics, the Dutch mathematician L. E. J. Brouwer incorporated Kant's and Schopenhauer's ideas in the philosophical school of intuitionism, where mathematics is considered as a purely mental activity instead of an analytic activity wherein objective properties of reality are revealed. Brouwer was also influenced by Schopenhauer's metaphysics, and wrote an essay on mysticism.
Schopenhauer's philosophy has made its way into a novel, "The Schopenhauer Cure", by American existential psychiatrist and emeritus professor of psychiatry Irvin Yalom.
Schopenhauer's philosophy, and the discussions on philosophical pessimism it has engendered, has been the focus of contemporary thinkers such as David Benatar, Thomas Ligotti, and Eugene Thacker. Their work also served as an inspiration for the popular HBO TV series "True Detective" as well as "Life Is Beautiful". In this regard, Schopenhauer is sometimes considered the founding father of today's antinatalism.

</doc>
<doc id="701" url="?curid=701" title="Angola">
Angola

Angola, officially the Republic of Angola, is a country on the west-central coast of Southern Africa. It is the second-largest Lusophone (Portuguese-speaking) country in both total area and population and is the seventh-largest country in Africa. It is bordered by Namibia to the south, the Democratic Republic of the Congo to the north, Zambia to the east, and the Atlantic Ocean to the west. Angola has an exclave province, the province of Cabinda, that borders the Republic of the Congo and the Democratic Republic of the Congo. The capital and most populous city is Luanda.
Angola has been inhabited since the Paleolithic Age. Its formation as a nation-state originates from the Kingdom of Kongo, the hegemonic state of a number of other Kikongo-speaking kingdoms that flourished in and after the 14th century. The Kingdom of Kongo became extremely wealthy and powerful through establishing the Atlantic slave trade with the Portuguese Empire. Its first explorers established relations with Kongo in 1483, and additional migrants gradually began building coastal settlements and trading posts. 
The banning of the slave trade in the 19th century severely disrupted Kongo's undiversified economic system. European settlers gradually began to establish themselves in the interior. The Portuguese colony that became Angola did not achieve its present borders until the early 20th century. There had been strong resistance by native groups such as the Cuamato, the Kwanyama, and the Mbunda. 
After a protracted anti-colonial struggle (1961-1974), Angola achieved independence in 1975 as a one-party Republic. But competing movements still struggled for power in the new nation. The country descended into a devastating civil war the same year, between the ruling People's Movement for the Liberation of Angola (MPLA), backed by the Soviet Union and Cuba; the insurgent National Union for the Total Independence of Angola, an originally Maoist and later anti-communist group supported by the United States and South Africa; and the militant organization National Liberation Front of Angola, backed by Zaire. 
The MPLA stayed in power. Since the end of the civil war in 2002, Angola has emerged as a relatively stable constitutional republic.
Economy.
Angola has vast mineral and petroleum reserves. Its economy is among the fastest-growing in the world, especially since the end of the civil war. But economic growth is highly uneven, with most of the nation's wealth concentrated in a disproportionately small part of the population. The largest investment and trade partners are China, the European Union, and the United States. 
Most Angolans have a low standard of living; life expectancy is among the lowest in the world, while infant mortality is among the highest. Since 2017, the government of João Lourenço has made fighting corruption its flagship program, so much so that many individuals from the previous administration are either jailed or awaiting trial. Whilst this effort has been recognised by foreign diplomats to be legitimate, some skeptics see the actions as being politically motivated.
Angola is a member of the United Nations, African Union, the Community of Portuguese Language Countries, and the Southern African Development Community. , the Angolan population is estimated at 37.2 million. Angola is multicultural and multiethnic. Angolan culture reflects centuries of Portuguese influence, namely the predominance of the Portuguese language and of the Catholic Church, intermingled with a variety of indigenous customs and traditions.
Etymology.
The name "Angola" comes from the Portuguese colonial name ('Kingdom of Angola'), which appeared as early as Paulo Dias de Novais's 1571 charter. The toponym was derived by the Portuguese from the title "ngola", held by the kings of Ndongo and Matamba. Ndongo in the highlands, between the Kwanza and Lucala rivers, was nominally a possession of the Kingdom of Kongo. But in the 16th century it was seeking greater independence.
History.
Early migrations and political units.
Modern Angola was populated predominantly by nomadic Khoi and San peoples prior to the first Bantu migrations. The Khoi and San peoples were hunter-gatherers, rather than practicing pastoralism or cultivation of crops. 
In the first millennium BC, they were displaced by Bantu peoples arriving from the north, most of whom likely originated in what is today northwestern Nigeria and southern Niger. Bantu speakers introduced the cultivation of bananas and taro, as well as maintenance of large cattle herds, to Angola's central highlands and the Luanda plain. Due to a number of inhibiting geographic factors throughout the territory of Angola, namely harshly traversable land, hot/humid climate, and a plethora of deadly diseases, intermingling of pre-colonial tribes in Angola had been rare.
After settlement of the migrants, a number of political entities developed. The best-known of these was the Kingdom of Kongo, based in Angola. It extended northward to what are now the Democratic Republic of the Congo, the Republic of the Congo, and Gabon. It established trade routes with other city-states and civilisations up and down the coast of southwestern and western Africa. Its traders even reached Great Zimbabwe and the Mutapa Empire, although the kingdom engaged in little or no trans-oceanic trade. To its south lay the Kingdom of Ndongo, from which the area of the later Portuguese colony was sometimes known as "Dongo". Next to that was the Kingdom of Matamba. The lesser Kingdom of Kakongo to the north was later a vassal of the Kingdom of Kongo. The people in all of these states spoke Kikongo as a common language.
Portuguese colonization.
Portuguese explorer Diogo Cão reached the area in 1484. The previous year, the Portuguese had established relations with the Kingdom of Kongo, which stretched at the time from modern Gabon in the north to the Kwanza River in the south. The Portuguese established their primary early trading post at Soyo, which is now the northernmost city in Angola apart from the Cabinda exclave.
Paulo Dias de Novais founded São Paulo de Loanda (Luanda) in 1575 with a hundred families of settlers and four hundred soldiers. Benguela was fortified in 1587 and became a township in 1617. An authoritarian state, the Kingdom of Kongo was highly centralised around its monarch and controlled neighbouring states as vassals. It had a strong economy, based on the industries of copper, ivory, salt, hides, and, to a lesser extent, slaves. The transition from a feudal system of slavery to a capitalist one with Portugal would prove crucial to the history of the Kingdom of Kongo.
As relations between Kongo and Portugal grew in the early 16th century, trade between the kingdoms also increased. Most of the trade was in palm cloth, copper, and ivory, but also increasing numbers of slaves. Kongo exported few slaves, and its slave market had remained internal. But, following the development of a successful sugar-growing colony after Portuguese settlement of São Tomé, Kongo became a major source of slaves for the island's traders and plantations. Correspondence by King Afonso documents the purchase and sale of slaves within the country. His accounts also detail which slaves captured in war were given or sold to Portuguese merchants.
Afonso continued to expand the kingdom of Kongo into the 1540s, expanding its borders to the south and east. The expansion of Kongo's population, coupled with Afonso's earlier religious reforms, allowed the ruler to centralize power in his capital and increase the power of the monarchy. He also established a royal monopoly on some trade. To govern the growing slave trade, Afonso and several Portuguese kings claimed a joint monopoly on the external slave trade.
The slave trade increasingly became Kongo's primary, and arguably sole, economic sector. A major obstacle for the Kingdom of Kongo was that slaves were the only commodity for which the European powers were willing to trade. Kongo lacked an effective international currency. Kongolese nobles could buy slaves with the national currency of nzimbu shells, which could be traded for slaves. These could be sold to gain international currency. 
As the slave trade was the only commodity in which Europeans were interested in the region during the 16th and 17th centuries, the Kongo economy was unable to diversify or later industrialise outside of sectors in which slavery was involved, such as the arms industry. The increased production and sale of guns within the kingdom was due to the salient issue of the slave trade, which had become an increasingly violent struggle. There was a constant need for slaves for the kings and queens to sell in exchange for foreign commodities, the absence of which would prevent them from having any influence with European powers such as Portugal and eventually the Dutch Republic. 
Kongolese kings needed this influence to garner support from European powers for quelling internal rebellions. The situation became increasingly complicated during the rule of Garcia II, who needed the assistance of the Dutch military to drive out the Portuguese from Luanda, in spite of the fact that Portugal was Kongo's primary slave trading partner.
By the early 17th century, the supply of foreign slaves captured by the Kongolese externally was waning. The government began to approve the enslavement of freeborn Kongolese citizens for relatively minor infractions, nearly any disobeying of the authoritarian system and the aristocracy. If several villagers were deemed guilty of a crime, it became relatively common for the whole village to be enslaved. The resulting chaos and internal conflict from Garcia II's reign would lead into that of his son and successor, António I. He was killed in 1665 by Portuguese at the Battle of Mbwila 1665, together with a substantial proportion of the aristocracy. The colonists were expanding their power. 
War broke out more widely in the Kingdom of Kongo after the death of António I. Much of the stability and access to iron ore and charcoal necessary for gunsmiths to maintain the arms industry was disrupted. From then on, in this period almost every Kongolese citizen was in danger of being enslaved. Many Kongolese subjects were adroit in making guns, and they were enslaved to have their skills available to colonists in the New World, where they worked as blacksmiths, ironworkers, and charcoal makers.
The Portuguese established several other settlements, forts and trading posts along the Angolan coast, principally trading in Angolan slaves for plantations. Local slave dealers provided a large number of slaves for the Portuguese Empire, usually in exchange for manufactured goods from Europe. This part of the Atlantic slave trade continued until after Brazil's independence in the 1820s.
Despite Portugal's territorial claims in Angola, its control over much of the country's vast interior was minimal. In the 16th century Portugal gained control of the coast through a series of treaties and wars. Life for European colonists was difficult and progress was slow. John Iliffe notes that "Portuguese records of Angola from the 16th century show that a great famine occurred on average every seventy years; accompanied by epidemic disease, it might kill one-third or one-half of the population, destroying the demographic growth of a generation and forcing colonists back into the river valleys".
During the Portuguese Restoration War, the Dutch West India Company occupied the principal settlement of Luanda in 1641, using alliances with local peoples to carry out attacks against Portuguese holdings elsewhere. A fleet under Salvador de Sá retook Luanda in 1648; reconquest of the rest of the territory was completed by 1650. New treaties with the Kongo were signed in 1649; others with Njinga's Kingdom of Matamba and Ndongo followed in 1656. The conquest of Pungo Andongo in 1671 was the last major Portuguese expansion from Luanda, as attempts to invade Kongo in 1670 and Matamba in 1681 failed. Colonial outposts also expanded inward from Benguela, but until the late 19th century the inroads from Luanda and Benguela were very limited. Hamstrung by a series of political upheavals in the early 1800s, Portugal was slow to mount a large scale annexation of Angolan territory.
The slave trade was abolished in Angola in 1836, and in 1854 the colonial government freed all its existing slaves. Four years later, a more progressive administration appointed by Portugal abolished slavery altogether. However, these decrees remained largely unenforceable, and the Portuguese depended on assistance from the British Royal Navy and what became known as the Blockade of Africa to enforce their ban on the slave trade. This coincided with a series of renewed military expeditions into the bush.
By the mid-nineteenth century Portugal had established its dominion as far north as the Congo River and as far south as Mossâmedes. Until the late 1880s, Portugal entertained proposals to link Angola with its colony in Mozambique but was blocked by British and Belgian opposition. In this period, the Portuguese came up against different forms of armed resistance from various peoples in Angola.
The Berlin Conference in 1884–1885 set the colony's borders, delineating the boundaries of Portuguese claims in Angola, although many details were unresolved until the 1920s. Trade between Portugal and its African territories rapidly increased as a result of protective tariffs, leading to increased development, and a wave of new Portuguese immigrants.
Between 1939 and 1943, Portuguese army operations against the Mucubal, who they accused of rebellion and cattle-theiving, resulted in hundreds of Mucubal killed. During the campaign, 3,529 were taken prisoner, 20% of whom were women and children, and imprisoned in concentration camps. Many died in captivity from undernourishment, violence and forced labor. Around 600 were sent to Sao Tome and Principe. Hundreds were also sent to a camp in Damba, where 26% died.
Angolan independence.
Under colonial law, black Angolans were forbidden from forming political parties or labour unions. The first nationalist movements did not take root until after World War II, spearheaded by a largely Westernised and Portuguese-speaking urban class, which included many mestiços. During the early 1960s they were joined by other associations stemming from "ad hoc" labour activism in the rural workforce. Portugal's refusal to address increasing Angolan demands for self-determination provoked an armed conflict, which erupted in 1961 with the Baixa de Cassanje revolt and gradually evolved into a protracted war of independence that persisted for the next twelve years. Throughout the conflict, three militant nationalist movements with their own partisan guerrilla wings emerged from the fighting between the Portuguese government and local forces, supported to varying degrees by the Portuguese Communist Party.
The "National Front for the Liberation of Angola" (FNLA) recruited from Bakongo refugees in Zaire. Benefiting from particularly favourable political circumstances in Léopoldville, and especially from a common border with Zaire, Angolan political exiles were able to build up a power base among a large expatriate community from related families, clans, and traditions. People on both sides of the border spoke mutually intelligible dialects and enjoyed shared ties to the historical Kingdom of Kongo. Though as foreigners skilled Angolans could not take advantage of Mobutu Sese Seko's state employment programme, some found work as middlemen for the absentee owners of various lucrative private ventures. The migrants eventually formed the FNLA with the intention of making a bid for political power upon their envisaged return to Angola.
A largely Ovimbundu guerrilla initiative against the Portuguese in central Angola from 1966 was spearheaded by Jonas Savimbi and the "National Union for the Total Independence of Angola" (UNITA). It remained handicapped by its geographic remoteness from friendly borders, the ethnic fragmentation of the Ovimbundu, and the isolation of peasants on European plantations where they had little opportunity to mobilise.
During the late 1950s, the rise of the Marxist–Leninist "Popular Movement for the Liberation of Angola" (MPLA) in the east and Dembos hills north of Luanda came to hold special significance. Formed as a coalition resistance movement by the Angolan Communist Party, the organisation's leadership remained predominantly Ambundu and courted public sector workers in Luanda. Although both the MPLA and its rivals accepted material assistance from the Soviet Union or the People's Republic of China, the former harboured strong anti-imperialist views and was openly critical of the United States and its support for Portugal. This allowed it to win important ground on the diplomatic front, soliciting support from nonaligned governments in Morocco, Ghana, Guinea, Mali, and the United Arab Republic.
The MPLA attempted to move its headquarters from Conakry to Léopoldville in October 1961, renewing efforts to create a common front with the FNLA, then known as the "Union of Angolan Peoples" (UPA) and its leader Holden Roberto. Roberto turned down the offer. When the MPLA first attempted to insert its own insurgents into Angola, the cadres were ambushed and annihilated by UPA partisans on Roberto's orders—setting a precedent for the bitter factional strife which would later ignite the Angolan Civil War.
Angolan Civil War.
Throughout the war of independence, the three rival nationalist movements were severely hampered by political and military factionalism, as well as their inability to unite guerrilla efforts against the Portuguese. Between 1961 and 1975 the MPLA, UNITA, and the FNLA competed for influence in the Angolan population and the international community. The Soviet Union and Cuba became especially sympathetic towards the MPLA and supplied that party with arms, ammunition, funding, and training. They also backed UNITA militants until it became clear that the latter was at irreconcilable odds with the MPLA.
The collapse of Portugal's Estado Novo government following the 1974 Carnation Revolution suspended all Portuguese military activity in Africa and the brokering of a ceasefire pending negotiations for Angolan independence. Encouraged by the Organisation of African Unity, Holden Roberto, Jonas Savimbi, and MPLA chairman Agostinho Neto met in Mombasa in early January 1975 and agreed to form a coalition government. This was ratified by the Alvor Agreement later that month, which called for general elections and set the country's independence date for 11 November 1975. All three factions, however, followed up on the ceasefire by taking advantage of the gradual Portuguese withdrawal to seize various strategic positions, acquire more arms, and enlarge their militant forces. The rapid influx of weapons from numerous external sources, especially the Soviet Union and the United States, as well as the escalation of tensions between the nationalist parties, fueled a new outbreak of hostilities. With tacit American and Zairean support the FNLA began massing large numbers of troops in northern Angola in an attempt to gain military superiority. Meanwhile, the MPLA began securing control of Luanda, a traditional Ambundu stronghold. Sporadic violence broke out in Luanda over the next few months after the FNLA attacked the MPLA's political headquarters in March 1975. The fighting intensified with street clashes in April and May, and UNITA became involved after over two hundred of its members were massacred by an MPLA contingent that June. An upswing in Soviet arms shipments to the MPLA influenced a decision by the Central Intelligence Agency to likewise provide substantial covert aid to the FNLA and UNITA.
In August 1975, the MPLA requested direct assistance from the Soviet Union in the form of ground troops. The Soviets declined, offering to send advisers but no troops; however, Cuba was more forthcoming and in late September dispatched nearly five hundred combat personnel to Angola, along with sophisticated weaponry and supplies. By independence, there were over a thousand Cuban soldiers in the country. They were kept supplied by a massive airbridge carried out with Soviet aircraft. The persistent buildup of Cuban and Soviet military aid allowed the MPLA to drive its opponents from Luanda and blunt an abortive intervention by Zairean and South African troops, which had deployed in a belated attempt to assist the FNLA and UNITA. The FNLA was largely annihilated after the decisive Battle of Quifangondo, although UNITA managed to withdraw its civil officials and militia from Luanda and seek sanctuary in the southern provinces. From there, Savimbi continued to mount a determined insurgent campaign against the MPLA.
Between 1975 and 1991, the MPLA implemented an economic and political system based on the principles of scientific socialism, incorporating central planning and a Marxist–Leninist one-party state. It embarked on an ambitious programme of nationalisation, and the domestic private sector was essentially abolished. Privately owned enterprises were nationalised and incorporated into a single umbrella of state-owned enterprises known as "Unidades Economicas Estatais" (UEE). Under the MPLA, Angola experienced a significant degree of modern industrialisation. However, corruption and graft also increased and public resources were either allocated inefficiently or simply embezzled by officials for personal enrichment. The ruling party survived an attempted coup d'état by the Maoist-oriented Communist Organisation of Angola (OCA) in 1977, which was suppressed after a series of bloody political purges left thousands of OCA supporters dead.
The MPLA abandoned its former Marxist ideology at its third party congress in 1990, and declared social democracy to be its new platform. Angola subsequently became a member of the International Monetary Fund; restrictions on the market economy were also reduced in an attempt to draw foreign investment. By May 1991 it reached a peace agreement with UNITA, the Bicesse Accords, which scheduled new general elections for September 1992. When the MPLA secured a major electoral victory, UNITA objected to the results of both the presidential and legislative vote count and returned to war. Following the election, the Halloween massacre occurred from 30 October to 1 November, where MPLA forces killed thousands of UNITA supporters.
21st century.
On 22 February 2002, government troops killed Savimbi in a skirmish in the Moxico province. UNITA and the MPLA consented to the Luena Memorandum of Understanding in April; UNITA agreed to give up its armed wing. With the elections in 2008 and 2012, an MPLA-ruled dominant-party system emerged, with UNITA and the FNLA as opposition parties.
Angola has a serious humanitarian crisis; the result of the prolonged war, of the abundance of minefields, and the continued political agitation in favour of the independence of the exclave of Cabinda (carried out in the context of the protracted Cabinda conflict by the FLEC). While most of the internally displaced have now squatted around the capital, in "musseques" (shanty towns) the general situation for Angolans remains desperate.
A drought in 2016 caused the worst food crisis in Southern Africa in 25 years, affecting 1.4 million people across seven of Angola's eighteen provinces. Food prices rose and acute malnutrition rates doubled, impacting over 95,000 children.
José Eduardo dos Santos stepped down as President of Angola after 38 years in 2017, being peacefully succeeded by João Lourenço, Santos' chosen successor. Some members of the dos Santos family were later linked to high levels of corruption. In July 2022, ex-president José Eduardo dos Santos died in Spain.
In August 2022, the ruling party, MPLA, won another majority and President Lourenço won a second five-year term in the election. However, the election was the tightest in Angola's history.
Geography.
At , Angola is the world's twenty-second largest country – comparable in size to Mali, or twice the size of France or of Texas. It lies mostly between latitudes 4° and 18°S, and longitudes 12° and 24°E.
Angola borders Namibia to the south, Zambia to the east, the Democratic Republic of the Congo to the north-east and the South Atlantic Ocean to the west.
The coastal exclave of Cabinda in the north has borders with the Republic of the Congo to the north and with the Democratic Republic of the Congo to the south. Angola has a favorable coastline for maritime trade, with four natural harbors: Luanda, Lobito, Moçâmedes, and Porto Alexandre. These natural indentations contrast with Africa's typical coastline of rocky cliffs and deep bays. Angola's capital, Luanda, lies on the Atlantic coast in the northwest of the country.
Angola had a 2018 Forest Landscape Integrity Index mean score of 8.35/10, ranking it 23rd globally out of 172 countries.
Climate.
Like the rest of tropical Africa, Angola experiences distinct, alternating rainy and dry seasons. In the north, the rainy season may last for as long as seven months—usually from September to April, with perhaps a brief slackening in January or February. In the south, the rainy season begins later, in November, and lasts until about February. The dry season ("cacimbo") is often characterized by a heavy morning mist. In general, precipitation is higher in the north, but at any latitude it is greater in the interior than along the coast and increases with altitude. Temperatures fall with distance from the equator and with altitude and tend to rise closer to the Atlantic Ocean. Thus, at Soyo, at the mouth of the Congo River, the average annual temperature is about 26 °C, but it is under 16 °C at Huambo on the temperate central plateau. The coolest months are July and August (in the middle of the dry season), when frost may sometimes form at higher altitudes.
Government and politics.
The Angolan government is composed of three branches of government: executive, legislative and judicial. The executive branch of the government is composed of the President, the vice-presidents and the Council of Ministers.
The legislative branch comprises a 220-seat unicameral legislature, the National Assembly of Angola, elected from multi-member province-wide and nationwide constituencies using party-list proportional representation. For decades, political power has been concentrated in the presidency.
After 38 years of rule, in 2017 President dos Santos stepped down from MPLA leadership. The leader of the winning party at the parliamentary elections in August 2017 would become the next president of Angola. The MPLA selected the former Defense Minister João Lourenço as Santos' chosen successor.
In what has been described as a political purge to cement his power and reduce the influence of the Dos Santos family, Lourenço subsequently sacked the chief of the national police, Ambrósio de Lemos, and the head of the intelligence service, Apolinário José Pereira. Both are considered allies of former president Dos Santos. He also removed Isabel Dos Santos, daughter of the former president, as head of the country's state oil company Sonangol. In August 2020, José Filomeno dos Santos, son of Angola's former president, was sentenced for five years in jail for fraud and corruption.
Constitution.
The Constitution of 2010 establishes the broad outlines of government structure and delineates the rights and duties of citizens. The legal system is based on Portuguese law and customary law but is weak and fragmented, and courts operate in only 12 of more than 140 municipalities. A Supreme Court serves as the appellate tribunal; a Constitutional Court does not hold the powers of judicial review. Governors of the 18 provinces are appointed by the president. After the end of the civil war, the regime came under pressure from within as well as from the international community to become more democratic and less authoritarian. Its reaction was to implement a number of changes without substantially changing its character.
The new constitution, adopted in 2010, did away with presidential elections, introducing a system in which the president and the vice-president of the political party that wins the parliamentary elections automatically become president and vice-president. Directly or indirectly, the president controls all other organs of the state, so there is "de facto" no separation of powers. In the classifications used in constitutional law, this government falls under the category of "authoritarian regime."
Justice.
A Supreme Court serves as a court of appeal. The Constitutional Court is the supreme body of the constitutional jurisdiction, established with the approval of Law no. 2/08, of 17 June – Organic Law of the Constitutional Court and Law n. 3/08, of 17 June – Organic Law of the Constitutional Process. The legal system is based on Portuguese and customary law. There are 12 courts in more than 140 counties in the country. Its first task was the validation of the candidacies of the political parties to the legislative elections of 5 September 2008. Thus, on 25 June 2008, the Constitutional Court was institutionalized and its Judicial Counselors assumed the position before the President of the Republic. Currently, seven advisory judges are present, four men and three women.
In 2014, a new penal code took effect in Angola. The classification of money-laundering as a crime is one of the novelties in the new legislation.
Administrative divisions.
 
, Angola is divided into eighteen provinces ("províncias") and 162 municipalities. The municipalities are further divided into 559 communes (townships). The provinces are:
Exclave of Cabinda.
With an area of approximately , the Northern Angolan province of Cabinda is unusual in being separated from the rest of the country by a strip, some wide, of the Democratic Republic of Congo along the lower Congo River. Cabinda borders the Congo Republic to the north and north-northeast and the DRC to the east and south. The town of Cabinda is the chief population centre.
According to a 1995 census, Cabinda had an estimated population of 600,000, approximately 400,000 of whom are citizens of neighboring countries. Population estimates are, however, highly unreliable. Consisting largely of tropical forest, Cabinda produces hardwoods, coffee, cocoa, crude rubber and palm oil.
The product for which it is best known, however, is its oil, which has given it the nickname, "the Kuwait of Africa". Cabinda's petroleum production from its considerable offshore reserves now accounts for more than half of Angola's output. Most of the oil along its coast was discovered under Portuguese rule by the Cabinda Gulf Oil Company (CABGOC) from 1968 onwards.
Ever since Portugal handed over sovereignty of its former overseas province of Angola to the local independence groups (MPLA, UNITA and FNLA), the territory of Cabinda has been a focus of separatist guerrilla actions opposing the Government of Angola (which has employed its armed forces, the FAA—Forças Armadas Angolanas) and Cabindan separatists.
Foreign relations.
Angola is a founding member state of the Community of Portuguese Language Countries (CPLP), also known as the Lusophone Commonwealth, an international organization and political association of Lusophone nations across four continents, where Portuguese is an official language.
On 16 October 2014, Angola was elected for the second time a non-permanent member of the United Nations Security Council, with 190 favorable votes out of a total of 193. The term of office began on 1 January 2015 and expired on 31 December 2016.
Since January 2014, the Republic of Angola has been chairing the International Conference for the Great Lakes Region (CIRGL). [80] In 2015, CIRGL Executive Secretary Ntumba Luaba said that Angola is the example to be followed by the members of the organization, due to the significant progress made during the 12 years of peace, namely in terms of socio-economic stability and political-military.
Military.
The Angolan Armed Forces (Forças Armadas Angolanas, FAA) are headed by a Chief of Staff who reports to the Minister of Defence. There are three divisions—the Army (Exército), Navy (Marinha de Guerra, MGA) and National Air Force (Força Aérea Nacional, FAN). Total manpower is 107,000; plus paramilitary forces of 10,000 (2015 est.).
Its equipment includes Russian-manufactured fighters, bombers and transport planes. There are also Brazilian-made EMB-312 Tucanos for training, Czech-made L-39 Albatroses for training and bombing, and a variety of western-made aircraft such as the C-212\Aviocar, Sud Aviation Alouette III, etc. A small number of FAA personnel are stationed in the Democratic Republic of the Congo (Kinshasa) and 500 more were deployed in March 2023 due to the resurgence of the M23. The FAA has also participated in the Southern African Development Community (SADC)'s mission for peace in Cabo Delgado, Mozambique.
Police.
The National Police departments are Public Order, Criminal Investigation, Traffic and Transport, Investigation and Inspection of Economic Activities, Taxation and Frontier Supervision, Riot Police and the Rapid Intervention Police. The National Police are in the process of standing up an air wing, to provide helicopter support for operations. The National Police are developing their criminal investigation and forensic capabilities. The force consists of an estimated 6,000 patrol officers, 2,500 taxation and frontier supervision officers, 182 criminal investigators, 100 financial crimes detectives, and approximately 90 economic activity inspectors.
The National Police have implemented a modernisation and development plan to increase the capabilities and efficiency of the total force. In addition to administrative reorganisation, modernisation projects include procurement of new vehicles, aircraft and equipment, construction of new police stations and forensic laboratories, restructured training programmes and the replacement of AKM rifles with 9 mm Uzis for officers in urban areas.
Human rights.
Angola was classified as 'not free' by Freedom House in the Freedom in the World 2014 report. The report noted that the August 2012 parliamentary elections, in which the ruling Popular Movement for the Liberation of Angola won more than 70% of the vote, suffered from serious flaws, including outdated and inaccurate voter rolls. Voter turnout dropped from 80% in 2008 to 60%.
A 2012 report by the U.S. Department of State said, "The three most important human rights abuses [in 2012] were official corruption and impunity; limits on the freedoms of assembly, association, speech, and press; and cruel and excessive punishment, including reported cases of torture and beatings as well as unlawful killings by police and other security personnel."
Angola ranked forty-two of forty-eight sub-Saharan African states on the 2007 Index of African Governance list and scored poorly on the 2013 Ibrahim Index of African Governance. It was ranked 39 out of 52 sub-Saharan African countries, scoring particularly badly in the areas of participation and human rights, sustainable economic opportunity and human development. The Ibrahim Index uses a number of variables to compile its list which reflects the state of governance in Africa.
In 2019, homosexual acts were decriminalized in Angola, and the government also prohibited discrimination based on sexual orientation. The vote was overwhelming: 155 for, 1 against, 7 abstaining.
Economy.
Angola has diamonds, oil, gold, copper and rich wildlife (which was dramatically depleted during the civil war), forest and fossil fuels. Since independence, oil and diamonds have been the most important economic resource. Smallholder and plantation agriculture dramatically dropped in the Angolan Civil War, but began to recover after 2002.
Angola's economy has in recent years moved on from the disarray caused by a quarter-century of Angolan civil war to become the fastest-growing economy in Africa and one of the fastest-growing in the world, with an average GDP growth of 20% between 2005 and 2007. In the period 2001–10, Angola had the world's highest annual average GDP growth, at 11.1%.
In 2004, the Exim Bank of China approved a $2 billion line of credit to Angola, to be used for rebuilding Angola's infrastructure, and to limit the influence of the International Monetary Fund there.
China is Angola's biggest trade partner and export destination as well as a significant source of imports. Bilateral trade reached $27.67 billion in 2011, up 11.5% year-on-year. China's imports, mainly crude oil and diamonds, increased 9.1% to $24.89 billion while China's exports to Angola, including mechanical and electrical products, machinery parts and construction materials, surged 38.8%. The oil glut led to a local price for unleaded gasoline of £0.37 a gallon.
As of 2021, the biggest import partners were the European Union, followed by China, Togo, the United States, and Brazil. More than half of Angola's exports go to China, followed by a significantly smaller amount to India, the European Union, and the United Arab Emirates.
The Angolan economy grew 18% in 2005, 26% in 2006 and 17.6% in 2007. Due to the global recession, the economy contracted an estimated −0.3% in 2009. The security brought about by the 2002 peace settlement has allowed the resettlement of 4 million displaced persons and a resulting large-scale increase in agriculture production. Angola's economy is expected to grow by 3.9 per cent in 2014 said the International Monetary Fund (IMF), robust growth in the non-oil economy, mainly driven by a very good performance in the agricultural sector, is expected to offset a temporary drop in oil production.
Angola's financial system is maintained by the National Bank of Angola and managed by the governor . According to a study on the banking sector, carried out by Deloitte, the monetary policy led by Banco Nacional de Angola (BNA), the Angolan national bank, allowed a decrease in the inflation rate put at 7.96% in December 2013, which contributed to the sector's growth trend. Estimates released by Angola's central bank, said the country's economy should grow at an annual average rate of 5 per cent over the next four years, boosted by the increasing participation of the private sector. Angola was ranked 132rd in the Global Innovation Index in 2023.
Although the country's economy has grown significantly since Angola achieved political stability in 2002, mainly due to fast-rising earnings in the oil sector, Angola faces huge social and economic problems. These are in part a result of almost continual armed conflict from 1961 on, although the highest level of destruction and socio-economic damage took place after the 1975 independence, during the long years of civil war. However, high poverty rates and blatant social inequality chiefly stems from persistent authoritarianism, "neo-patrimonial" practices at all levels of the political, administrative, military and economic structures, and of a pervasive corruption. The main beneficiaries are political, administrative, economic and military power holders, who have accumulated (and continue to accumulate) enormous wealth.
"Secondary beneficiaries" are the middle strata that are about to become social classes. However, almost half the population has to be considered poor, with dramatic differences between the countryside and the cities, where slightly more than 50% of the people reside.
A study carried out in 2008 by the Angolan Instituto Nacional de Estatística found that in rural areas roughly 58% must be classified as "poor" according to UN norms but in the urban areas only 19%, and an overall rate of 37%. In cities, a majority of families, well beyond those officially classified as poor, must adopt a variety of survival strategies. In urban areas social inequality is most evident and it is extreme in Luanda. In the Human Development Index Angola constantly ranks in the bottom group.
In January 2020, a leak of government documents known as the "Luanda Leaks" showed that U.S. consulting companies such as Boston Consulting Group, McKinsey &amp; Company, and PricewaterhouseCoopers had helped members of the family of former President José Eduardo dos Santos (especially his daughter Isabel dos Santos) corruptly run Sonangol for their own personal profit, helping them use the company's revenues to fund vanity projects in France and Switzerland. After further revelations in the Pandora Papers, former generals Dias and do Nascimento and former presidential advisers were also accused of misappropriating significant public funds for personal benefit.
The enormous differences between the regions pose a serious structural problem for the Angolan economy, illustrated by the fact that about one third of economic activities are concentrated in Luanda and neighbouring Bengo province, while several areas of the interior suffer economic stagnation and even regression.
One of the economic consequences of social and regional disparities is a sharp increase in Angolan private investments abroad. The small fringe of Angolan society where most of the asset accumulation takes place seeks to spread its assets, for reasons of security and profit. For the time being, the biggest share of these investments is concentrated in Portugal where the Angolan presence (including the family of the state president) in banks as well as in the domains of energy, telecommunications, and mass media has become notable, as has the acquisition of vineyards and orchards as well as of tourism enterprises.
Angola has upgraded critical infrastructure, an investment made possible by funds from the country's development of oil resources. According to a report, just slightly more than ten years after the end of the civil war Angola's standard of living has overall greatly improved. Life expectancy, which was just 46 years in 2002, reached 51 in 2011. Mortality rates for children fell from 25 per cent in 2001 to 19 per cent in 2010 and the number of students enrolled in primary school has tripled since 2001. However, at the same time the social and economic inequality that has characterised the country for so long has not diminished, but has deepened in all respects.
With a stock of assets corresponding to 70 billion Kz (US$6.8 billion), Angola is now the third-largest financial market in sub-Saharan Africa, surpassed only by Nigeria and South Africa. According to the Angolan Minister of Economy, Abraão Gourgel, the financial market of the country grew modestly since 2002 and now occupies third place in sub-Saharan Africa.
On 19 December 2014, the Capital Market in Angola was launched. BODIVA (Angola Stock Exchange and Derivatives, in English) was allocated the secondary public debt market, and was expected to launch the corporate debt market by 2015, though the stock market itself was only expected to commence trading in 2016.
Natural resources.
"The Economist" reported in 2008 that diamonds and oil make up 60% of Angola's economy, almost all of the country's revenue and all of its dominant exports. Growth is almost entirely driven by rising oil production which surpassed in late 2005 and was expected to grow to by 2007. Control of the oil industry is consolidated in Sonangol Group, a conglomerate owned by the Angolan government. In December 2006, Angola was admitted as a member of OPEC. In 2022, the country produced an average of 1.165 million barrels of oil per day, according to its National Oil, Gas and Biofuel's Agency (ANPG).
According to the Heritage Foundation, a conservative American think tank, oil production from Angola has increased so significantly that Angola now is China's biggest supplier of oil. "China has extended three multi-billion dollar lines of credit to the Angolan government; two loans of $2 billion from China Exim Bank, one in 2004, the second in 2007, as well as one loan in 2005 of $2.9 billion from China International Fund Ltd."
Growing oil revenues also created opportunities for corruption: according to a recent Human Rights Watch report, US$32 billion disappeared from government accounts in 2007–2010. Furthermore, Sonangol, the state-run oil company, controls 51% of Cabinda's oil. Due to this market control, the company ends up determining the profit received by the government and the taxes it pays. The council of foreign affairs states that the World Bank mentioned that Sonangol is a taxpayer, it carries out quasi-fiscal activities, it invests public funds, and, as concessionaire, it is a sector regulator. This multifarious work program creates conflicts of interest and characterises a complex relationship between Sonangol and the government that weakens the formal budgetary process and creates uncertainty as regards the actual fiscal stance of the state."
In 2002, Angola demanded compensation for oil spills allegedly caused by Chevron Corporation, the first time it had fined a multinational corporation operating in its waters.
Operations in its diamond mines include partnerships between state-run Endiama and mining companies such as ALROSA which operate in Angola.
Access to biocapacity in Angola is higher than world average. In 2016, Angola had 1.9 global hectares of biocapacity per person within its territory, slightly more than world average of 1.6 global hectares per person. In 2016, Angola used 1.01 global hectares of biocapacity per person - their ecological footprint of consumption. This means they use about half as much biocapacity as Angola contains. As a result, Angola is running a biocapacity reserve.
Agriculture.
Agriculture and forestry is an area of potential opportunity for the country. The African Economic Outlook organization states that "Angola requires 4.5 million tonnes a year of grain but grows only about 55% of the maize it needs, 20% of the rice and just 5% of its required wheat".
In addition, the World Bank estimates that "less than 3 per cent of Angola's abundant fertile land is cultivated and the economic potential of the forestry sector remains largely unexploited".
Before independence in 1975, Angola was a bread-basket of southern Africa and a major exporter of bananas, coffee and sisal, but three decades of civil war destroyed fertile countryside, left it littered with landmines and drove millions into the cities. The country now depends on expensive food imports, mainly from South Africa and Portugal, while more than 90% of farming is done at the family and subsistence level. Thousands of Angolan small-scale farmers are trapped in poverty.
Transport.
Transport in Angola consists of:
Angola centers its port trade in five main ports: Namibe, Lobito, Soyo, Cabinda and Luanda. The port of Luanda is the largest of the five, as well as being one of the busiest on the African continent.
Two trans-African automobile routes pass through Angola: the Tripoli-Cape Town Highway and the Beira-Lobito Highway. Travel on highways outside of towns and cities in Angola (and in some cases within) is often not best advised for those without four-by-four vehicles. While reasonable road infrastructure has existed within Angola, time and war have taken their toll on the road surfaces, leaving many severely potholed, littered with broken asphalt. In many areas drivers have established alternative tracks to avoid the worst parts of the surface, although careful attention must be paid to the presence or absence of landmine warning markers by the side of the road. The Angolan government has contracted the restoration of many of the country's roads. The road between Lubango and Namibe, for example, was completed recently with funding from the European Union, and is comparable to many European main routes. Completing the road infrastructure is likely to take some decades, but substantial efforts are already being made.
Telecommunications.
The telecommunications industry is considered one of the main strategic sectors in Angola.
In October 2014, the building of an optic fiber underwater cable was announced. This project aims to turn Angola into a continental hub, thus improving Internet connections both nationally and internationally.
On 11 March 2015, the First Angolan Forum of Telecommunications and Information Technology was held in Luanda under the motto "The challenges of telecommunications in the current context of Angola", to promote debate on topical issues on telecommunications in Angola and worldwide. A study of this sector, presented at the forum, said Angola had the first telecommunications operator in Africa to test LTE – with speeds up to 400 Mbit/s – and mobile penetration of about 75%; there are about 3.5 million smartphones in the Angolan market; There are about of optical fibre installed in the country.
The first Angolan satellite, AngoSat-1, was launched into orbit on 26 December 2017. It was launched from the Baikonur space center in Kazakhstan on board a Zenit 3F rocket. The satellite was built by Russia's RSC Energia, a subsidiary of the state-run space industry player Roscosmos. The satellite payload was supplied by Airbus Defence &amp; Space. Due to an on-board power failure during solar panel deployment, on 27 December, RSC Energia revealed that they lost communications contact with the satellite. Although, subsequent attempts to restore communications with the satellite were successful, the satellite eventually stopped sending data and RSC Energia confirmed that AngoSat-1 was inoperable. The launch of AngoSat-1 was aimed at ensuring telecommunications throughout the country. According to Aristides Safeca, Secretary of State for Telecommunications, the satellite was aimed at providing telecommunications services, TV, internet and e-government and was expected to remain in operation "at best" for 18 years.
A replacement satellite named AngoSat-2 was pursued and was expected to be in service by 2020. As of February 2021, Ango-Sat-2 was about 60% ready. The officials reported the launch was expected in about 17 months, by July 2022. The launch of AngoSat-2 occurred on 12 October 2022.
Technology.
The management of the top-level domain '.ao' passed from Portugal to Angola in 2015, following new legislation. A joint decree of Minister of Telecommunications and Information Technologies José Carvalho da Rocha and the minister of Science and Technology, Maria Cândida Pereira Teixeira, states that "under the massification" of that Angolan domain, "conditions are created for the transfer of the domain root '.ao' of Portugal to Angola".
Demographics.
Angola has a population of 24,383,301 inhabitants according to the preliminary results of its 2014 census, the first one conducted or carried out since 15 December 1970. It is composed of Ovimbundu (language Umbundu) 37%, Ambundu (language Kimbundu) 23%, Bakongo 13%, and 32% other ethnic groups (including the Chokwe, the Ovambo, the Ganguela and the Xindonga) as well as about 2% "mulattos" (mixed European and African), 1.6% Chinese and 1% European. The Ambundu and Ovimbundu ethnic groups combined form a majority of the population, at 62%. However, on 23 March 2016, official data revealed by Angola's National Statistic Institute – Instituto Nacional de Estatística (INE), states that Angola has a population of 25,789,024 inhabitants.
It is estimated that Angola was host to 12,100 refugees and 2,900 asylum seekers by the end of 2007. 11,400 of those refugees were originally from the Democratic Republic of Congo, who arrived in the 1970s. there were an estimated 400,000 Democratic Republic of the Congo migrant workers, at least 220,000 Portuguese, and about 259,000 Chinese living in Angola. 1 million Angolans are mixed race (black and white). Also, 40,000 Vietnamese live in the country.
Since 2003, more than 400,000 Congolese migrants have been expelled from Angola. Prior to independence in 1975, Angola had a community of approximately 350,000 Portuguese, but the vast majority left after independence and the ensuing civil war. However, Angola has recovered its Portuguese minority in recent years; currently, there are about 200,000 registered with the consulates, and increasing due to the debt crisis in Portugal and the relative prosperity in Angola. The Chinese population stands at 258,920, mostly composed of temporary migrants. Also, there is a small Brazilian community of about 5,000 people. The Roma were deported to Angola from Portugal.
, the total fertility rate of Angola is 5.54 children born per woman (2012 estimates), the 11th highest in the world.
Languages.
The languages in Angola are those originally spoken by the different ethnic groups and Portuguese, introduced during the Portuguese colonial era. The most widely spoken indigenous languages are Umbundu, Kimbundu and Kikongo, in that order. Portuguese is the official language of the country.
Although the exact numbers of those fluent in Portuguese or who speak Portuguese as a first language are unknown, a 2012 study mentions that Portuguese is the first language of 39% of the population. In 2014, a census carried out by the Instituto Nacional de Estatística in Angola mentions that 71.15% of the nearly 25.8 million inhabitants of Angola (meaning around 18.3 million people) use Portuguese as a first or second language.
According to the 2014 census, Portuguese is spoken by 71.1% of Angolans, Umbundu by 23%, Kikongo by 8.2%, Kimbundu by 7.8%, Chokwe by 6.5%, Nyaneka by 3.4%, Ngangela by 3.1%, Fiote by 2.4%, Kwanyama by 2.3%, Muhumbi by 2.1%, Luvale by 1%, and other languages by 4.1%.
Religion.
There are about 1,000 religious communities, mostly Christian, in Angola. While reliable statistics are nonexistent, estimates have it that more than half of the population are Catholics, while about a quarter adhere to the Protestant churches introduced during the colonial period: the Congregationalists mainly among the Ovimbundu of the Central Highlands and the coastal region to its west, the Methodists concentrating on the Kimbundu speaking strip from Luanda to Malanje, the Baptists almost exclusively among the Bakongo of the north-west (now present in Luanda as well) and dispersed Adventists, Reformed, and Lutherans.
In Luanda and region there subsists a nucleus of the "syncretic" Tocoists and in the north-west a sprinkling of Kimbanguism can be found, spreading from the Congo/Zaïre. Since independence, hundreds of Pentecostal and similar communities have sprung up in the cities, whereby now about 50% of the population is living; several of these communities/churches are of Brazilian origin.
 the U.S. Department of State estimates the Muslim population at 80,000–90,000, less than 1% of the population, while the Islamic Community of Angola puts the figure closer to 500,000. Muslims consist largely of migrants from West Africa and the Middle East (especially Lebanon), although some are local converts. The Angolan government does not legally recognize any Muslim organizations and often shuts down mosques or prevents their construction.
In a study assessing nations' levels of religious regulation and persecution with scores ranging from 0 to 10 where 0 represented low levels of regulation or persecution, Angola was scored 0.8 on Government Regulation of Religion, 4.0 on Social Regulation of Religion, 0 on Government Favoritism of Religion and 0 on Religious Persecution.
Foreign missionaries were very active prior to independence in 1975, although since the beginning of the anti-colonial fight in 1961 the Portuguese colonial authorities expelled a series of Protestant missionaries and closed mission stations based on the belief that the missionaries were inciting pro-independence sentiments. Missionaries have been able to return to the country since the early 1990s, although security conditions due to the civil war have prevented them until 2002 from restoring many of their former inland mission stations.
The Catholic Church and some major Protestant denominations mostly keep to themselves in contrast to the "New Churches" which actively proselytize. Catholics, as well as some major Protestant denominations, provide help for the poor in the form of crop seeds, farm animals, medical care and education.
Health.
Epidemics of cholera, malaria, rabies and African hemorrhagic fevers like Marburg hemorrhagic fever, are common diseases in several parts of the country. Many regions in this country have high incidence rates of tuberculosis and high HIV prevalence rates. Dengue, filariasis, leishmaniasis and onchocerciasis (river blindness) are other diseases carried by insects that also occur in the region. Angola has one of the highest infant mortality rates in the world and one of the world's lowest life expectancies. A 2007 survey concluded that low and deficient niacin status was common in Angola. Demographic and Health Surveys is currently conducting several surveys in Angola on malaria, domestic violence and more.
In September 2014, the Angolan Institute for Cancer Control (IACC) was created by presidential decree, and it will integrate the National Health Service in Angola. The purpose of this new centre is to ensure health and medical care in oncology, policy implementation, programmes and plans for prevention and specialised treatment. This cancer institute will be assumed as a reference institution in the central and southern regions of Africa.
In 2014, Angola launched a national campaign of vaccination against measles, extended to every child under ten years old and aiming to go to all 18 provinces in the country. The measure is part of the Strategic Plan for the Elimination of Measles 2014–2020 created by the Angolan Ministry of Health which includes strengthening routine immunisation, a proper dealing with measles cases, national campaigns, introducing a second dose of vaccination in the national routine vaccination calendar and active epidemiological surveillance for measles. This campaign took place together with the vaccination against polio and vitamin A supplementation.
A yellow fever outbreak, the worst in the country in three decades began in December 2015. By August 2016, when the outbreak began to subside, nearly 4,000 people were suspected of being infected. As many as 369 may have died. The outbreak began in the capital, Luanda, and spread to at least 16 of the 18 provinces.
Education.
Although by law education in Angola is compulsory and free for eight years, the government reports that a percentage of pupils are not attending due to a lack of school buildings and teachers. Pupils are often responsible for paying additional school-related expenses, including fees for books and supplies.
In 1999, the gross primary enrollment rate was 74 per cent and in 1998, the most recent year for which data are available, the net primary enrollment rate was 61 per cent. Gross and net enrollment ratios are based on the number of pupils formally registered in primary school and therefore do not necessarily reflect actual school attendance. There continue to be significant disparities in enrollment between rural and urban areas. In 1995, 71.2 per cent of children ages 7 to 14 years were attending school. It is reported that higher percentages of boys attend school than girls. During the Angolan Civil War (1975–2002), nearly half of all schools were reportedly looted and destroyed, leading to current problems with overcrowding.
The Ministry of Education recruited 20,000 new teachers in 2005 and continued to implement teacher training. Teachers tend to be underpaid, inadequately trained and overworked (sometimes teaching two or three shifts a day). Some teachers may reportedly demand payment or bribes directly from their pupils. Other factors, such as the presence of landmines, lack of resources and identity papers, and poor health prevent children from regularly attending school. Although budgetary allocations for education were increased in 2004, the education system in Angola continues to be extremely under-funded.
According to estimates by the UNESCO Institute for Statistics, the adult literacy rate in 2011 was 70.4%. By 2015, this had increased to 71.1%. 82.9% of men and 54.2% of women are literate as of 2001. Since independence from Portugal in 1975, a number of Angolan students continued to be admitted every year at high schools, polytechnical institutes and universities in Portugal and Brazil through bilateral agreements; in general, these students belong to the elites.
In September 2014, the Angolan Ministry of Education announced an investment of 16 million Euros in the computerisation of over 300 classrooms across the country. The project also includes training teachers at a national level, "as a way to introduce and use new information technologies in primary schools, thus reflecting an improvement in the quality of teaching".
In 2010, the Angolan government started building the Angolan Media Libraries Network, distributed throughout several provinces in the country to facilitate the people's access to information and knowledge. Each site has a bibliographic archive, multimedia resources and computers with Internet access, as well as areas for reading, researching and socialising. The plan envisages the establishment of one media library in each Angolan province by 2017. The project also includes the implementation of several media libraries, in order to provide the several contents available in the fixed media libraries to the most isolated populations in the country. At this time, the mobile media libraries are already operating in the provinces of Luanda, Malanje, Uíge, Cabinda and Lunda South. As for REMA, the provinces of Luanda, Benguela, Lubango and Soyo have currently working media libraries.
Culture.
Angolan culture has been heavily influenced by Portuguese culture, especially in language and religion, and the culture of the indigenous ethnic groups of Angola, predominantly Bantu culture.
The diverse ethnic communities—the Ovimbundu, Ambundu, Bakongo, Chokwe, Mbunda and other peoples—to varying degrees maintain their own cultural traits, traditions and languages, but in the cities, where slightly more than half of the population now lives, a mixed culture has been emerging since colonial times; in Luanda, since its foundation in the 16th century.
In this urban culture, Portuguese heritage has become more and more dominant. African roots are evident in music and dance and is moulding the way in which Portuguese is spoken. This process is well reflected in contemporary Angolan literature, especially in the works of Angolan authors.
In 2014, Angola resumed the National Festival of Angolan Culture after a 25-year break. The festival took place in all the provincial capitals and lasted for 20 days, with the theme "Culture as a Factor of Peace and Development.
Cinema.
In 1972, one of Angola's first feature films, Sarah Maldoror's internationally co-produced "Sambizanga", was released at the Carthage Film Festival to critical acclaim, winning the "Tanit d'Or", the festival's highest prize.
Sports.
Basketball is the second most popular sport in Angola. Its national team has won the AfroBasket 11 times and holds the record of most titles. As a top team in Africa, it is a regular competitor at the Summer Olympic Games and the FIBA World Cup. Angola is home to one of Africa's first competitive leagues. Bruno Fernando, a player for the Atlanta Hawks, is the only current NBA player from Angola.
In football, Angola hosted the 2010 Africa Cup of Nations. The Angola national football team qualified for the 2006 FIFA World Cup, their first appearance in the World Cup finals. They were eliminated after one defeat and two draws in the group stage. They won three COSAFA Cups and finished runner-up in the 2011 African Nations Championship.
Angola has participated in the World Women's Handball Championship for several years. The country has also appeared in the Summer Olympics for seven years and both regularly competes in and once has hosted the FIRS Roller Hockey World Cup, where the best finish is sixth. Angola is also often believed to have historic roots in the martial art "Capoeira Angola" and "Batuque" which were practised by enslaved African Angolans transported as part of the Atlantic slave trade.

</doc>
<doc id="704" url="?curid=704" title="Demographics of Angola">
Demographics of Angola

 
Demographic features of the population of Angola include population density, ethnicity, education level, health of the populace, economic status, religious affiliations and other aspects.
According to 2014 census data, Angola had a population of 25,789,024 inhabitants in 2014.
Ethnically, there are three main groups, each speaking a Bantu language: the Ovimbundu who represent 37% of the population, the Ambundu with 25%, and the Bakongo 11%. Other numerically important groups include the closely interrelated Chokwe and Lunda, the Ganguela and Nyaneka-Khumbi (in both cases classification terms that stand for a variety of small groups), the Ovambo, the Herero, the Xindonga and scattered residual groups of San. In addition, mixed race (European and African) people amount to about 4%, with nearly 1% of the population being whites, mainly ethnically Portuguese.
As a former overseas territory of Portugal until 1975, Angola possesses a Portuguese population of over 200,000, a number that has been growing from 2000 onwards, because of Angola's growing demand for qualified human resources. Currently, over 300,000 Angolans are white, 1 million Angolans are mixed race (black and white) and 50,000 Angolans are from China, which accounts for 1.35 million people. In 1974, white Angolans made up a population of 350,000 people in an overall population of 6.3 million Angolans at that time. The only reliable source on these numbers is Gerald Bender &amp; Stanley Yoder, "Whites in Angola on the Eve of Independence: The Politics of Numbers", "Africa Today", 21 (4) 1974, pp. 23 – 37. Today, many Angolans who are not ethnic Portuguese can claim Portuguese nationality under Portuguese law. Estimates on the overall population are given in "O Pais". Besides the Portuguese, significant numbers of people from other European and from diverse Latin American countries (especially Brazil) can be found. From the 2000s, many Chinese have settled and started up small businesses, while at least as many have come as workers for large enterprises (construction or other). Observers claim that the Chinese community in Angola might include as many as 300,000 persons at the end of 2010, but reliable statistics are not at this stage available. In 1974/75, over 25,000 Cuban soldiers arrived in Angola to help the MPLA forces at the beginning of the Angolan Civil War. Once this was over, a massive development cooperation in the field of health and education brought in numerous civil personnel from Cuba. However, only a very small percentage of all these people has remained in Angola, either for personal reasons (intermarriage) or as professionals (e.g., medical doctors).
The largest religious denomination is Catholicism, to which adheres about half the population. Roughly 26% are followers of traditional forms of Protestantism (Congregationals, Methodists, Baptista, Lutherans, Reformed), but over the last decades there has in addition been a growth of Pentecostal communities and African Initiated Churches. In 2006, one out of 221 people were Jehovah's Witnesses. Africans from Mali, Nigeria and Senegal are mostly Sunnite Muslims, but do not make up more than 1 - 2% of the population. By now few Angolans retain African traditional religions following different ethnic faiths.
Population.
According to the 2022 revision of the world factbook the total population was 34,795,287 in 2022. The proportion of children below the age of 14 in 2020 was 47.83%, 49.87% was between 15 and 65 years of age, while 2.3% was 65 years or older.
Vital statistics.
Registration of vital events in Angola is not complete. The website Our World in Data prepared the following estimates based on statistics from the Population Department of the United Nations.
Fertility and Births.
Total Fertility Rate (TFR) (Wanted TFR) and Crude Birth Rate (CBR):
Other demographics statistics.
Demographic statistics according to the World Population Review in 2022.
The following demographic statistics are from the CIA World Factbook, unless otherwise indicated.
Population growth rate.
The population is growing by 3.52% annually. There are 44.2 births and 9.2 deaths per 1,000 citizens. The net migration rate is 0.2 migrants per 1,000 citizens. The fertility rate of Angola is 6.16 children born per woman as of 2017. The infant mortality rate is 67.6 deaths for every 1,000 live births with 73.3 deaths for males and 61.8 deaths for females for every 1,000 live births. Life expectancy at birth is 60.2 years; 58.2 years for males and 62.3 years for females.
Health.
According to the CIA World Factbook, 2% of adults (aged 15–49) are living with HIV/AIDS (as of 2009). The risk of contracting disease is very high. There are food and waterborne diseases, bacterial and protozoal diarrhea, hepatitis A, and typhoid fever; vectorborne diseases, malaria, African trypanosomiasis (sleeping sickness); respiratory disease: meningococcal meningitis, and schistosomiasis, a water contact disease, as of 2005.
Ethnic groups.
Roughly 37% of Angolans are Ovimbundu, 25% are Ambundu, 13% are Bakongo, 2% are mestiço, 1-2% are white Africans, and people from other African ethnicities make up 22% of Angola's population.
Romani people were deported to Angola from Portugal.
Religions.
Angola is a majority Christian country. Official statistics do not exist, however it is estimated that over 80% belong to a Christian church or community. More than half are Catholic, the remaining ones comprising members of traditional Protestant churches as well as of Pentecostal communities. Only 0.1% are Muslims - generally immigrants from other African countries. Traditional indigenous religions are practiced by a very small minority, generally in peripheral rural societies.
Education.
Literacy is quite low, with 71.1% of the population over the age of 15 able to read and write in Portuguese. 82% of males and 60.7% of women are literate as of 2015.
Languages.
Portuguese is the official language of Angola, but Bantu and other African languages are also widely spoken. In fact, Kikongo, Kimbundu, Umbundu, Tuchokwe, Ganguela, and Ukanyama have the official status of "national languages". The mastery of Portuguese is widespread; in the cities the overwhelming majority are either fluent in Portuguese or have at least a reasonable working knowledge of this language; an increasing minority are native Portuguese speakers and have a poor, if any, knowledge of an African language.
References.
Attribution:

</doc>
<doc id="705" url="?curid=705" title="Politics of Angola">
Politics of Angola

 
The current political regime in Angola is presidentialism, in which the President of the Republic is also head of state and government; it is advised by a Council of Ministers, which together with the President form the national executive power. Legislative power rests with the 220 parliamentarians elected to the National Assembly. The President of the Republic, together with the parliament, appoints the majority of the members of the two highest bodies of the judiciary, that is, the Constitutional Court and the Supreme Court. The judiciary is still made up of the Court of Auditors and the Supreme Military Court.
The Angolan government is composed of three branches of government: executive, legislative and judicial. For decades, political power has been concentrated in the presidency with the People's Movement for the Liberation of Angola.
History.
Since the adoption of a new constitution in 2010, the politics of Angola takes place in a framework of a presidential republic, whereby the President of Angola is both head of state and head of government, and of a multi-party system. Executive power is exercised by the government. Legislative power is vested in the President, the government and parliament.
Angola changed from a one-party Marxist-Leninist system ruled by the Popular Movement for the Liberation of Angola (MPLA), in place since independence in 1975, to a multiparty democracy based on a new constitution adopted in 1992. That same year the first parliamentary and presidential elections were held. The MPLA won an absolute majority in the parliamentary elections. In the presidential elections, President José Eduardo dos Santos won the first round election with more than 49% of the vote to Jonas Savimbi's 40%. A runoff election would have been necessary, but never took place. The renewal of civil war immediately after the elections, which were considered as fraudulent by UNITA, and the collapse of the Lusaka Protocol, created a split situation. To a certain degree the new democratic institutions worked, notably the National Assembly, with the active participation of UNITA's and the FNLA's elected MPs - while José Eduardo dos Santos continued to exercise his functions without democratic legitimation. However the armed forces of the MPLA (now the official armed forces of the Angolan state) and of UNITA fought each other until the leader of UNITA, Jonas Savimbi, was killed in action in 2002.
From 2002 to 2010, the system as defined by the constitution of 1992 functioned in a relatively normal way. The executive branch of the government was composed of the President, the Prime Minister and Council of Ministers. The Council of Ministers, composed of all ministers and vice ministers, met regularly to discuss policy issues. Governors of the 18 provinces were appointed by and served at the pleasure of the president. The Constitutional Law of 1992 established the broad outlines of government structure and the rights and duties of citizens. The legal system was based on Portuguese and customary law but was weak and fragmented. Courts operated in only 12 of more than 140 municipalities. A Supreme Court served as the appellate tribunal; a Constitutional Court with powers of judicial review was never constituted despite statutory authorization. In practice, power was more and more concentrated in the hands of the President who, supported by an ever-increasing staff, largely controlled parliament, government, and the judiciary.
The 26-year-long civil war has ravaged the country's political and social institutions. The UN estimates of 1.8 million internally displaced persons (IDPs), while generally the accepted figure for war-affected people is 4 million. Daily conditions of life throughout the country and specifically Luanda (population approximately 6 million) mirror the collapse of administrative infrastructure as well as many social institutions. The ongoing grave economic situation largely prevents any government support for social institutions. Hospitals are without medicines or basic equipment, schools are without books, and public employees often lack the basic supplies for their day-to-day work.
José Eduardo dos Santos stepped down as President of Angola after 38 years in 2017, being peacefully succeeded by João Lourenço, Santos' chosen successor. However, President João Lourenço started a campaign against corruption of the dos Santos era. In November 2017, Isabel dos Santos, the billionaire daughter of former President José Eduardo dos Santos, was fired from her position as head of the country's state oil company Sonangol. In August 2020, José Filomeno dos Santos, son of Angola's former president, was sentenced for five years in jail for fraud and corruption.
In August 2022, the ruling party, MPLA, won another outright majority and President Joao Lourenco won a second five-year term in the election. However, the election was the tightest in Angola’s history.
Executive branch.
The 2010 constitution grants the President almost absolute power. Elections for the National assembly are to take place every five years, and the President is automatically the leader of the winning party or coalition. It is for the President to appoint (and dismiss) all of the following:
The President is also provided a variety of powers, like defining the policy of the country. Even though it's not up to him/her to make laws (only to promulgate them and make edicts), the President is the leader of the winning party.
The only "relevant" post that is not directly appointed by the President is the Vice-President, which is the second in the winning party.
José Eduardo dos Santos stepped down as President of Angola after 38 years in 2017, being peacefully succeeded by João Lourenço, Santos' chosen successor.
Legislative branch.
The National Assembly ("Assembleia Nacional") has 223 members, elected for a four-year term, 130 members by proportional representation, 90 members in provincial districts, and 3 members to represent Angolans abroad. The general elections in 1997 were rescheduled for 5 September 2008. The ruling party MPLA won 82% (191 seats in the National Assembly) and the main opposition party won only 10% (16 seats). The elections however have been described as only partly free but certainly not fair. A White Book on the elections in 2008 lists up all irregularities surrounding the Parliamentary elections of 2008.
Judicial branch.
Supreme Court (or "Tribunal da Relacao") judges of the Supreme Court are appointed by the president. The Constitutional Court, with the power of judicial review, contains 11 justices. Four are appointed by the President, four by the National Assembly, two by the Superior Council of the Judiciary, and one elected by the public.
Administrative divisions.
Angola has eighteen provinces: Bengo, Benguela, Bie, Cabinda, Cuando Cubango, Cuanza Norte, Cuanza Sul, Cunene, Huambo, Huila, Luanda, Lunda Norte, Lunda Sul, Malanje, Moxico, Namibe, Uige, Zaire
Political pressure groups and leaders.
Front for the Liberation of the Enclave of Cabinda or FLEC (Henrique N'zita Tiago; António Bento Bembe)
International organization participation.
African, Caribbean and Pacific Group of States, AfDB, CEEAC, United Nations Economic Commission for Africa, FAO, Group of 77, IAEA, IBRD, ICAO, International Criminal Court (signatory), ICFTU, International Red Cross and Red Crescent Movement, International Development Association, IFAD, IFC, IFRCS, International Labour Organization, International Monetary Fund, International Maritime Organization, Interpol, IOC, International Organization for Migration, ISO (correspondent), ITU, Non-Aligned Council (temporary), UNCTAD, UNESCO, UNIDO, UPU, World Customs Organization, World Federation of Trade Unions, WHO, WIPO, WMO, WToO, WTrO

</doc>
<doc id="706" url="?curid=706" title="Economy of Angola">
Economy of Angola

The economy of Angola remains heavily influenced by the effects of four decades of conflict in the last part of the 20th century, the war for independence from Portugal (1961–75) and the subsequent civil war (1975–2002). Poverty since 2002 is reduced over 50% and a third of the population relies on subsistence agriculture. Since 2002, when the 27-year civil war ended, government policy prioritized the repair and improvement of infrastructure and strengthening of political and social institutions. During the first decade of the 21st century, Angola's economy was one of the fastest-growing in the world, with reported annual average GDP growth of 11.1 percent from 2001 to 2010. High international oil prices and rising oil production contributed to strong economic growth, although with high inequality, at that time. 2022 trade surplus was $30 billion, compared to $48 billion in 2012.
Corruption is rife throughout the economy and the country remains heavily dependent on the oil sector, which in 2017 accounted for over 90 percent of exports by value and 64 percent of government revenue. With the end of the oil boom, from 2015 Angola entered into a period of economic contraction.
History.
The Angolan economy has been dominated by the production of raw materials and the use of cheap labor since European rule began in the sixteenth century. The Portuguese used Angola principally as a source for the thriving slave trade across the Atlantic; Luanda became the greatest slaving port in Africa. After the Portuguese Empire abolished the slave trade in Angola in 1858, it began using concessional agreements, granting exclusive rights to a private company to exploit land, people, and all other resources within a given territory. In Mozambique, this policy spawned a number of companies notorious for their exploitation of local labor. But in Angola, only Diamang showed even moderate success. At the same time, Portuguese began emigrating to Angola to establish farms and plantations ("fazendas") to grow cash crops for export. Although these farms were only partially successful before World War II, they formed the basis for the later economic growth.
The principal exports of the post-slave economy in the 19th century were rubber, beeswax, and ivory. Prior to the First World War, exportation of coffee, palm kernels and oil, cattle, leather and hides, and salt fish joined the principal exports, with small quantities of gold and cotton also being produced. Grains, sugar, and rum were also produced for local consumption. The principal imports were foodstuffs, cotton goods, hardware, and British coal. Legislation against foreign traders was implemented in the 1890s. The territory's prosperity, however, continued to depend on plantations worked by labor "indentured" from the interior.
Before World War II, the Portuguese government was concerned primarily with keeping its colonies self-sufficient and therefore invested little capital in Angola's local economy. It built no roads until the mid-1920s, and the first railroad, the Benguela railway, was not completed until 1929. Between 1900 and 1940, only 35,000 Portuguese emigrants settled in Angola, and most worked in commerce in the cities, facilitating trade with Portugal. In the rural areas, Portuguese settlers often found it difficult to make a living because of fluctuating world prices for sugarcane and sisal and the difficulties in obtaining cheap labor to farm their crops. As a result, they often suspended their operations until the market prices rose and instead marketed the produce of Angolan farmers.
But in the wake of World War II, the rapid growth of industrialization worldwide and the parallel requirements for raw materials led Portugal to develop closer ties with its colonies and to begin actively developing the Angolan economy. In the 1930s, Portugal started to develop closer trade ties with its colonies, and by 1940 it absorbed 63 percent of Angolan exports and accounted for 47 percent of Angolan imports, up from 39 percent and 37 percent, respectively, a decade earlier. When the price of Angola's principal crops—coffee and sisal—jumped after the war, the Portuguese government began to reinvest some profits inside the country, initiating a series of projects to develop infrastructure. During the 1950s, Portugal built dams, hydroelectric power stations, and transportation systems. In addition, Portuguese citizens were encouraged to emigrate to Angola, where planned settlements ("colonatos") were established for them in the rural areas. Finally, the Portuguese initiated mining operations for iron ore, manganese, and copper to complement industrial activities at home, and in 1955 the first successful oil wells were drilled in Angola. By 1960 the Angolan economy had been completely transformed, boasting a successful commercial agricultural sector, a promising mineral and petroleum production enterprise, and an incipient manufacturing industry.
Yet by 1976, these encouraging developments had been reversed. The economy was in complete disarray in the aftermath of the war of independence and the subsequent internal fighting of the liberation movements. According to the ruling MPLA-PT, in August 1976 more than 80 percent of the agricultural plantations had been abandoned by their Portuguese owners; only 284 out of 692 factories continued to operate; more than 30,000 medium-level and high-level managers, technicians, and skilled workers had left the country; and 2,500 enterprises had been closed (75 percent of which had been abandoned by their owners). Furthermore, only 8,000 vehicles remained out of 153,000 registered, dozens of bridges had been destroyed, the trading network was disrupted, administrative services did not exist, and files and studies were missing.
Angola's economic ills can also be traced to the legacy of Portuguese colonial development. Many of the white settlers had come to Angola after 1950 and were understandably quick to repatriate during the war of independence. During their stay, however, these settlers had appropriated Angolan lands, disrupting local peasant production of cash and subsistence crops. Moreover, Angola's industries depended on trade with Portugal—the colony's overwhelmingly dominant trade partner—for both markets and machinery. Only the petroleum and diamond industries boasted a wider clientele for investment and markets. Most important, the Portuguese had not trained Angolans to operate the larger industrial or agricultural enterprises, nor had they actively educated the population. Upon independence Angola thus found itself without markets or expertise to maintain even minimal economic growth.
As a result, the government intervened, nationalizing most businesses and farms abandoned by the Portuguese. It established state farms to continue producing coffee, sugar, and sisal, and it took over the operations of all factories to maintain production. These attempts usually failed, primarily because of the lack of experienced managers and the continuing disruptions in rural areas caused by the UNITA insurgency. Only the petroleum sector continued to operate successfully, and by 1980 this sector had helped the gross domestic product reach US$3.6 billion, its highest level up to 1988. In the face of serious economic problems and the continuing war throughout the countryside, in 1987 the government announced plans to liberalize economic policies and promote private investment and involvement in the economy.
1990s.
United Nations Angola Verification Mission III and MONUA spent US$1.5 billion overseeing implementation of the Lusaka Protocol, a 1994 peace accord that ultimately failed to end the civil war. The protocol prohibited UNITA from buying foreign arms, a provision the United Nations largely did not enforce, so both sides continued to build up their stockpile. UNITA purchased weapons in 1996 and 1997 from private sources in Albania and Bulgaria, and from Zaire, South Africa, Republic of the Congo, Zambia, Togo, and Burkina Faso. In October 1997 the UN imposed travel sanctions on UNITA leaders, but the UN waited until July 1998 to limit UNITA's exportation of diamonds and freeze UNITA bank accounts. While the U.S. government gave US$250 million to UNITA between 1986 and 1991, UNITA made US$1.72 billion between 1994 and 1999 exporting diamonds, primarily through Zaire to Europe. At the same time the Angolan government received large amounts of weapons from the governments of Belarus, Brazil, Bulgaria, China, and South Africa. While no arms shipment to the government violated the protocol, no country informed the U.N. Register on Conventional Weapons as required.
Despite the increase in civil warfare in late 1998, the economy grew by an estimated 4% in 1999. The government introduced new currency denominations in 1999, including a 1 and 5 kwanza note.
2000s.
An economic reform effort was launched in 1998. Angola ranked 160 of 174 nations in the United Nations Human Development Index in 2000. In April 2000 Angola started an International Monetary Fund (IMF) Staff-Monitored Program (SMP). The program formally lapsed in June 2001, but the IMF remains engaged. In this context the Government of Angola has succeeded in unifying exchange rates and has raised fuel, electricity, and water rates. The Commercial Code, telecommunications law, and Foreign Investment Code are being modernized. A privatization effort, prepared with World Bank assistance, has begun with the BCI bank. Nevertheless, a legacy of fiscal mismanagement and corruption persists. The civil war internally displaced 3.8 million people, 32% of the population, by 2001. The security brought about by the 2002 peace settlement has led to the resettlement of 4 million displaced persons, thus resulting in large-scale increases in agriculture production.
Angola produced over of diamonds in 2003, and production was expected to grow to per year by 2007. In 2004, China's Eximbank approved a $2 billion line of credit to Angola to rebuild infrastructure. The economy grew 18% in 2005 and growth was expected to reach 26% in 2006 and stay above 10% for the rest of the decade. By 2020, Angola had a national debt of $76 billion, of which $20 billion is to China.
The construction industry is taking advantage of the growing economy, with various housing projects stimulated by the government such as the "Angola Investe" program and the "Casa Feliz" or "Meña" projects. Not all public construction projects are functional. For example, Kilamba Kiaxi, where a whole new satellite town of Luanda, consisting of housing facilities for several hundreds of thousands of people, was completely uninhabited for over four years because of skyrocketing prices, but completely sold out after the government decreased the original price and created mortgage plans at around the election time and thus made it affordable for middle-class people. ChevronTexaco started pumping from Block 14 in January 2000, but production decreased to in 2007 due to poor-quality oil. Angola joined the Organization of the Petroleum Exporting Countries on January 1, 2007. Cabinda Gulf Oil Company found Malange-1, an oil reservoir in Block 14, on August 9, 2007.
Overview.
Despite its abundant natural resources, output per capita is among the world's lowest. Subsistence agriculture provides the main livelihood for 85% of the population. Oil production and the supporting activities are vital to the economy, contributing about 45% to GDP and 90% of exports. Growth is almost entirely driven by rising oil production which surpassed in late-2005 and which is expected to grow to by 2007. Control of the oil industry is consolidated in Sonangol Group, a conglomerate owned by the Angolan government. With revenues booming from oil exports, the government has started to implement ambitious development programs to build roads and other basic infrastructure for the nation.
In the last decade of the colonial period, Angola was a major African food exporter but now imports almost all its food. Severe wartime conditions, including extensive planting of landmines throughout the countryside, have brought agricultural activities to a near-standstill. Some efforts to recover have gone forward, however, notably in fisheries. Coffee production, though a fraction of its pre-1975 level, is sufficient for domestic needs and some exports. Expanding oil production is now almost half of GDP and 90% of exports, at . Diamonds provided much of the revenue for Jonas Savimbi's UNITA rebellion through illicit trade. Other rich resources await development: gold, forest products, fisheries, iron ore, coffee, and fruits.
This is a chart of trend of nominal gross domestic product of Angola at market prices using International Monetary Fund data; figures are in millions of units.
The following table shows the main economic indicators in 1980–2023. Inflation below 5% is in green.
Agriculture.
Angola produced, in 2018:
In addition to smaller productions of other agricultural products, like coffee (16 thousand tons).
Foreign trade.
Exports in 2004 reached US$10,530,764,911. The vast majority of Angola's exports, 92% in 2004, are petroleum products. US$785 million worth of diamonds, 7.5% of exports, were sold abroad that year. Nearly all of Angola's oil goes to the United States, in 2006, making it the eighth largest supplier of oil to the United States, and to China, in 2006. In the first quarter of 2008, Angola became the main exporter of oil to China. The rest of its petroleum exports go to Europe and Latin America. U.S. companies account for more than half the investment in Angola, with Chevron-Texaco leading the way. The U.S. exports industrial goods and services, primarily oilfield equipment, mining equipment, chemicals, aircraft, and food, to Angola, while principally importing petroleum. Trade between Angola and South Africa exceeded US$300 million in 2007. From the 2000s, many Chinese have settled and started up businesses.
Resources.
Petroleum.
Angola produces and exports more petroleum than any other nation in sub-Saharan Africa, surpassing Nigeria first in the 2000s, then in 2022. In January 2007 Angola became a member of OPEC, before leaving in December 2023, as they wanted to expand their oil production. Under the Lourenço since 2017, the country has made efforts to incentive investments and reverse declining production, resulting in fresh investments made by international oil companies.
Chevron Corporation, TotalEnergies., ExxonMobil, Eni, and BP all operate in the country and represent a vast majority of daily production.
Block 17, operated by TotalEnergies, is Angola's biggest producing asset and is known as the Golden Block. The French major is currently executing several subsea tieback projects there, including CLOV 3 and Begonia, whose final investment decisions (FIDs) were taken in 2022.
The United Nations has criticized the Angolan government for using torture, rape, summary executions, arbitrary detention, and disappearances, actions which Angolan government has justified on the need to maintain oil output.
Angola is the third-largest trading partner of the United States in Sub-Saharan Africa, largely because of its petroleum exports. The U.S. imports 7% of its oil from Angola, about three times as much as it imported from Kuwait just prior to the Gulf War in 1991. The U.S. Government has invested US$4 billion in Angola's petroleum sector.
Oil makes up over 90% of Angola's exports.
Diamonds.
Angola is the third largest producer of diamonds in Africa and has only explored 40% of the diamond-rich territory within the country, but has had difficulty in attracting foreign investment because of corruption, human rights violations, and diamond smuggling. Production rose by 30% in 2006 and Endiama, the national diamond company of Angola, expects production to increase by 8% in 2007 to 10 million carats annually. The government is trying to attract foreign companies to the provinces of Bié, Malanje and Uíge.
The Angolan government loses $375 million annually from diamond smuggling. In 2003, the government began Operation Brilliant, an anti-smuggling investigation that arrested and deported 250,000 smugglers between 2003 and 2006. Rafael Marques, a journalist and human rights activist, described the diamond industry in his 2006 "Angola's Deadly Diamonds" report as plagued by "murders, beatings, arbitrary detentions and other human rights violations." Marques called on foreign countries to boycott Angola's "conflict diamonds". In December 2014, the Bureau of International Labor Affairs issued a "List of Goods Produced by Child Labor or Forced Labor" that classified Angola as one of the major diamond-producing African countries relying on both child labor and forced labor. The U.S. Department of Labor reported that "there is little publicly available information on [Angola's] efforts to enforce child labor law". Diamonds accounted for 1.48% of Angolan exports in 2014.
Iron.
Under Portuguese rule, Angola began mining iron in 1957, producing 1.2 million tons in 1967 and 6.2 million tons by 1971. In the early 1970s, 70% of Portuguese Angola's iron exports went to Western Europe and Japan. After independence in 1975, the Angolan Civil War (1975–2002) destroyed most of the territory's mining infrastructure. The redevelopment of the Angolan mining industry started in the late 2000s.

</doc>
<doc id="708" url="?curid=708" title="Transport in Angola">
Transport in Angola

 
Transport in Angola comprises:
Roads.
Two trans-African automobile routes pass through Angola:
Railways.
There are three separate railway lines in Angola:
Reconstruction of these three lines began in 2005 and they are now all operational. The Benguela Railway connects to the Democratic Republic of the Congo.
Pipelines.
In April 2012, the Zambian Development Agency (ZDA) and an Angolan company signed a memorandum of understanding (MoU) to build a multi-product pipeline from Lobito to Lusaka, Zambia, to deliver various refined products to Zambia.
Angola plans to build an oil refinery in Lobito in the coming years.
Ports and harbors.
The government plans to build a deep-water port at Barra do Dande, north of Luanda, in Bengo province near Caxito.
Airports.
Heliports.
International and domestic services are maintained by TAAG Angola Airlines, Turkish Airlines, Aeroflot, British Airways, Brussels Airlines, Lufthansa, Air France, Cubana, Ethiopian Airlines, Emirates, Delta Air Lines, Royal Air Maroc, Iberia, Hainan Airlines, Kenya Airways, South African Airways, TAP Air Portugal and several regional carriers. There are airstrips at Benguela, Cabinda, Huambo, Moçâmedes, and Catumbela.

</doc>
<doc id="709" url="?curid=709" title="Angolan Armed Forces">
Angolan Armed Forces

The Angolan Armed Forces () or FAA is the military of Angola. The FAA consist of the Angolan Army (), the Angolan Navy () and the National Air Force of Angola (). Reported total manpower in 2021 was about 107,000. The FAA is headed by the Chief of the General Staff António Egídio de Sousa Santos since 2018, who reports to the minister of National Defense, currently João Ernesto dos Santos.
History.
Roots.
The FAA succeeded to the previous People's Armed Forces for the Liberation of Angola (FAPLA) following the abortive Bicesse Accord with the Armed Forces of the Liberation of Angola (FALA), armed wing of the National Union for the Total Independence of Angola (UNITA). As part of the peace agreement, troops from both armies were to be demilitarized and then integrated. Integration was never completed as UNITA and FALA went back to war in 1992. Later, consequences for FALA personnel in Luanda were harsh with FAPLA veterans persecuting their erstwhile opponents in certain areas and reports of vigilantism.
Founding.
The Angolan Armed Forces were created on 9 October 1991. The institutionalization of the FAA was made in the Bicesse Accords, signed in 1991, between the Angolan Government and UNITA. The principles that would govern the FAA were defined in a joint proposal presented on September 24, 1991 and approved on 9 October. On 14 November 1991, Generals João Baptista de Matos and Abílio Kamalata Numa were appointed to the Superior Command of the Armed Forces. The ceremony took place at the Hotel Presidente Luanda, and was presided over by the then-minister França Vandúnem.
Branches.
Army.
The Army ("Exército") is the land component of the FAA. It is organized in six military regions (Cabinda, Luanda, North, Center, East and South), with an infantry division being based in each one. Distributed by the six military regions / infantry divisions, there are 25 motorized infantry brigades, one tank brigade and one engineering brigade. The Army also includes an artillery regiment, the Military Artillery School, the Army Military Academy, an anti-aircraft defense group, a composite land artillery group, a military police regiment, a logistical transportation regiment and a field artillery brigade. The Army further includes the Special Forces Brigade (including Commandos and Special Operations units), but this unit is under the direct command of the General Staff of the FAA.
Air Force.
The National Air Force of Angola (FANA, "Força Aérea Nacional de Angola") is the air component of the FAA. It is organized in six aviation regiments, each including several squadrons. To each of the regiments correspond an air base. Besides the aviation regiments, there is also a Pilot Training School.
The Air Force's personnel total about 8,000; its equipment includes transport aircraft and six Russian-manufactured Sukhoi Su-27 fighter aircraft. In 2002, one was lost during the civil war with UNITA forces.
In 1991, the Air Force/Air Defense Forces had 8,000 personnel and 90 combat-capable aircraft, including 22 fighters, 59 fighter ground attack aircraft and 16 attack helicopters.
Navy.
The Angola Navy (MGA, "Marinha de Guerra de Angola") is the naval component of the FAA. It is organized in two naval zones (North and South), with naval bases in Luanda, Lobito and Moçâmedes. It includes a Marines Brigade and a Marines School, based in Ambriz. The Navy numbers about 1,000 personnel and operates only a handful of small patrol craft and barges.
The Navy has been neglected and ignored as a military arm mainly due to the guerrilla struggle against the Portuguese and the nature of the civil war. From the early 1990s to the present the Angolan Navy has shrunk from around 4,200 personnel to around 1,000, resulting in the loss of skills and expertise needed to maintain equipment. Portugal has been providing training through its Technical Military Cooperation (CTM) programme. The Navy is requesting procurement of a frigate, three corvettes, three offshore patrol vessel and additional fast patrol boats.
Most of the vessels in the navy's inventory dates back from the 1980s or earlier, and many of its ships are inoperable due to age and lack of maintenance. However the navy acquired new boats from Spain and France in the 1990s. Germany has delivered several Fast Attack Craft for border protection in 2011.
In September 2014 it was reported that the Angolan Navy would acquire seven Macaé-class patrol vessels from Brazil as part of a Technical Memorandum of Understanding (MoU) covering the production of the vessels as part of Angola's Naval Power Development Programme (Pronaval). The military of Angola aims to modernize its naval capability, presumably due to a rise in maritime piracy within the Gulf of Guinea which may have an adverse effect on the country's economy.
The navy's current known inventory includes the following:
The navy also has several aircraft for maritime patrol:
Specialized units.
Special forces.
The FAA include several types of special forces, namely the Commandos, the Special Operations and the Marines. The Angolan special forces follow the general model of the analogous Portuguese special forces, receiving a similar training.
The Commandos and the Special forces are part of the Special Forces Brigade (BRIFE, "Brigada de Forças Especiais"), based at Cabo Ledo, in the Bengo Province. The BRIFE includes two battalions of commandos, a battalion of special operations and sub-units of combat support and service support. The BRIFE also included the Special Actions Group (GAE, "Grupo de Ações Especiais"), which is presently inactive and that was dedicated to long range reconnaissance, covert and sabotage operations. In the Cabo Ledo base is also installed the Special Forces Training School (EFFE, "Escola de Formação de Forças Especiais"). Both the BRIFE and the EFFE are directly under the Directorate of Special Forces of the General Staff of the Armed Forces.
The marines ("fuzileiros navais") constitute the Marines Brigade of the Angolan Navy. The Marines Brigade is not permanently dependent of the Directorate of Special Forces, but can detach their units and elements to be put under the command of that body for the conduction of exercises or real operations.
Since the disbandment of the Angolan Parachute Battalion in 2004, the FAA do not have a specialized paratrooper unit. However, elements of the commandos, special operations and marines are parachute qualified.
Territorial troops.
The Directorate of People's Defense and Territorial Troops of the Defence Ministry or ODP was established in late 1975. It had 600,000 members, having personnel in virtually every village by 1979. It had both armed and unarmed units dispersed in villages throughout the country. The People's Vigilance Brigades () also serve a similar purpose.
Training establishments.
Armed Forces Academy.
The Military Academy () is a military university public higher education establishment whose mission is to train officers of the Permanent Staff of the Army. It has been in operation since 21 August 2009 by presidential decree. Its headquarters are in Lobito. It trains in the following specialties:
Institutions/other units.
Military Hospitals.
The Military hospital of the FAA is the Main Military Hospital. It has the following lineage:
It provides specialized medical assistance in accordance with the military health system; It also promotes post-graduate education and scientific research. Currently, the Main Military Hospital serves 39 special medical specialties. It is a headed by a Director General whose main supporting body is the Board of Directors.
Supreme Military Court.
The Supreme Military Court is the highest organ of the hierarchy of military courts. The Presiding Judge, the Deputy Presiding Judge and the other Counselor Judges of the Supreme Military Court are appointed by the President of the Republic. The composition, organization, powers and functioning of the Supreme Military Court are established by law.
Military Bands.
The FAA maintains Portuguese-style military bands in all three branches and in individual units. The primary band is the 100-member Music Band of the Presidential Security Household. The music band of the Army Command was created on 16 June 1994 and four years later, on 15 August 1998, the National Air Force created a music band within an artistic brigade. The navy has its own marching band, as well as a small musical group known as "Banda 10 de Julho" (10th July Band), based at the Luanda Naval Base.
Foreign deployments.
The FAPLA's main counterinsurgency effort was directed against UNITA in the southeast, and its conventional capabilities were demonstrated principally in the undeclared South African Border War. The FAPLA first performed its external assistance mission with the dispatch of 1,000 to 1,500 troops to São Tomé and Príncipe in 1977 to bolster the socialist regime of President Manuel Pinto da Costa. During the next several years, Angolan forces conducted joint exercises with their counterparts and exchanged technical operational visits. The Angolan expeditionary force was reduced to about 500 in early 1985.
The Angolan Armed Forces were controversially involved in training the armed forces of fellow Lusophone states Cape Verde and Guinea-Bissau. In the case of the latter, the 2012 Guinea-Bissau coup d'état was cited by the coup leaders as due to Angola's involvement in trying to "reform" the military in connivance with the civilian leadership.
Occasionally skirmishes on the DRC-Angola border happening ,sometimes also in connection with the Cabinda conflict. In 2020 one Angolan soldier died after a gun battle with congolese forces in Kasai region on DRC territory. A presence during the unrest in Ivory Coast, 2010–2011, were not officially confirmed. However, the , citing "Jeune Afrique", said that among President Gbagbo's guards were 92 personnel of President Dos Santos's Presidential Guard Unit. Angola is basically interested in the participation of the FAA operations of the African Union and has formed special units for this purpose.
In 2021, the Angolan Parliament approved integration of FAA into Southern African Development Community (SADC)'s mission for peace in Cabo Delgado, Mozambique. Angola sent a team of 20 officers to participate.
Further reading.
David Birmingham, African Affairs, Vol. 77, No. 309 (Oct. 1978), pp. 554–564
Published by: Oxford University Press on behalf of The Royal African Society

</doc>
<doc id="710" url="?curid=710" title="Foreign relations of Angola">
Foreign relations of Angola

The foreign relations of Angola are based on Angola's strong support of U.S. foreign policy as the Angolan economy is dependent on U.S. foreign aid.
From 1975 to 1989, Angola was aligned with the Eastern bloc, in particular the Soviet Union, Libya, and Cuba. Since then, it has focused on improving relationships with Western countries, cultivating links with other Portuguese-speaking countries, and asserting its own national interests in Central Africa through military and diplomatic intervention. In 1993, it established formal diplomatic relations with the United States. It has entered the Southern African Development Community as a vehicle for improving ties with its largely Anglophone neighbors to the south. Zimbabwe and Namibia joined Angola in its military intervention in the Democratic Republic of the Congo, where Angolan troops remain in support of the Joseph Kabila government. It also has intervened in the Republic of the Congo (Brazzaville) in support of Denis Sassou-Nguesso in the civil war.
Since 1998, Angola has successfully worked with the United Nations Security Council to impose and carry out sanctions on UNITA. More recently, it has extended those efforts to controls on conflict diamonds, the primary source of revenue for UNITA during the Civil War that ended in 2002. At the same time, Angola has promoted the revival of the Community of Portuguese-Speaking Countries (CPLP) as a forum for cultural exchange and expanding ties with Portugal (its former ruler) and Brazil (which shares many cultural affinities with Angola) in particular. Angola is a member of the Port Management Association of Eastern and Southern Africa (PMAESA).
Diplomatic relations.
List of countries which Angola maintains diplomatic relations with:

</doc>
<doc id="711" url="?curid=711" title="Albert Sidney Johnston">
Albert Sidney Johnston

Albert Sidney Johnston (February 2, 1803 – April 6, 1862) was an American military officer who served as a general in three different armies: the Texian Army, the United States Army, and the Confederate States Army. He saw extensive combat during his 34-year military career, fighting actions in the Black Hawk War, the Texas-Indian Wars, the Mexican–American War, the Utah War, and the American Civil War.
Considered by Confederate States President Jefferson Davis to be the finest general officer in the Confederacy before the later emergence of Robert E. Lee, he was killed early in the Civil War at the Battle of Shiloh on April 6, 1862. Johnston was the highest-ranking officer on either side killed during the entire war. Davis believed the loss of General Johnston "was the turning point of our fate."
Johnston was unrelated to Confederate general Joseph E. Johnston.
Early life and education.
Johnston was born in Washington, Kentucky, the youngest son of Dr. John and Abigail (Harris) Johnston. His father was a native of Salisbury, Connecticut. Although Albert Johnston was born in Kentucky, he lived much of his life in Texas, which he considered his home. He was first educated at Transylvania University in Lexington, Kentucky, where he met fellow student Jefferson Davis. Both were appointed to the United States Military Academy at West Point, New York, Davis two years behind Johnston. In 1826, Johnston graduated eighth of 41 cadets in his class from West Point with a commission as a brevet second lieutenant in the 2nd U.S. Infantry.
Johnston was assigned to posts in New York and Missouri. In August 1827 he participated in the expedition to capture Red Bird, the rebellious Winnebago chief. Johnston later wrote: "I must confess that I consider Red Bird one of the noblest and most dignified men I ever saw... He said: 'I have offended. I sacrifice myself to save my country.'" Johnston served in the brief Black Hawk War of 1832 as chief of staff to Brevet Brigadier General Henry Atkinson. The commander praised Johnston for "talents of the first order, a gallant soldier by profession and education and a gentleman of high standing and integrity."
Marriage and family.
In 1829, he married Henrietta Preston, sister of Kentucky politician and future Civil War general William Preston. They had three children, of whom two survived to adulthood. Their son, William Preston Johnston, became a colonel in the Confederate States Army. The senior Johnston resigned his commission in 1834 to care for his dying wife in Kentucky, who succumbed two years later to tuberculosis.
After serving as Secretary of War for the Republic of Texas in 1838–40, Johnston resigned and went back to Kentucky. In 1843, he married Eliza Griffin, his late wife's first cousin. The couple moved to Texas, where they settled on a large plantation in Brazoria County. Johnston named the property "China Grove". Here they raised Johnston's two children from his first marriage and the first three children born to Eliza and him. A sixth child was born later when the family lived in Los Angeles, where they had permanently settled.
Texian Army.
Johnston moved to Texas in 1836 and enlisted as a private in the Texian Army after the Texas War of Independence from the Republic of Mexico. He was named Adjutant General as a colonel in the Republic of Texas Army on August 5, 1836. On January 31, 1837, he became senior brigadier general in command of the Texas Army.
On February 5, 1837, Johnston fought in a duel with Texas Brigadier General Felix Huston, who was angered and offended by Johnston's promotion. Huston had been the acting commander of the army and perceived Johnston's appointment as a slight from the Texas government. Johnston was shot through the hip and severely wounded, requiring him to relinquish his post during his recovery.
Afterwards, Johnston said he fought Huston "as a public duty... he had but little respect for the practice of dueling." He believed that the "safety of the republic depended upon the efficiency of the army... and upon the good discipline and subordination of the troops, which could only be secured by their obedience to their legal commander. General Huston embodied the lawless spirit in the army, which had to be met and controlled at whatever personal peril."
Many years later, Huston said that the duel was "a shameful piece of business, and I wouldn't do it again under any circumstances... Why, when I reflect upon the circumstances, I hate myself... that one act blackened all the good ones of my life. But I couldn't challenge Congress; and President Houston, although a duelist, was too far above me in rank. Well, thank God, I didn't kill him."
On December 22, 1838, Mirabeau B. Lamar, the second president of the Republic of Texas, appointed Johnston as Secretary of War. He defended the Texas border against Mexican attempts to recover the state in rebellion. In 1839, he campaigned against Native Americans in northern Texas during the Cherokee War of 1838–39. At the Battle of the Neches, Johnston and Vice President David G. Burnet were both cited in the commander's report "for active exertions on the field" and "having behaved in such a manner as reflects great credit upon themselves." In February 1840, he resigned and returned to Kentucky.
United States Army.
When the United States declared war on Mexico in May 1846, Johnston rode 400 miles from his home in Galveston to Port Isabel to volunteer for service in Brigadier General Zachary Taylor's Army of Occupation. Johnston was elected as colonel of the 1st Texas Rifle Volunteers but the enlistments of his soldiers ran out just before the army's advance on Monterrey, so Taylor appointed him as the inspector general of Brigadier General William O. Butler's division of volunteers. Johnston convinced a few volunteers of his former regiment to stay on and fight.
During the Battle of Monterrey, Butler was wounded and carried to the rear, and Johnston assumed an active leadership role in the division. Future U.S. general, Joseph Hooker, was with Johnston at Monterrey and wrote: "It was through [Johnston's] agency, mainly, that our division was saved from a cruel slaughter... The coolness and magnificent presence [that he] displayed on this field... left an impression on my mind that I have never forgotten." General Taylor considered Johnston "the best soldier he had ever commanded."
Johnston resigned from the army just after the battle of Monterrey in October 1846. He had promised his wife, Eliza, that he would only volunteer for six months' service. In addition, President James K. Polk's administration's preference for officers associated with the Democratic Party prevented the promotion of those, such as Johnston, who were perceived as Whigs:
He remained on his plantation after the war until he was appointed by later 12th president Zachary Taylor to the U.S. Army as a major and was made a paymaster in December 1849 for a district of Texas encompassing the military posts from the upper Colorado River to the upper Trinity River. He served in that role for more than five years, making six tours and traveling more than annually on the Indian frontier of Texas. He served on the Texas frontier at Fort Mason and elsewhere in the western United States.
In 1855, 14th president Franklin Pierce appointed him colonel of the new 2nd U.S. Cavalry (the unit that preceded the modern 5th U.S.), a new regiment, which he organized, his lieutenant colonel being Robert E. Lee, and his majors William J. Hardee and George H. Thomas. Other subordinates in this unit included Earl Van Dorn, Edmund Kirby Smith, Nathan G. Evans, Innis N. Palmer, George Stoneman, R.W. Johnson, John B. Hood, and Charles W. Field, all future Civil War generals. On March 31, 1856, Johnston received a promotion to temporary command of the entire Department of Texas. He campaigned aggressively against the Comanche, writing to his daughter that "the Indians harass our frontiers and the 2nd Cavalry and other troops thrash them wherever they catch them." In March 1857, Brigadier General David E. Twiggs was appointed permanent commander of the department and Johnston returned to his position as colonel of the 2nd Cavalry.
Utah War.
As a key figure in the Utah War, Johnston took command of the U.S. forces dispatched to crush the Mormon rebellion in November 1857. Their objective was to install Alfred Cumming as governor of the Utah Territory, replacing Brigham Young, and restore U.S. legal authority in the region. As Johnston had replaced Brigadier General William S. Harney in command, he only joined the army after it had already departed for Utah. Johnston's adjutant general, and future U.S. general in the Civil War, Major Fitz John Porter wrote: "Experienced on the Plains and of established reputation for energy, courage, and resources, [Johnston's] presence restored confidence at all points, and encouraged the weak-hearted and panic-stricken multitude. The long chain of wagons, kinked, tangled, and hard to move, uncoiled and went forward smoothly."
Johnston worked tirelessly over the next few months to maintain the effectiveness of his army in the harsh winter environment at Fort Bridger, Wyoming. Major Porter wrote to an associate: "Col. Johnston has done everything to add to the efficiency of the command – and put it in a condition to sustain the dignity and honor of the country – More he cannot do… Don't let any one come here over Col. Johnston – It would be much against the wishes and hopes of everyone here – who would gladly see him a Brigadier General." Even the Mormons commended Johnston's actions, with the Salt Lake City "Deseret News" reporting that "It takes a cool brain and good judgment to maintain a contented army and healthy camp through a stormy winter in the Wasatch Mountains."
Johnston and his troops hoped for war. They had learned of the Mountain Meadows Massacre and wanted revenge against the Mormons. However, a peaceful resolution was reached after the army had endured the harsh winter at Fort Bridger. In late June 1858, Johnston led the army through Salt Lake City without incident to establish Camp Floyd some 50 miles distant. In a report to the War Department, Johnston reported that "horrible crimes… have been perpetrated in this territory, crimes of a magnitude and of an apparently studied refinement in atrocity, hardly to be conceived of, and which have gone unwhipped of justice." Nevertheless, Johnston's army peacefully occupied the Utah Territory. U.S. Army Commander-in-Chief, Major General Winfield Scott, was delighted with Johnston's performance during the campaign and recommended his promotion to brevet brigadier general: "Colonel Johns[t]on is more than a good officer – he is a God send to the country thro' the army." The Senate confirmed Johnston's promotion on March 24, 1858.
With regard to the relations established by Johnston with the Native American tribes of the area, Major Porter reported that "Colonel Johnston took every occasion to bring the Indians within knowledge and influence of the army, and induced numerous chiefs to come to his camp... Colonel Johnston was ever kind, but firm, and dignified to them... The Utes, Pi-Utes, Bannocks, and other tribes, visited Colonel Johnston, and all went away expressing themselves pleased, assuring him that so long as he remained they would prove his friends, which the colonel told them would be best for them. Thus he effectively destroyed all influence of the Mormons over them, and insured friendly treatment to travelers to and from California and Oregon."
In August 1859, parts of Johnston's Army of Utah were implicated as participants in an alleged massacre at Spring Valley, a retaliation against an Indian massacre of an emigrant train to California. There are conflicting reports of the event and Johnston only referenced it in a November 1859 report to Scott. He wrote: "I have ascertained that three [emigrant] parties were robbed, and ten or twelve of their members, comprising men, women, and children, murdered... The perpetrators of the robbery of the first party were severely chastised by a detachment of dragoons, under the command of Lieutenant Gay. The troops failed to discover the robbers of the last two parties that were attacked. They are supposed to be vagabonds from the Shoshonee (sic) or Snake and Bannack (sic) Indians, whose chiefs deny any complicity with these predatory bands. There is abundant evidence to prove that these robber bands are accompanied by white men, and probably instigated and led by them. On that account I am inclined to believe the disclaimer of the Indians referred to, of having any knowledge of the robberies or any share in the plunder." The only evidence of the massacre is the account of Elijah Nicholas Wilson (written in 1910, about 51 years after the incident) and oral histories.
In late February 1860, Johnston received orders from the War Department recalling him to Washington D.C. to prepare for a new assignment. He spent 1860 in Kentucky until December 21, when he sailed for California to take command of the Department of the Pacific.
Slavery.
Johnston was a slave owner and a strong supporter of slavery. By 1846, he owned four slaves in Texas. In 1855, having discovered that a slave was stealing from the Army payroll, Johnston refused to have him physically punished and instead sold him for $1,000 to recoup the losses. Johnston explained that "whipping will not restore what is lost and it will not benefit the [culprit], whom a lifetime of kind treatment has failed to make honest." In 1856, he called abolitionism "fanatical, idolatrous, negro worshipping" in a letter to his son, fearing that the abolitionists would incite a slave revolt in the Southern states. Upon moving to California, Johnston sold one slave to his son and freed another, Randolph or "Ran", who agreed to accompany the family on the condition of a $12/month contract for five more years of servitude. Ran accompanied Johnston throughout the American Civil War until the latter's death. Johnston's wife Eliza celebrated the absence of blacks in California, writing, "where the darky is in any numbers it should be as slaves."
American Civil War.
At the outbreak of the American Civil War, Johnston was the commander of the U.S. Army Department of the Pacific in California. Like many regular army officers from the Southern United States, he opposed secession. Nevertheless, Johnston resigned his commission soon after he heard of the Confederate states' declarations of secession. The War Department accepted it on May 6, 1861, effective May 3. On April 28, he moved to Los Angeles, the home of his wife's brother John Griffin. Considering staying in California with his wife and five children, Johnston remained there until May. A sixth child was born in the family home in Los Angeles. His eldest son, Capt. Albert S. Johnston, Jr. was later killed in an accidental explosion on a steamer ship while on liberty in Los Angeles in 1863.
Soon, Johnston enlisted in the Los Angeles Mounted Rifles (a pro-Southern militia unit) as a private, leaving Warner's Ranch on May 27. He participated in their trek across the Southwestern deserts to Texas, crossing the Colorado River into the Confederate Territory of Arizona on July 4, 1861. His escort was commanded by Alonzo Ridley, Undersheriff of Los Angeles, who remained at Johnston's side until he was killed.
Early in the Civil War, Confederate President Jefferson Davis decided that the Confederacy would attempt to hold as much territory as possible, distributing military forces around its borders and coasts. In the summer of 1861, Davis appointed several generals to defend Confederate lines from the Mississippi River east to the Allegheny Mountains. Aged 58 when the war began, Johnston was old by Army standards. He came east to offer his service for the Confederacy without having been promised anything, merely hoping for an assignment.
The most sensitive, and in many ways, the most crucial areas, along the Mississippi River and in western Tennessee along the Tennessee and the Cumberland rivers were placed under the command of Maj. Gen. Leonidas Polk and Brig. Gen. Gideon J. Pillow. The latter had initially been in command in Tennessee as that State's top general. Their impolitic occupation of Columbus, Kentucky, on September 3, 1861, two days before Johnston arrived in the Confederacy's capital of Richmond, Virginia, after his cross-country journey, drove Kentucky from its stated neutrality. The majority of Kentuckians allied with the U.S. camp. Polk and Pillow's action gave U.S. Brig. Gen. Ulysses S. Grant an excuse to take control of the strategically located town of Paducah, Kentucky, without raising the ire of most Kentuckians and the pro-U.S. majority in the State legislature.
Confederate command in Western Theater.
On September 10, 1861, Johnston was assigned to command the huge area of the Confederacy west of the Allegheny Mountains, except for coastal areas. He became commander of the Confederacy's western armies in the area often called the Western Department or Western Military Department. Johnston's appointment as a full general by his friend and admirer Jefferson Davis had already been confirmed by the Confederate Senate on August 31, 1861. The appointment had been backdated to rank from May 30, 1861, making him the second-highest-ranking general in the Confederate States Army. Only Adjutant General and Inspector General Samuel Cooper ranked ahead of him. After his appointment, Johnston immediately headed for his new territory. He was permitted to call on Arkansas, Tennessee, and Mississippi governors for new troops. However, politics largely stifled this authority, especially concerning Mississippi. On September 13, 1861, Johnston ordered Brig. Gen. Felix Zollicoffer with 4,000 men to occupy Cumberland Gap in Kentucky to block U.S. troops from coming into eastern Tennessee. The Kentucky legislature had voted to side with the United States after the occupation of Columbus by Polk. By September 18, Johnston had Brig. Gen. Simon Bolivar Buckner with another 4,000 men blocking the railroad route to Tennessee at Bowling Green, Kentucky.
Johnston had fewer than 40,000 men spread throughout Kentucky, Tennessee, Arkansas, and Missouri. Of these, 10,000 were in Missouri under Missouri State Guard Maj. Gen. Sterling Price. Johnston did not quickly gain many recruits when he first requested them from the governors, but his more serious problem was lacking sufficient arms and ammunition for the troops he already had. As the Confederate government concentrated efforts on the units in the East, they gave Johnston small numbers of reinforcements and minimal amounts of arms and material. Johnston maintained his defense by conducting raids and other measures to make it appear he had larger forces than he did, a strategy that worked for several months. Johnston's tactics had so annoyed and confused U.S. Brig. Gen. William Tecumseh Sherman in Kentucky that he became paranoid and mentally unstable. Sherman overestimated Johnston's forces and was relieved by Brig. Gen. Don Carlos Buell on November 9, 1861. However, in his "Memoirs", Sherman strongly rebutted this account.
Battle of Mill Springs.
East Tennessee (a heavily pro-union region of the southern U.S. during the Civil War) was occupied for the Confederacy by two unimpressive brigadier generals appointed by Jefferson Davis: Felix Zollicoffer, a brave but untrained and inexperienced officer, and soon-to-be Maj. Gen. George B. Crittenden, a former U.S. Army officer with apparent alcohol problems. While Crittenden was away in Richmond, Zollicoffer moved his forces to the north bank of the upper Cumberland River near Mill Springs (now Nancy, Kentucky), putting the river to his back and his forces into a trap. Zollicoffer decided it was impossible to obey orders to return to the other side of the river because of the scarcity of transport and proximity of U.S. troops. When U.S. Brig. Gen. George H. Thomas moved against the Confederates, Crittenden decided to attack one of the two parts of Thomas's command at Logan's Cross Roads near Mill Springs before the U.S. forces could unite. At the Battle of Mill Springs on January 19, 1862, the ill-prepared Confederates, after a night march in the rain, attacked the U.S. soldiers with some initial success. As the battle progressed, Zollicoffer was killed and the Confederates were turned back and routed by a U.S. bayonet charge, suffering 533 casualties from their force of 4,000 while Crittenden's conduct in the battle was so inept that subordinates accused him of being drunk. The Confederate troops who escaped were assigned to other units as General Crittenden faced an investigation of his conduct.
After the Confederate defeat at Mill Springs, Davis sent Johnston a brigade and a few other scattered reinforcements. He also assigned him Gen. P. G. T. Beauregard, who was supposed to attract recruits because of his victories early in the war and act as a competent subordinate for Johnston. The brigade was led by Brig. Gen. John B. Floyd, considered incompetent. He took command at Fort Donelson as the senior general present just before U.S. Brig. Gen. Ulysses S. Grant attacked the fort. Historians believe the assignment of Beauregard to the west stimulated U.S. commanders to attack the forts before Beauregard could make a difference in the theater. U.S. Army officers heard that he was bringing 15 regiments with him, but this was an exaggeration of his forces.
Fort Henry, Fort Donelson, Nashville.
Based on the assumption that Kentucky neutrality would act as a shield against a direct invasion from the north, circumstances that no longer applied in September 1861, Tennessee initially had sent men to Virginia and concentrated defenses in the Mississippi Valley. Even before Johnston arrived in Tennessee, construction of two forts had been started to defend the Tennessee and the Cumberland rivers, which provided avenues into the State from the north. Both forts were located in Tennessee to respect Kentucky neutrality, but these were not in ideal locations. Fort Henry on the Tennessee River was in an unfavorable low-lying location, commanded by hills on the Kentucky side of the river. Fort Donelson on the Cumberland River, although in a better location, had a vulnerable land side and did not have enough heavy artillery to defend against gunboats.
Maj. Gen. Polk ignored the problems of the forts when he took command. After Johnston took command, Polk at first refused to comply with Johnston's order to send an engineer, Lt. Joseph K. Dixon, to inspect the forts. After Johnston asserted his authority, Polk had to allow Dixon to proceed. Dixon recommended that the forts be maintained and strengthened, although they were not in ideal locations, because much work had been done on them, and the Confederates might not have time to build new ones. Johnston accepted his recommendations. Johnston wanted Major Alexander P. Stewart to command the forts, but President Davis appointed Brig. Gen. Lloyd Tilghman as commander.
To prevent Polk from dissipating his forces by allowing some men to join a partisan group, Johnston ordered him to send Brig. Gen. Gideon Pillow and 5,000 men to Fort Donelson. Pillow took up a position at nearby Clarksville, Tennessee, and did not move into the fort until February 7, 1862. Alerted by a U.S. reconnaissance on January 14, 1862, Johnston ordered Tilghman to fortify the high ground opposite Fort Henry, which Polk had failed to do despite Johnston's orders. Tilghman failed to act decisively on these orders, which were too late to be adequately carried out in any event.
Gen. Beauregard arrived at Johnston's headquarters at Bowling Green on February 4, 1862, and was given overall command of Polk's force at the western end of Johnston's line at Columbus, Kentucky. On February 6, 1862, U.S. gunboats quickly reduced the defenses of ill-sited Fort Henry, inflicting 21 casualties on the small remaining Confederate force. Brig. Gen. Lloyd Tilghman surrendered the 94 remaining officers and men of his approximately 3,000-man force, which had not been sent to Fort Donelson, before Grant's U.S. forces could even take up their positions. Johnston knew he could be trapped at Bowling Green if Fort Donelson fell, so he moved his force to Nashville, the capital of Tennessee and an increasingly important Confederate industrial center, beginning on February 11, 1862.
Johnston also reinforced Fort Donelson with 12,000 more men, including those under Floyd and Pillow, a curious decision given his thought that the U.S. gunboats alone could take the fort. He ordered the fort commanders to evacuate the troops if the fort could not be held. The senior generals sent to the fort to command the enlarged garrison, Gideon J. Pillow and John B. Floyd, squandered their chance to avoid having to surrender most of the garrison and on February 16, 1862, Brig. Gen. Simon Buckner, having been abandoned by Floyd and Pillow, surrendered Fort Donelson. Colonel Nathan Bedford Forrest escaped with his cavalry force of about 700 men before the surrender. The Confederates suffered about 1,500 casualties, with an estimated 12,000 to 14,000 taken prisoner. U.S. casualties were 500 killed, 2,108 wounded, and 224 missing.
Johnston, who had little choice in allowing Floyd and Pillow to take charge at Fort Donelson based on seniority after he ordered them to add their forces to the garrison, took the blame and suffered calls for his removal because a full explanation to the press and public would have exposed the weakness of the Confederate position. His passive defensive performance while positioning himself in a forward position at Bowling Green, spreading his forces too thinly, not concentrating his forces in the face of U.S. advances, and appointing or relying upon inadequate or incompetent subordinates subjected him to criticism at the time and by later historians. The fall of the forts exposed Nashville to an imminent attack, and it fell without resistance to U.S. forces under Brig. Gen. Buell on February 25, 1862, two days after Johnston had to pull his forces out to avoid having them captured as well.
Concentration at Corinth.
Johnston was in a perilous situation after the fall of Ft. Donelson and Henry; with barely 17,000 men to face an overwhelming concentration of Union force, he hastily fled south into Mississippi by way of Nashville and then into northern Alabama. Johnston himself retreated with the force under his personal command, the Army of Central Kentucky, from the vicinity of Nashville. With Beauregard's help, Johnston decided to concentrate forces with those formerly under Polk and now already under Beauregard's command at the strategically located railroad crossroads of Corinth, Mississippi, which he reached by a circuitous route. Johnston kept the U.S. forces, now under the overall command of Maj. Gen. Henry Halleck, confused and hesitant to move, allowing Johnston to reach his objective undetected. He scraped together reinforcements from Louisiana, as well as part of Polk's force at Island No. 10, and 10,000 additional troops under Braxton Bragg brought up from Mobile. Bragg at least calmed the nerves of Beauregard and Polk, who had become agitated by their apparent dire situation in the face of numerically superior forces, before Johnston's arrival on March 24, 1862.
Johnston's army of 17,000 men gave the Confederates a combined force of about 40,000 to 44,669 men at Corinth. On March 29, 1862, Johnston officially took command of this combined force, which continued to use the Army of the Mississippi name under which Beauregard had organized it on March 5.
Johnston's only hope was to crush Grant before Buell and others could reinforce him. He started his army in motion on April 3, intent on surprising Grant's force as soon as the next day. It was not an easy undertaking; his army had been hastily thrown together, two-thirds of the soldiers had never fired a shot in battle, and drill, discipline, and staff work were so poor that the different divisions kept stumbling into each other on the march. Beauregard felt that this offensive was a mistake and could not possibly succeed, but Johnston replied "I would fight them if they were a million" as he drove his army on to Pittsburg Landing. His army was finally in position within a mile or two of Grant's force, undetected, by the evening of April 5, 1862.
Battle of Shiloh and death.
Johnston launched a massive surprise attack with his concentrated forces against Grant at the Battle of Shiloh on April 6, 1862. As the Confederate forces overran the U.S. camps, Johnston personally rallied troops up and down the line on his horse. One of his famous moments in the battle occurred when he witnessed some of his soldiers breaking from the ranks to pillage and loot the U.S. camps and was outraged to see a young lieutenant among them. "None of that, sir", Johnston roared at the officer, "we are not here for plunder." Then, realizing he had embarrassed the man, he picked up a tin cup from a table and announced, "Let this be my share of the spoils today", before directing his army onward.
At about 2:30 pm, while leading one of those charges against a U.S. camp near the "Peach Orchard", he was wounded, taking a bullet behind his right knee. The bullet clipped a part of his popliteal artery, and his boot filled up with blood. No medical personnel were on the scene since Johnston had sent his personal surgeon to care for the wounded Confederate troops and U.S. prisoners earlier in the battle.
Within a few minutes, Johnston was observed by his staff to be nearly fainting. Among his staff was Isham G. Harris, the Governor of Tennessee, who had ceased to make any real effort to function as governor after learning that Abraham Lincoln had appointed Andrew Johnson as military governor of Tennessee. Seeing Johnston slumping in his saddle and his face turning deathly pale, Harris asked: "General, are you wounded?" Johnston glanced down at his leg wound, then faced Harris and said his last words in a weak voice: "Yes... and I fear seriously." Harris and other staff officers removed Johnston from his horse, carried him to a small ravine near the "Hornets Nest", and desperately tried to aid the general, who had lost consciousness. Harris then sent an aide to fetch Johnston's surgeon but did not apply a tourniquet to Johnston's wounded leg. A few minutes later, Johnston died from blood loss before a doctor could be found. It is believed that Johnston may have lived for as long as one hour after receiving his fatal wound. It was later discovered that Johnston had a tourniquet in his pocket when he died.Harris and the other officers wrapped General Johnston's body in a blanket to not damage the troops' morale with the sight of the dead general. Johnston and his wounded horse, Fire Eater, were taken to his field headquarters on the Corinth road, where his body remained in his tent for the remainder of the battle. P. G. T. Beauregard assumed command of the army. He resumed leading the Confederate assault, which continued advancing and pushed the U.S. forces back to a final defensive line near the Tennessee river. With his army exhausted and daylight almost gone, Beauregard called off the final Confederate attack around 1900 hours, figuring he could finish off the U.S. army the following morning. However, Grant was reinforced by 20,000 fresh troops from Don Carlos Buell's Army of the Ohio during the night and led a successful counter-attack the following day, driving the Confederates from the field and winning the battle. As the Confederate army retreated to Corinth, Johnston's body was taken to the home of Colonel William Inge, which had been his headquarters in Corinth. It was covered in the Confederate flag and lay in state for several hours.
It is possible that a Confederate soldier fired the fatal round, as many Confederates were firing at the U.S. lines while Johnston charged well in advance of his soldiers. Alonzo Ridley of Los Angeles commanded the bodyguard "the Guides" of Gen. A. S. Johnston and was by his side when he fell.
Johnston was the highest-ranking fatality of the war on either side and his death was a strong blow to the morale of the Confederacy. At the time, Davis considered him the best general in the country.
Legacy and honors.
Johnston was survived by his wife, Eliza, and six children. His wife and five younger children, including one born after he went to war, chose to live out their days at home in Los Angeles with Eliza's brother, Dr. John Strother Griffin. Johnston's eldest son, Albert Sidney Jr. (born in Texas), had already followed him into the Confederate States Army. In 1863, Albert Jr. was on his way out of San Pedro harbor on a ferry after taking home leave in Los Angeles. While a steamer was taking on passengers from the ferry, a wave swamped the smaller boat, causing its boilers to explode. Albert Jr. was killed in the accident.
Upon his passing, General Johnston received the highest praise ever given by the Confederate government: accounts were published on December 20, 1862, and after that, in the Los Angeles "Star" of his family's hometown. Johnston Street, Hancock Street, and Griffin Avenue, each in northeast Los Angeles, are named after the general and his family, who lived in the neighborhood.
Johnston was initially buried in New Orleans. In 1866, a joint resolution of the Texas Legislature was passed to have his body moved and reinterred at the Texas State Cemetery in Austin. The re-interment occurred in 1867. Forty years later, the state appointed Elisabet Ney to design a monument and sculpture of him to be erected at the grave site, installed in 1905.
The Texas Historical Commission has erected a historical marker near the entrance of what was once Johnston's plantation. An adjacent marker was erected by the San Jacinto Chapter of the Daughters of The Republic of Texas and the Lee, Roberts, and Davis Chapter of the United Daughters of the Confederate States of America.
In 1916, the University of Texas at Austin recognized several confederate veterans (including Johnston) with statues on its South Mall. On August 21, 2017, as part of the wave of confederate monument removals in America, Johnston's statue was taken down. Plans were announced to add it to the Briscoe Center for American History on the east side of the university campus.
Johnston was inducted to the Texas Military Hall of Honor in 1980.
In the fall of 2018, A. S. Johnston Elementary School in Dallas, Texas, was renamed Cedar Crest Elementary. Johnston Middle School in Houston, Texas, was also renamed Meyerland Middle School. Three other elementary schools named for Confederate veterans were renamed simultaneously.

</doc>
<doc id="713" url="?curid=713" title="Android (robot)">
Android (robot)

An android is a humanoid robot or other artificial being often made from a flesh-like material. Historically, androids existed only in the domain of science fiction and were frequently seen in film and television, but advances in robot technology have allowed the design of functional and realistic humanoid robots.
Terminology.
The "Oxford English Dictionary" traces the earliest use (as "Androides") to Ephraim Chambers' 1728 "Cyclopaedia," in reference to an automaton that St. Albertus Magnus allegedly created. By the late 1700s, "androides", elaborate mechanical devices resembling humans performing human activities, were displayed in exhibit halls.
The term "android" appears in US patents as early as 1863 in reference to miniature human-like toy automatons. The term "android" was used in a more modern sense by the French author Auguste Villiers de l'Isle-Adam in his work "Tomorrow's Eve" (1886), featuring an artificial humanoid robot named Hadaly. The term made an impact into English pulp science fiction starting from Jack Williamson's "The Cometeers" (1936) and the distinction between mechanical robots and fleshy androids was popularized by Edmond Hamilton's Captain Future stories (1940–1944).
Although Karel Čapek's robots in "R.U.R. (Rossum's Universal Robots)" (1921)—the play that introduced the word "robot" to the world—were organic artificial humans, the word "robot" has come to primarily refer to mechanical humans, animals, and other beings. The term "android" can mean either one of these, while a cyborg ("cybernetic organism" or "bionic man") would be a creature that is a combination of organic and mechanical parts.
The term "droid", popularized by George Lucas in the original "Star Wars" film and now used widely within science fiction, originated as an abridgment of "android", but has been used by Lucas and others to mean any robot, including distinctly non-human form machines like R2-D2. The word "android" was used in "" episode "What Are Little Girls Made Of?" The abbreviation "andy", coined as a pejorative by writer Philip K. Dick in his novel "Do Androids Dream of Electric Sheep?", has seen some further usage, such as within the TV series "Total Recall 2070".
While the term "android" is used in reference to human-looking robots in general (not necessarily male-looking humanoid robots), a robot with a female appearance can also be referred to as a "gynoid". Besides one can refer to robots without alluding to their sexual appearance by calling them "anthrobots" (a portmanteau of anthrōpos and robot; see "anthrobotics") or "anthropoids" (short for anthropoid robots; the term "humanoids" is not appropriate because it is already commonly used to refer to human-like organic species in the context of science fiction, futurism and speculative astrobiology).
Authors have used the term "android" in more diverse ways than "robot" or "cyborg". In some fictional works, the difference between a robot and android is only superficial, with androids being made to look like humans on the outside but with robot-like internal mechanics. In other stories, authors have used the word "android" to mean a wholly organic, yet artificial, creation. Other fictional depictions of androids fall somewhere in between.
Eric G. Wilson, who defines an android as a "synthetic human being", distinguishes between three types of android, based on their body's composition:
Although human morphology is not necessarily the ideal form for working robots, the fascination in developing robots that can mimic it can be found historically in the assimilation of two concepts: "simulacra" (devices that exhibit likeness) and "automata" (devices that have independence).
Projects.
Several projects aiming to create androids that look, and, to a certain degree, speak or act like a human being have been launched or are underway.
Japan.
Japanese robotics have been leading the field since the 1970s. Waseda University initiated the WABOT project in 1967, and in 1972 completed the WABOT-1, the first android, a full-scale humanoid intelligent robot. Its limb control system allowed it to walk with the lower limbs, and to grip and transport objects with hands, using tactile sensors. Its vision system allowed it to measure distances and directions to objects using external receptors, artificial eyes and ears. And its conversation system allowed it to communicate with a person in Japanese, with an artificial mouth.
In 1984, WABOT-2 was revealed, and made a number of improvements. It was capable of playing the organ. Wabot-2 had ten fingers and two feet, and was able to read a score of music. It was also able to accompany a person. In 1986, Honda began its humanoid research and development program, to create humanoid robots capable of interacting successfully with humans.
The Intelligent Robotics Lab, directed by Hiroshi Ishiguro at Osaka University, and the Kokoro company demonstrated the Actroid at Expo 2005 in Aichi Prefecture, Japan and released the Telenoid R1 in 2010. In 2006, Kokoro developed a new "DER 2" android. The height of the human body part of DER2 is 165 cm. There are 47 mobile points. DER2 can not only change its expression but also move its hands and feet and twist its body. The "air servosystem" which Kokoro developed originally is used for the actuator. As a result of having an actuator controlled precisely with air pressure via a servosystem, the movement is very fluid and there is very little noise. DER2 realized a slimmer body than that of the former version by using a smaller cylinder. Outwardly DER2 has a more beautiful proportion. Compared to the previous model, DER2 has thinner arms and a wider repertoire of expressions. Once programmed, it is able to choreograph its motions and gestures with its voice.
The Intelligent Mechatronics Lab, directed by Hiroshi Kobayashi at the Tokyo University of Science, has developed an android head called "Saya", which was exhibited at Robodex 2002 in Yokohama, Japan. There are several other initiatives around the world involving humanoid research and development at this time, which will hopefully introduce a broader spectrum of realized technology in the near future. Now Saya is "working" at the Science University of Tokyo as a guide.
The Waseda University (Japan) and NTT docomo's manufacturers have succeeded in creating a shape-shifting robot "WD-2". It is capable of changing its face. At first, the creators decided the positions of the necessary points to express the outline, eyes, nose, and so on of a certain person. The robot expresses its face by moving all points to the decided positions, they say. The first version of the robot was first developed back in 2003. After that, a year later, they made a couple of major improvements to the design. The robot features an elastic mask made from the average head dummy. It uses a driving system with a 3DOF unit. The WD-2 robot can change its facial features by activating specific facial points on a mask, with each point possessing three degrees of freedom. This one has 17 facial points, for a total of 56 degrees of freedom. As for the materials they used, the WD-2's mask is fabricated with a highly elastic material called Septom, with bits of steel wool mixed in for added strength. Other technical features reveal a shaft driven behind the mask at the desired facial point, driven by a DC motor with a simple pulley and a slide screw. Apparently, the researchers can also modify the shape of the mask based on actual human faces. To "copy" a face, they need only a 3D scanner to determine the locations of an individual's 17 facial points. After that, they are then driven into position using a laptop and 56 motor control boards. In addition, the researchers also mention that the shifting robot can even display an individual's hair style and skin color if a photo of their face is projected onto the 3D Mask.
Singapore.
Prof Nadia Thalmann, a Nanyang Technological University scientist, directed efforts of the Institute for Media Innovation along with the School of Computer Engineering in the development of a social robot, Nadine. Nadine is powered by software similar to Apple's Siri or Microsoft's Cortana. Nadine may become a personal assistant in offices and homes in future, or she may become a companion for the young and the elderly.
Assoc Prof Gerald Seet from the School of Mechanical &amp; Aerospace Engineering and the BeingThere Centre led a three-year R&amp;D development in tele-presence robotics, creating EDGAR. A remote user can control EDGAR with the user's face and expressions displayed on the robot's face in real time. The robot also mimics their upper body movements.
South Korea.
KITECH researched and developed EveR-1, an android interpersonal communications model capable of emulating human emotional expression via facial "musculature" and capable of rudimentary conversation, having a vocabulary of around 400 words. She is tall and weighs , matching the average figure of a Korean woman in her twenties. EveR-1's name derives from the Biblical Eve, plus the letter "r" for "robot". EveR-1's advanced computing processing power enables speech recognition and vocal synthesis, at the same time processing lip synchronization and visual recognition by 90-degree micro-CCD cameras with face recognition technology. An independent microchip inside her artificial brain handles gesture expression, body coordination, and emotion expression. Her whole body is made of highly advanced synthetic jelly silicon and with 60 artificial joints in her face, neck, and lower body; she is able to demonstrate realistic facial expressions and sing while simultaneously dancing. In South Korea, the Ministry of Information and Communication had an ambitious plan to put a robot in every household by 2020. Several robot cities have been planned for the country: the first will be built in 2016 at a cost of 500 billion won (US$440 million), of which 50 billion is direct government investment. The new robot city will feature research and development centers for manufacturers and part suppliers, as well as exhibition halls and a stadium for robot competitions. The country's new Robotics Ethics Charter will establish ground rules and laws for human interaction with robots in the future, setting standards for robotics users and manufacturers, as well as guidelines on ethical standards to be programmed into robots to prevent human abuse of robots and vice versa.
United States.
Walt Disney and a staff of Imagineers created Great Moments with Mr. Lincoln that debuted at the 1964 New York World's Fair.
Dr. William Barry, an Education Futurist and former visiting West Point Professor of Philosophy and Ethical Reasoning at the United States Military Academy, created an AI android character named "Maria Bot". This Interface AI android was named after the infamous fictional robot Maria in the 1927 film "Metropolis", as a well-behaved distant relative. Maria Bot is the first AI Android Teaching Assistant at the university level. Maria Bot has appeared as a keynote speaker as a duo with Barry for a TEDx talk in Everett, Washington in February 2020.
Resembling a human from the shoulders up, Maria Bot is a virtual being android that has complex facial expressions and head movement and engages in conversation about a variety of subjects. She uses AI to process and synthesize information to make her own decisions on how to talk and engage. She collects data through conversations, direct data inputs such as books or articles, and through internet sources.
Maria Bot was built by an international high-tech company for Barry to help improve education quality and eliminate education poverty. Maria Bot is designed to create new ways for students to engage and discuss ethical issues raised by the increasing presence of robots and artificial intelligence. Barry also uses Maria Bot to demonstrate that programming a robot with life-affirming, ethical framework makes them more likely to help humans to do the same.
Maria Bot is an ambassador robot for good and ethical AI technology.
Hanson Robotics, Inc., of Texas and KAIST produced an android portrait of Albert Einstein, using Hanson's facial android technology mounted on KAIST's life-size walking bipedal robot body. This Einstein android, also called "Albert Hubo", thus represents the first full-body walking android in history. Hanson Robotics, the FedEx Institute of Technology, and the University of Texas at Arlington also developed the android portrait of sci-fi author Philip K. Dick (creator of "Do Androids Dream of Electric Sheep?", the basis for the film "Blade Runner"), with full conversational capabilities that incorporated thousands of pages of the author's works. In 2005, the PKD android won a first-place artificial intelligence award from AAAI.
Use in fiction.
Androids are a staple of science fiction. Isaac Asimov pioneered the fictionalization of the science of robotics and artificial intelligence, notably in his 1950s series "I, Robot". One thing common to most fictional androids is that the real-life technological challenges associated with creating thoroughly human-like robots—such as the creation of strong artificial intelligence—are assumed to have been solved. Fictional androids are often depicted as mentally and physically equal or superior to humans—moving, thinking and speaking as fluidly as them.
The tension between the nonhuman substance and the human appearance—or even human ambitions—of androids is the dramatic impetus behind most of their fictional depictions. Some android heroes seek, like Pinocchio, to become human, as in the film "Bicentennial Man", or Data in "". Others, as in the film "Westworld", rebel against abuse by careless humans. Android hunter Deckard in "Do Androids Dream of Electric Sheep?" and its film adaptation "Blade Runner" discovers that his targets appear to be, in some ways, more "human" than he is. Android stories, therefore, are not essentially stories "about" androids; they are stories about the human condition and what it means to be human.
One aspect of writing about the meaning of humanity is to use discrimination against androids as a mechanism for exploring racism in society, as in "Blade Runner". Perhaps the clearest example of this is John Brunner's 1968 novel "Into the Slave Nebula", where the blue-skinned android slaves are explicitly shown to be fully human. More recently, the androids Bishop and Annalee Call in the films "Aliens" and "Alien Resurrection" are used as vehicles for exploring how humans deal with the presence of an "Other". The 2018 video game "" also explores how androids are treated as second class citizens in a near future society.
Female androids, or "gynoids", are often seen in science fiction, and can be viewed as a continuation of the long tradition of men attempting to create the stereotypical "perfect woman". Examples include the Greek myth of "Pygmalion" and the female robot Maria in Fritz Lang's "Metropolis". Some gynoids, like Pris in "Blade Runner", are designed as sex-objects, with the intent of "pleasing men's violent sexual desires", or as submissive, servile companions, such as in "The Stepford Wives". Fiction about gynoids has therefore been described as reinforcing "essentialist ideas of femininity", although others have suggested that the treatment of androids is a way of exploring racism and misogyny in society.
The 2015 Japanese film "Sayonara", starring Geminoid F, was promoted as "the first movie to feature an android performing opposite a human actor".
Blending technological advancement and biological life, androids have been depicted in popular video games such as Megaman series among others. 

</doc>
<doc id="717" url="?curid=717" title="Alberta">
Alberta

Alberta is one of the thirteen provinces and territories of Canada. It is a part of Western Canada and is one of the three prairie provinces. Alberta borders British Columbia to the west, Saskatchewan to the east, the Northwest Territories to the north, and the U.S. state of Montana to the south. It is one of the only two landlocked provinces in Canada, with Saskatchewan being the other. The eastern part of the province is occupied by the Great Plains, while the western part borders the Rocky Mountains. The province has a predominantly continental climate but experiences quick temperature changes due to air aridity. Seasonal temperature swings are less pronounced in western Alberta due to occasional Chinook winds.
Alberta is the fourth largest province by area at , and the fourth most populous, being home to 4,262,635 people. Alberta's capital is Edmonton, while Calgary is its largest city. The two are Alberta's largest census metropolitan areas. More than half of Albertans live in either Edmonton or Calgary, which contributes to continuing the rivalry between the two cities. English is the official language of the province. In 2016, 76.0% of Albertans were anglophone, 1.8% were francophone and 22.2% were allophone.
Alberta's economy is based on hydrocarbons, petrochemical industries, livestock and agriculture. The oil and gas industry has been a pillar of Alberta's economy since 1947, when substantial oil deposits were discovered at Leduc No. 1 well. It has also become a part of the province's identity. Since Alberta is the province most rich in hydrocarbons, it provides 70% of the oil and natural gas produced on Canadian soil. In 2018, Alberta's output was billion, 15.27% of Canada's GDP.
Until the 1930s, Alberta's political landscape consisted of two major parties: the centre-left Liberals and the agrarian United Farmers of Alberta. Today, Alberta is generally perceived as a conservative province. The right-wing Social Credit Party held office continually from 1935 to 1971 before the centre-right Progressive Conservatives held office continually from 1971 to 2015, the latter being the longest unbroken run in government at the provincial or federal level in Canadian history.
Since before becoming part of Canada, Alberta has been home to several First Nations like Plains Indians and Woodland Cree. It was also a territory used by fur traders of the rival companies Hudson's Bay Company and North West Company. The Dominion of Canada bought the lands that would become Alberta as part of the NWT in 1870. From the late 1800s to early 1900s, many immigrants arrived to prevent the prairies from being annexed by the US. Growing wheat and cattle ranching also became very profitable. In 1905, the Alberta Act was passed, creating the province of Alberta. Massive oil reserves were discovered in 1947. The exploitation of oil sands began in 1967.
Alberta is renowned for its natural beauty, richness in fossils and for housing important nature reserves. Alberta is home to six UNESCO designated World Heritage Sites: the Canadian Rocky Mountain Parks, Dinosaur Provincial Park, Head-Smashed-In Buffalo Jump, Waterton-Glacier International Peace Park, Wood Buffalo National Park and Writing-on-Stone Provincial Park. Other popular sites include Banff National Park, Elk Island National Park, Jasper National Park, Waterton Lakes National Park, and Drumheller.
Etymology.
Alberta was named after Princess Louise Caroline Alberta (1848–1939), the fourth daughter of Queen Victoria. Princess Louise was the wife of John Campbell, Marquess of Lorne, Governor General of Canada (1878–83). Lake Louise and Mount Alberta were also named in her honour.
The name "Alberta" is a feminine Latinized form of Albert, the name of Princess Louise's father, the Prince Consort ( , masculine) and its Germanic cognates, ultimately derived from the Proto-Germanic language "*Aþalaberhtaz" (compound of "noble" + "bright/famous").
Geography.
Alberta, with an area of , is the fourth-largest province after Quebec, Ontario, and British Columbia.
Alberta's southern border is the 49th parallel north, which separates it from the U.S. state of Montana. The 60th parallel north divides Alberta from the Northwest Territories. The 110th meridian west separates it from the province of Saskatchewan; while on the west its boundary with British Columbia follows the 120th meridian west south from the Northwest Territories at 60°N until it reaches the Continental Divide at the Rocky Mountains, and from that point follows the line of peaks marking the Continental Divide in a generally southeasterly direction until it reaches the Montana border at 49°N.
The province extends north to south and east to west at its maximum width. Its highest point is at the summit of Mount Columbia in the Rocky Mountains along the southwest border while its lowest point is on the Slave River in Wood Buffalo National Park in the northeast.
With the exception of the semi-arid climate of the steppe in the south-eastern section, the province has adequate water resources. There are numerous rivers and lakes in Alberta used for swimming, fishing and a range of water sports. There are three large lakes, Lake Claire () in Wood Buffalo National Park, Lesser Slave Lake (), and Lake Athabasca (), which lies in both Alberta and Saskatchewan. The longest river in the province is the Athabasca River, which travels from the Columbia Icefield in the Rocky Mountains to Lake Athabasca.
The largest river is the Peace River with an average flow of . The Peace River originates in the Rocky Mountains of northern British Columbia and flows through northern Alberta and into the Slave River, a tributary of the Mackenzie River.
Alberta's capital city, Edmonton, is located at about the geographic centre of the province. It is the most northerly major city in Canada and serves as a gateway and hub for resource development in northern Canada. With its proximity to Canada's largest oil fields, the region has most of western Canada's oil refinery capacity. Calgary is about south of Edmonton and north of Montana, surrounded by extensive ranching country. Almost 75% of the province's population lives in the Calgary–Edmonton Corridor. The land grant policy to the railways served as a means to populate the province in its early years.
Most of the northern half of the province is boreal forest, while the Rocky Mountains along the southwestern boundary are largely temperate coniferous forests of the Alberta Mountain forests and Alberta–British Columbia foothills forests. The southern quarter of the province is prairie, ranging from shortgrass prairie in the southeastern corner to mixed grass prairie in an arc to the west and north of it. The central aspen parkland region extending in a broad arc between the prairies and the forests, from Calgary, north to Edmonton, and then east to Lloydminster, contains the most fertile soil in the province and most of the population. Much of the unforested part of Alberta is given over either to grain farming or cattle ranching, with mixed farming more common in the north and centre, while ranching and irrigated agriculture predominate in the south.
The Alberta badlands are located in southeastern Alberta, where the Red Deer River crosses the flat prairie and farmland, and features deep canyons and striking landforms. Dinosaur Provincial Park, near Brooks, showcases the badlands terrain, desert flora, and remnants from Alberta's past when dinosaurs roamed the then lush landscape.
Climate.
Alberta extends for over from north to south; its climate, therefore, varies considerably. Average high temperatures in January range from in the southwest to in the far north. The presence of the Rocky Mountains also influences the climate to the southwest, which disrupts the flow of the prevailing westerly winds and causes them to drop most of their moisture on the western slopes of the mountain ranges before reaching the province, casting a rain shadow over much of Alberta. The northerly location and isolation from the weather systems of the Pacific Ocean cause Alberta to have a dry climate with little moderation from the ocean. Annual precipitation ranges from in the southeast to in the north, except in the foothills of the Rocky Mountains where total precipitation including snowfall can reach annually.
Northern Alberta is mostly covered by boreal forest and has a subarctic climate. The agricultural area of southern Alberta has a semi-arid steppe climate because the annual precipitation is less than the water that evaporates or is used by plants. The southeastern corner of Alberta, part of the Palliser Triangle, experiences greater summer heat and lower rainfall than the rest of the province, and as a result, suffers frequent crop yield problems and occasional severe droughts. Western Alberta is protected by the mountains and enjoys the mild temperatures brought by winter Chinook winds. Central and parts of northwestern Alberta in the Peace River region are largely aspen parkland, a biome transitional between prairie to the south and boreal forest to the north.
Alberta has a humid continental climate with warm summers and cold winters. The province is open to cold Arctic weather systems from the north, which often produce cold winter conditions. As the fronts between the air masses shift north and south across Alberta, the temperature can change rapidly. Arctic air masses in the winter produce extreme minimum temperatures varying from in northern Alberta to in southern Alberta, although temperatures at these extremes are rare.
In the summer, continental air masses have produced record maximum temperatures from in the mountains to over in southeastern Alberta. Alberta is a sunny province. Annual bright sunshine totals range between 1,900 up to just under 2,600 hours per year. Northern Alberta gets about 18 hours of daylight in the summer. The average daytime temperatures range from around in the Rocky Mountain valleys and far north, up to around in the dry prairie of the southeast. The northern and western parts of the province experience higher rainfall and lower evaporation rates caused by cooler summer temperatures. The south and east-central portions are prone to drought-like conditions sometimes persisting for several years, although even these areas can receive heavy precipitation, sometimes resulting in flooding.
In the winter, the Alberta clipper, a type of intense, fast-moving winter storm that generally forms over or near the province and, pushed with great speed by the continental polar jetstream, descends over the rest of southern Canada and the northern tier of the United States. In southwestern Alberta, the cold winters are frequently interrupted by warm, dry Chinook winds blowing from the mountains, which can propel temperatures upward from frigid conditions to well above the freezing point in a very short period. During one Chinook recorded at Pincher Creek, temperatures soared from in just one hour. The region around Lethbridge has the most Chinooks, averaging 30 to 35 Chinook days per year. Calgary has a 56% chance of a white Christmas, while Edmonton has an 86% chance.
After Saskatchewan, Alberta experiences the most tornadoes in Canada with an average of 15 verified per year. Thunderstorms, some of them severe, are frequent in the summer, especially in central and southern Alberta. The region surrounding the Calgary–Edmonton Corridor is notable for having the highest frequency of hail in Canada, which is caused by orographic lifting from the nearby Rocky Mountains, enhancing the updraft/downdraft cycle necessary for the formation of hail.
Ecology.
Flora.
In central and northern Alberta the arrival of spring is marked by the early flowering of the prairie crocus ("Pulsatilla nuttalliana") "anemone"; this member of the buttercup family has been recorded flowering as early as March, though April is the usual month for the general population. Other prairie flora known to flower early are the golden bean ("Thermopsis rhombifolia") and wild rose ("Rosa acicularis"). Members of the sunflower ("Helianthus") family blossom on the prairie in the summer months between July and September. The southern and east central parts of Alberta are covered by short prairie grass, which dries up as summer lengthens, to be replaced by hardy perennials such as the prairie coneflower ("Ratibida"), fleabane, and sage ("Artemisia"). Both yellow and white sweet clover ("Melilotus") can be found throughout the southern and central areas of the province.
The trees in the parkland region of the province grow in clumps and belts on the hillsides. These are largely deciduous, typically aspen, poplar, and willow. Many species of willow and other shrubs grow in virtually any terrain. North of the North Saskatchewan River, evergreen forests prevail for thousands of square kilometres. Aspen poplar, balsam poplar ("Populus balsamifera") or in some parts cottonwood ("Populus deltoides"), and paper birch ("Betula papyrifera") are the primary large deciduous species. Conifers include jack pine ("Pinus banksiana"), Rocky Mountain pine, lodgepole pine ("Pinus contorta"), both white and black spruce, and the deciduous conifer tamarack ("Larix laricina").
Fauna.
The four climatic regions (alpine, boreal forest, parkland, and prairie) of Alberta are home to many different species of animals. The south and central prairie was the homeland of the American bison, also known as buffalo, with its grasses providing pasture and breeding ground for millions of buffalo. The buffalo population was decimated during early settlement, but since then, buffalo have made a comeback, living on farms and in parks all over Alberta.
Herbivores are found throughout the province. Moose, mule deer, elk, and white-tailed deer are found in the wooded regions, and pronghorn can be found in the prairies of southern Alberta. Bighorn sheep and mountain goats live in the Rocky Mountains. Rabbits, porcupines, skunks, squirrels, and many species of rodents and reptiles live in every corner of the province. Alberta is home to only one venomous snake species, the prairie rattlesnake.
Alberta is home to many large carnivores such as wolves, grizzly bears, black bears, and mountain lions, which are found in the mountains and wooded regions. Smaller carnivores of the canine and feline families include coyotes, red foxes, Canada lynx, and bobcats. Wolverines can also be found in the northwestern areas of the province.
Central and northern Alberta and the region farther north are the nesting ground of many migratory birds. Vast numbers of ducks, geese, swans and pelicans arrive in Alberta every spring and nest on or near one of the hundreds of small lakes that dot northern Alberta. Eagles, hawks, owls, and crows are plentiful, and a huge variety of smaller seed and insect-eating birds can be found. Alberta, like other temperate regions, is home to mosquitoes, flies, wasps, and bees. Rivers and lakes are populated with pike, walleye, whitefish, rainbow, speckled, brown trout, and sturgeon. Native to the province, the bull trout, is the provincial fish and an official symbol of Alberta. Turtles are found in some water bodies in the southern part of the province. Frogs and salamanders are a few of the amphibians that make their homes in Alberta.
Alberta is the only province in Canada — as well as one of the few places in the world — that is free from Norwegian rats. Since the early 1950s, the Government of Alberta has operated a rat-control program, which has been so successful that only isolated instances of wild rat sightings are reported, usually of rats arriving in the province aboard trucks or by rail. In 2006, Alberta Agriculture reported zero findings of wild rats; the only rat interceptions have been domesticated rats that have been seized from their owners. It is illegal for individual Albertans to own or keep Norwegian rats of any description; the animals can only be kept in the province by zoos, universities and colleges, and recognized research institutions. In 2009, several rats were
found and captured, in small pockets in southern Alberta, putting Alberta's rat-free status in jeopardy. A colony of rats was subsequently found in a landfill near Medicine Hat in 2012 and again in 2014.
Paleontology.
Alberta has one of the greatest diversities and abundances of Late Cretaceous dinosaur fossils worldwide. Taxa are represented by complete fossil skeletons, isolated material, microvertebrate remains, and even mass graves. At least 38 dinosaur type specimens were collected in the province. The Foremost Formation, Oldman Formation and Dinosaur Park Formations collectively comprise the Judith River Group and are the most thoroughly studied dinosaur-bearing strata in Alberta.
Dinosaur-bearing strata are distributed widely throughout Alberta. The Dinosaur Provincial Park area contains outcrops of the Dinosaur Park Formation and Oldman Formation. In Alberta's central and southern regions are intermittent Scollard Formation outcrops. In the Drumheller Valley and Edmonton regions there are exposed Horseshoe Canyon facies. Other formations have been recorded as well, like the Milk River and Foremost Formations. The latter two have a lower diversity of documented dinosaurs, primarily due to their lower total fossil quantity and neglect from collectors who are hindered by the isolation and scarcity of exposed outcrops. Their dinosaur fossils are primarily teeth recovered from microvertebrate fossil sites. Additional geologic formations that have produced only a few fossils are the Belly River Group and St. Mary River Formations of the southwest and the northwestern Wapiti Formation, which contains two "Pachyrhinosaurus" bone beds. The Bearpaw Formation represents strata deposited during a marine transgression. Dinosaurs are known from this formation, but represent specimens washed out to sea or reworked from older sediments.
History.
Paleo-Indians arrived in Alberta at least 10,000 years ago, toward the end of the last ice age. They are thought to have migrated from Siberia to Alaska on a land bridge across the Bering Strait and then possibly moved down the east side of the Rocky Mountains through Alberta to settle the Americas. Others may have migrated down the coast of British Columbia and then moved inland. Over time they differentiated into various First Nations peoples, including the Plains Indians of southern Alberta such as those of the Blackfoot Confederacy and the Plains Cree, who generally lived by hunting buffalo, and the more northerly tribes such as the Woodland Cree and Chipewyan who hunted, trapped, and fished for a living.
The first Europeans to visit Alberta were French Canadians during the late 18th century, working as fur traders. French was the predominant language used in some early fur trading forts in the region, such as the first Fort Edmonton (in present-day Fort Saskatchewan). After the British arrival in Canada, approximately half of the province of Alberta, south of the Athabasca River drainage, became part of Rupert's Land which consisted of all land drained by rivers flowing into Hudson Bay. This area was granted by Charles II of England to the Hudson's Bay Company (HBC) in 1670, and rival fur trading companies were not allowed to trade in it.
The Athabasca River and the rivers north of it were not in HBC territory because they drained into the Arctic Ocean instead of Hudson Bay, and they were prime habitats for fur-bearing animals. The first European explorer of the Athabasca region was Peter Pond, who learned of the Methye Portage, which allowed travel from southern rivers into the rivers north of Rupert's Land. Other North American fur traders formed the North West Company (NWC) of Montreal to compete with the HBC in 1779. The NWC occupied the northern part of Alberta territory. Peter Pond built Fort Athabasca on Lac la Biche in 1778. Roderick Mackenzie built Fort Chipewyan on Lake Athabasca ten years later in 1788. His cousin, Sir Alexander Mackenzie, followed the North Saskatchewan River to its northernmost point near Edmonton, then setting northward on foot, trekked to the Athabasca River, which he followed to Lake Athabasca. It was there he discovered the mighty outflow river which bears his name—the Mackenzie River—which he followed to its outlet in the Arctic Ocean. Returning to Lake Athabasca, he followed the Peace River upstream, eventually reaching the Pacific Ocean, and so he became the first European to cross the North American continent north of Mexico.
The extreme southernmost portion of Alberta was part of the French (and Spanish) territory of Louisiana and was sold to the United States in 1803. In the Treaty of 1818, the portion of Louisiana north of the Forty-Ninth Parallel was ceded to Great Britain.
Fur trade expanded in the north, but bloody battles occurred between the rival HBC and NWC, and in 1821 the British government forced them to merge to stop the hostilities. The amalgamated Hudson's Bay Company dominated trade in Alberta until 1870 when the newly formed Canadian Government purchased Rupert's Land. Northern Alberta was included in the North-Western Territory until 1870, when it and Rupert's land became Canada's North-West Territories.
First Nations negotiated the Numbered Treaties with the Crown in which the Crown gained title to the land that would later become Alberta, and the Crown committed to the ongoing support of the First Nations and guaranteed their hunting and fishing rights. The most significant treaties for Alberta are Treaty 6 (1876), Treaty 7 (1877) and Treaty 8 (1899).
The District of Alberta was created as part of the North-West Territories in 1882. As settlement increased, local representatives to the North-West Legislative Assembly were added. After a long campaign for autonomy, in 1905, the District of Alberta was enlarged and given provincial status, with the election of Alexander Cameron Rutherford as the first premier. Less than a decade later, the First World War presented special challenges to the new province as an extraordinary number of volunteers left relatively few workers to maintain services and production. Over 50% of Alberta's doctors volunteered for service overseas.
On June 21, 2013, during the 2013 Alberta floods Alberta experienced heavy rainfall that triggered catastrophic flooding throughout much of the southern half of the province along the Bow, Elbow, Highwood and Oldman rivers and tributaries. A dozen municipalities in Southern Alberta declared local states of emergency on June 21 as water levels rose and numerous communities were placed under evacuation orders.
In 2016, the Fort McMurray wildfire resulted in the largest fire evacuation of residents in Alberta's history, as more than 80,000 people were ordered to evacuate.
From 2020 until restrictions were lifted in 2022, Alberta was affected by the COVID-19 pandemic.
Demographics.
The 2021 Canadian census reported Alberta had a population of 4,262,635 living in 1,633,220 of its 1,772,670 total dwellings, an 4.8% change from its 2016 population of 4,067,175. With a land area of , it had a population density of in 2021. Statistics Canada estimated the province to have a population of 4,800,768 in Q1 of 2024.
Since 2000, Alberta's population has experienced a relatively high rate of growth, mainly because of its burgeoning economy. Between 2003 and 2004, the province had high birthrates (on par with some larger provinces such as British Columbia), relatively high immigration, and a high rate of interprovincial migration compared to other provinces.
In 2016, Alberta continued to have the youngest population among the provinces with a median age of 36.7 years, compared with the national median of 41.2 years. Also in 2016, Alberta had the smallest proportion of seniors (12.3%) among the provinces and one of the highest population shares of children (19.2%), further contributing to Alberta's young and growing population.
About 81% of the population lives in urban areas and only about 19% in rural areas. The Calgary–Edmonton Corridor is the most urbanized area in the province and is one of the most densely populated areas of Canada. Many of Alberta's cities and towns have experienced very high rates of growth in recent history. Alberta's population rose from 73,022 in 1901 to 3,290,350 according to the 2006 census.
According to the 2016 census Alberta has 779,155 residents (19.2%) between the ages of 0–14, 2,787,805 residents (68.5%) between the ages of 15–64, and 500,215 residents (12.3%) aged 65 and over.
Additionally, as per the 2016 census, 1,769,500 residents hold a postsecondary certificate, diploma or degree, 895,885 residents have obtained a secondary (high) school diploma or equivalency certificate, and 540,665 residents do not have any certificate, diploma or degree.
Language.
As of the 2021 Canadian Census, the ten most spoken languages in the province included English (4,109,720 or 98.37%), French (260,415 or 6.23%), Tagalog (172,625 or 4.13%), Punjabi (126,385 or 3.03%), Spanish (116,070 or 2.78%), Hindi (94,015 or 2.25%), Mandarin (82,095 or 1.97%), Arabic (76,760 or 1.84%), Cantonese (74,960 or 1.79%), and German (65,370 or 1.56%). The question on knowledge of languages allows for multiple responses.
As of the 2016 census, English is the most common mother tongue, with 2,991,485 native speakers. This is followed by Tagalog, with 99,035 speakers, German, with 80,050 speakers, French, with 72,150 native speakers, and Punjabi, with 68,695 speakers.
The 2006 census found that English, with 2,576,670 native speakers, was the most common mother tongue of Albertans, representing 79.99% of the population. The next most common mother tongues were Chinese with 97,275 native speakers (3.02%), followed by German with 84,505 native speakers (2.62%) and French with 61,225 (1.90%). Other mother tongues include: Punjabi, with 36,320 native speakers (1.13%); Tagalog, with 29,740 (0.92%); Ukrainian, with 29,455 (0.91%); Spanish, with 29,125 (0.90%); Polish, with 21,990 (0.68%); Arabic, with 20,495 (0.64%); Dutch, with 19,980 (0.62%); and Vietnamese, with 19,350 (0.60%). The most common aboriginal language is Cree 17,215 (0.53%). Other common mother tongues include Italian with 13,095 speakers (0.41%); Urdu with 11,275 (0.35%); and Korean with 10,845 (0.33%); then Hindi 8,985 (0.28%); Persian 7,700 (0.24%); Portuguese 7,205 (0.22%); and Hungarian 6,770 (0.21%).
According to Statistics Canada, Alberta is home to the second-highest proportion (2%) of Francophones in western Canada (after Manitoba). Despite this, relatively few Albertans claim French as their mother tongue. Many of Alberta's French-speaking residents live in the central and northwestern regions of the province, after migration from other areas of Canada or descending from Métis.
Ethnicity.
Alberta has considerable ethnic diversity. In line with the rest of Canada, many are descended from immigrants of Western European nations, notably England, Scotland, Ireland, Wales and France, but large numbers later came from other regions of Europe, notably Germany, Ukraine and Scandinavia.
In the 2006 Canadian census, the most commonly reported ethnic origins among Albertans were: 885,825 English (27.2%); 679,705 German (20.9%); 667,405 Canadian (20.5%); 661,265 Scottish (20.3%); 539,160 Irish (16.6%); 388,210 French (11.9%); 332,180 Ukrainian (10.2%); 172,910 Dutch (5.3%); 170,935 Polish (5.2%); 169,355 North American Indian (5.2%); 144,585 Norwegian (4.4%); and 137,600 Chinese (4.2%). (Each person could choose as many ethnicities as were applicable.) Amongst those of British heritage, the Scots have had a particularly strong influence on place-names, with the names of many cities and towns including Calgary, Airdrie, Canmore, and Banff having Scottish origins.
Both Edmonton and Calgary have historic Chinatowns, and Calgary has Canada's third-largest Chinese community. The Chinese presence began with workers employed in the building of the Canadian Pacific Railway in the 1880s.
In 2021, 27.8% of the population consisted of visible minorities and 6.8% of the population was Indigenous, mostly of First Nations and Métis descent. There was also a small number of Inuit in the province. The Indigenous population has been growing at a faster rate than the population of Alberta as a whole.
Religion.
According to the 2021 census, religious groups in Alberta included:
As of the 2011 National Household Survey, the largest religious group was Roman Catholic, representing 24.3% of the population. Alberta had the second-highest percentage of non-religious residents among the provinces (after British Columbia) at 31.6% of the population. Of the remainder, 7.5% of the population identified themselves as belonging to the United Church of Canada, while 3.9% were Anglican. Lutherans made up 3.3% of the population while Baptists comprised 1.9%.
Members of LDS Church are mostly concentrated in the extreme south of the province. Alberta has a population of Hutterites, a communal Anabaptist sect similar to the Mennonites, and has a significant population of Seventh-day Adventists. Alberta is home to several Byzantine Rite Churches as part of the legacy of Eastern European immigration, including the Ukrainian Catholic Eparchy of Edmonton, and the Ukrainian Orthodox Church of Canada's Western Diocese which is based in Edmonton. Muslims made up 3.2% of the population, Sikhs 1.5%, Buddhists 1.2%, and Hindus 1.0%. Many of these are immigrants, but others have roots that go back to the first settlers of the prairies. Canada's oldest mosque, the Al-Rashid Mosque, is located in Edmonton, whereas Calgary is home to Canada's largest mosque, the Baitun Nur Mosque. Alberta is also home to a growing Jewish population of about 15,400 people who constituted 0.3% of Alberta's population. Most of Alberta's Jews live in the metropolitan areas of Calgary (8,200) and Edmonton (5,500).
Economy.
Alberta's economy was one of the strongest in the world, supported by the burgeoning petroleum industry and to a lesser extent, agriculture and technology. In 2013, Alberta's per capita GDP exceeded that of the United States, Norway, or Switzerland, and was the highest of any province in Canada at This was 56% higher than the national average of and more than twice that of some of the Atlantic provinces. In 2006, the deviation from the national average was the largest for any province in Canadian history. According to the 2006 census, the median annual family income after taxes was $70,986 in Alberta (compared to $60,270 in Canada as a whole). In 2014, Alberta had the second-largest economy in Canada after Ontario, with a GDP exceeding . The GDP of the province calculated at basic prices rose by 4.6% in 2017 to $327.4 billion, which was the largest increase recorded in Canada, and it ended two consecutive years of decreases.
Alberta's debt-to-GDP ratio is projected to peak at 12.1% in fiscal year 2021–2022, falling to 11.3% the following year.
The Calgary-Edmonton Corridor is the most urbanized region in the province and one of the densest in Canada. The region covers a distance of roughly north to south. In 2001, the population of the Calgary-Edmonton Corridor was 2.15 million (72% of Alberta's population). It is also one of the fastest-growing regions in the country. A 2003 study by TD Bank Financial Group found the corridor to be the only Canadian urban centre to amass a United States level of wealth while maintaining a Canadian style quality of life, offering universal health care benefits. The report found that GDP per capita in the corridor was 10% above average United States metropolitan areas and 40% above other Canadian cities at that time.
The Fraser Institute states that Alberta also has very high levels of economic freedom and rates Alberta as the freest economy in Canada, and second-freest economy amongst U.S. states and Canadian provinces.
In 2014, merchandise exports totalled US$121.4 billion. Energy revenues totalled $111.7 billion and Energy resource exports totalled $90.8 billion. Farm Cash receipts from agricultural products totalled $12.9 billion. Shipments of forest products totalled $5.4 billion while exports were $2.7 billion. Manufacturing sales totalled $79.4 billion, and Alberta's information and communications technology (ICT) industries generated over $13 billion in revenue. In total, Alberta's 2014 GDP amassed $364.5 billion in 2007 dollars, or $414.3 billion in 2015 dollars. In 2015, Alberta's GDP grew unstably despite low oil prices, with growth rates as high 4.4% and as low as 0.2%.
Agriculture and forestry.
Agriculture has a significant position in the province's economy. The province has over three million head of cattle, and Alberta beef has a healthy worldwide market. Forty percent of all Canadian beef is produced in Alberta. The province also produces the most bison meat in Canada. Sheep for wool and mutton are also raised.
Wheat and canola are primary farm crops, with Alberta leading the provinces in spring wheat production; other grains are also prominent. Much of the farming is dryland farming, often with fallow seasons interspersed with cultivation. Continuous cropping (in which there is no fallow season) is gradually becoming a more common mode of production because of increased profits and a reduction of soil erosion. Across the province, the once common grain elevator is slowly being lost as rail lines are decreasing; farmers typically truck the grain to central points.
Alberta is the leading beekeeping province of Canada, with some beekeepers wintering hives indoors in specially designed barns in southern Alberta, then migrating north during the summer into the Peace River valley where the season is short but the working days are long for honeybees to produce honey from clover and fireweed. Hybrid canola also requires bee pollination, and some beekeepers service this need.
Forestry plays a vital role in Alberta's economy, providing over 15,000 jobs and contributing billions of dollars annually. Uses for harvested timber include pulpwood, hardwood, engineered wood and bioproducts such as chemicals and biofuels.
Industry.
Alberta is the largest producer of conventional crude oil, synthetic crude, natural gas and gas products in Canada. Alberta is the world's second-largest exporter of natural gas and the fourth-largest producer. Two of the largest producers of petrochemicals in North America are located in central and north-central Alberta. In both Red Deer and Edmonton, polyethylene and vinyl manufacturers produce products that are shipped all over the world. Edmonton's oil refineries provide the raw materials for a large petrochemical industry to the east of Edmonton.
The Athabasca oil sands surrounding Fort McMurray have estimated unconventional oil reserves approximately equal to the conventional oil reserves of the rest of the world, estimated to be . Many companies employ both conventional strip mining and non-conventional in situ methods to extract the bitumen from the oil sands. As of late 2006, there were over $100 billion in oil sands projects under construction or in the planning stages in northeastern Alberta.
Another factor determining the viability of oil extraction from the oil sands is the price of oil. The oil price increases since 2003 have made it profitable to extract this oil, which in the past would give little profit or even a loss. By mid-2014, rising costs and stabilizing oil prices threatened the economic viability of some projects. An example of this was the shelving of the Joslyn North project in the Athabasca region in May 2014.
With concerted effort and support from the provincial government, several high-tech industries have found their birth in Alberta, notably patents related to interactive liquid-crystal display systems. With a growing economy, Alberta has several financial institutions dealing with civil and private funds.
Tourism.
Alberta has been a tourist destination from the early days of the 20th century, with attractions including outdoor locales for skiing, hiking, and camping, shopping locales such as West Edmonton Mall, Calgary Stampede, outdoor festivals, professional athletic events, international sporting competitions such as the Commonwealth Games and Olympic Games, as well as more eclectic attractions. According to Alberta Economic Development, Calgary and Edmonton both host over four million visitors annually. Banff, Jasper and the Rocky Mountains are visited by about three million people per year. Alberta tourism relies heavily on Southern Ontario tourists, as well as tourists from other parts of Canada, the United States, and many other countries.
There are also natural attractions like Elk Island National Park, Wood Buffalo National Park, and the Columbia Icefield. Alberta's Rockies include well-known tourist destinations Banff National Park and Jasper National Park. The two mountain parks are connected by the scenic Icefields Parkway. Banff is located west of Calgary on Highway 1, and Jasper is located west of Edmonton on the Yellowhead Highway. Five of Canada's fourteen UNESCO World Heritage Sites are located within the province: Canadian Rocky Mountain Parks, Waterton-Glacier International Peace Park, Wood Buffalo National Park, Dinosaur Provincial Park and Head-Smashed-In Buffalo Jump. A number of these areas hold ski resorts, most notably Banff Sunshine, Lake Louise, Marmot Basin, Norquay and Nakiska.
About 1.2 million people visit the Calgary Stampede, a celebration of Canada's own Wild West and the cattle ranching industry. About 700,000 people enjoy Edmonton's K-Days (formerly Klondike Days and Capital EX). Edmonton was the gateway to the only all-Canadian route to the Yukon gold fields, and the only route which did not require gold-seekers to travel the exhausting and dangerous Chilkoot Pass.
Another tourist destination that draws more than 650,000 visitors each year is the Drumheller Valley, located northeast of Calgary. Drumheller, known as the "Dinosaur Capital of The World", offers the Royal Tyrrell Museum of Palaeontology. Drumheller also had a rich mining history being one of Western Canada's largest coal producers during the war years. Another attraction in east-central Alberta is Alberta Prairie Railway Excursions, a popular tourist attraction operated out of Stettler, that offers train excursions into the prairie and caters to tens of thousands of visitors every year.
Government and politics.
The Government of Alberta is organized as a parliamentary democracy with a unicameral legislature. Its unicameral legislature—the Legislative Assembly—consists of 87 members elected first past the post (FPTP) from single-member constituencies. Locally municipal governments and school boards are elected and operate separately. Their boundaries do not necessarily coincide.
As King of Canada, Charles III is the head of state of Alberta. His duties concerning the Government of Alberta are carried out by Lieutenant Governor Salma Lakhani. The King and lieutenant governor are figureheads whose actions are highly restricted by custom and constitutional convention. The lieutenant governor handles numerous honorific duties in the name of the King. The government is headed by the premier. The premier is normally a member of the Legislative Assembly, and draws all the members of the Cabinet from among the members of the Legislative Assembly. The City of Edmonton is the seat of the provincial government—the capital of Alberta. The current premier is Danielle Smith, who was sworn in on October 11, 2022.
Alberta's elections have tended to yield much more conservative outcomes than those of other Canadian provinces. From the 1980s to the 2010s, Alberta had three main political parties, the Progressive Conservatives ("Conservatives" or "Tories"), the Liberals, and the social democratic New Democrats. The Wildrose Party, a more libertarian party formed in early 2008, gained much support in the 2012 election and became the official opposition, a role it held until 2017 when it was dissolved and succeeded by the new United Conservative Party created by the merger of Wildrose and the Progressive Conservatives. The strongly conservative Social Credit Party was a power in Alberta for many decades, but fell from the political map after the Progressive Conservatives came to power in 1971.
For 44 years the Progressive Conservatives governed Alberta. They lost the 2015 election to the NDP (which formed their own government for the first time in provincial history, breaking almost 80 consecutive years of right-wing rule), suggesting at the time a possible shift to the left in the province, also indicated by the election of progressive mayors in both of Alberta's major cities. Since becoming a province in 1905, Alberta has seen only five changes of government—only six parties have governed Alberta: the Liberals, from 1905 to 1921; the United Farmers of Alberta, from 1921 to 1935; the Social Credit Party, from 1935 to 1971; the Progressive Conservative Party, from 1971 to 2015; from 2015 to 2019, the Alberta New Democratic Party; and from 2019, the United Conservative Party, with the most recent transfer of power being the first time in provincial history that an incumbent government was not returned to a second term.
Administrative divisions.
The province is divided into ten types of local governments – urban municipalities (including cities, towns, villages and summer villages), specialized municipalities, rural municipalities (including municipal districts (often named as counties), improvement districts, and special areas), Métis settlements, and Indian reserves. All types of municipalities are governed by local residents and were incorporated under various provincial acts, with the exception of improvement districts (governed by either the provincial or federal government), and Indian reserves (governed by local band governments under federal jurisdiction).
Law enforcement.
Policing in the province of Alberta upon its creation was the responsibility of the Royal Northwest Mounted Police. In 1917, due to pressures of the First World War, the Alberta Provincial Police was created. This organization policed the province until it was disbanded as a Great Depression-era cost-cutting measure in 1932. It was at that time the, now renamed, Royal Canadian Mounted Police resumed policing of the province, specifically RCMP "K" Division. With the advent of the Alberta Sheriffs Branch, the distribution of duties of law enforcement in Alberta has been evolving as certain aspects, such as traffic enforcement, mobile surveillance and the close protection of the Premier of Alberta have been transferred to the Sheriffs. In 2006, Alberta formed the Alberta Law Enforcement Response Teams (ALERT) to combat organized crime and the serious offences that accompany it. ALERT is made up of members of the RCMP, Sheriffs Branch, and various major municipal police forces in Alberta.
Military.
Military bases in Alberta include Canadian Forces Base (CFB) Cold Lake, CFB Edmonton, CFB Suffield and CFB Wainwright. Air force units stationed at CFB Cold Lake have access to the Cold Lake Air Weapons Range. CFB Edmonton is the headquarters for the 3rd Canadian Division. CFB Suffield hosts British troops and is the largest training facility in Canada.
Taxation.
According to Alberta's 2009 budget, government revenue in that year came mainly from royalties on non-renewable natural resources (30.4%), personal income taxes (22.3%), corporate and other taxes (19.6%), and grants from the federal government primarily for infrastructure projects (9.8%). In 2014, Alberta received $6.1 billion in bitumen royalties. With the drop in the price of oil in 2015 it was down to $1.4 billion. In 2016, Alberta received "about $837 million in royalty payments from oil sands Royalty Projects". According to the 2018–2021 fiscal plan, the two top sources of revenue in 2016 were personal income tax at $10,763 million and federal transfers of $7,976 million with total resource revenue at $3,097 million. Alberta is the only province in Canada without a provincial sales tax. Alberta residents are subject to the federal sales tax, the Goods and Services Tax of 5%.
From 2001 to 2016, Alberta was the only Canadian province to have a flat tax of 10% of taxable income, which was introduced by Premier, Ralph Klein, as part of the Alberta Tax Advantage, which also included a zero-percent tax on income below a "generous personal exemption".
In 2016, under Premier Rachel Notley, while most Albertans continued to pay the 10% income tax rate, new tax brackets 12%, 14%, and 15% for those with higher incomes ($128,145 annually or more) were introduced. Alberta's personal income tax system maintained a progressive character by continuing to grant residents personal tax exemptions of $18,451, in addition to a variety of tax deductions for persons with disabilities, students, and the aged. Alberta's municipalities and school jurisdictions have their own governments who usually work in co-operation with the provincial government. By 2018, most Albertans continued to pay the 10% income tax rate.
According to a March 2015 Statistics Canada report, the median household income in Alberta in 2014 was about $100,000, which is 23% higher than the Canadian national average.
Based on Statistic Canada reports, low-income Albertans, who earn less than $25,000 and those in the high-income bracket earning $150,000 or more, are the lowest-taxed people in Canada. Those in the middle income brackets representing those that earn about $25,000 to $75,000 pay more in provincial taxes than residents in British Columbia and Ontario. In terms of income tax, Alberta is the "best province" for those with a low income because there is no provincial income tax for those who earn $18,915 or less. Even with the 2016 progressive tax brackets up to 15%, Albertans who have the highest incomes, those with a $150,000 annual income or more—about 178,000 people in 2015, pay the least in taxes in Canada. — About 1.9 million Albertans earned between $25,000 and $150,000 in 2015.
Alberta also privatized alcohol distribution. By 2010, privatization had increased outlets from 304 stores to 1,726; 1,300 jobs to 4,000 jobs; and 3,325 products to 16,495 products. Tax revenue also increased from $400 million to $700 million.
In 2017/18 Alberta collected about $2.4 billion in education property taxes from municipalities. Alberta municipalities raise a significant portion of their income through levying property taxes. The value of assessed property in Alberta was approximately $727 billion in 2011. Most real property is assessed according to its market value. The exceptions to market value assessment are farmland, railways, machinery and equipment and linear property, all of which is assessed by regulated rates. Depending on the property type, property owners may appeal a property assessment to their municipal 'Local Assessment Review Board', 'Composite Assessment Review Board,' or the Alberta Municipal Government Board.
Culture.
Calgary is famous for its Stampede, dubbed "The Greatest Outdoor Show on Earth". The Stampede is Canada's biggest rodeo festival and features various races and competitions, such as calf roping and bull riding. In line with the western tradition of rodeo are the cultural artisans that reside and create unique Alberta western heritage crafts.
Summer brings many festivals to Alberta, especially in Edmonton. The Edmonton Fringe Festival is the world's second-largest after the Edinburgh Festival. Both Calgary and Edmonton host many annual festivals and events, including folk music festivals. The city's "heritage days" festival sees the participation of over 70 ethnic groups. Edmonton's Churchill Square is home to a large number of the festivals, including A Taste of Edmonton and The Works Art &amp; Design Festival throughout the summer months.
In 2019, Minister of Culture and Tourism Ricardo Miranda announced the Alberta Artist in Residence program in conjunction with the province's first Month of the Artist to celebrate the arts and the value they bring to the province, both socially and economically, The artist is selected each year via a public and competitive process is expected to do community outreach and attend events to promote the arts throughout the province. The award comes with $60,000 funding which includes travel and materials costs. On January 31, 2019, Lauren Crazybull was named Alberta's first artist in residence. Alberta is the first province to launch an artist in residence program in Canada.
Education.
As with any Canadian province, the Alberta Legislature has (almost) exclusive authority to make laws respecting education. Since 1905, the Legislature has used this capacity to continue the model of locally elected public and separate school boards which originated prior to 1905, as well as to create and regulate universities, colleges, technical institutions, and other educational forms and institutions (public charter schools, private schools, homeschooling).
Elementary and secondary.
There are forty-two public school jurisdictions in Alberta, and seventeen operating separate school jurisdictions. Sixteen of the operating separate school jurisdictions have a Catholic electorate, and one (St. Albert) has a Protestant electorate. In addition, one Protestant separate school district, Glen Avon, survives as a ward of the St. Paul Education Region. The City of Lloydminster straddles the Albertan/Saskatchewan border, and both the public and separate school systems in that city are counted in the above numbers: both of them operate according to Saskatchewan law.
For many years, the provincial government has funded the greater part of the cost of providing K–12 education. Prior to 1994, public and separate school boards in Alberta had the legislative authority to levy a local tax on property as supplementary support for local education. In 1994, the government of the province eliminated this right for public school boards, but not for separate school boards. Since 1994, there has continued to be a tax on property in support of K–12 education; the difference is that the provincial government now sets the mill rate, the money is collected by the local municipal authority and remitted to the provincial government. The relevant legislation requires that all the money raised by this property tax must go to support K–12 education provided by school boards. The provincial government pools the property tax funds from across the province and distributes them, according to a formula, to public and separate school jurisdictions and Francophone authorities.
Public and separate school boards, charter schools, and private schools all follow the Program of Studies and the curriculum approved by the provincial department of education (Alberta Education). Homeschool tutors may choose to follow the Program of Studies or develop their own Program of Studies. Public and separate schools, charter schools, and approved private schools all employ teachers who are certificated by Alberta Education, they administer Provincial Achievement Tests and Diploma Examinations set by Alberta Education, and they may grant high school graduation certificates endorsed by Alberta Education.
Post-secondary.
Several publicly funded post-secondary institutions are governed under the province's "Post-secondary Learning Act". This includes four comprehensive research universities that provides undergraduate and graduate degrees, Athabasca University, the University of Alberta, the University of Calgary, and the University of Lethbridge; and three undergraduate universities that primarily provide bachelor's degrees, the Alberta University of the Arts, Grant MacEwan University, and Mount Royal University.
Nine comprehensive community colleges offer primarily offer diploma and certificate programs, Bow Valley College, Keyano College, Lakeland College, Lethbridge College, Medicine Hat College, NorQuest College, Northern Lakes College, Olds College, and Portage College. In addition, there are also four polytechnic institutes that provide specific career training and provides apprenticeships and diplomas, the Northern Alberta Institute of Technology, the Southern Alberta Institute of Technology, Northwestern Polytechnic, and Red Deer Polytechnic. The Banff Centre for Arts and Creativity is a specialized arts and cultural institution that is also empowered to provide diploma programs under the "Post-secondary Learning Act".
Alberta is also home to five independent postsecondary institutions that provide diplomas/degrees for approved programming, Ambrose University, Burman University, Concordia University of Edmonton, The King's University, and St. Mary's University. Although the five institutions operate under their own legislation, they remain partly governed by the province's "Post-secondary Learning Act". In addition to these institutions, there are also 190 private career colleges in Alberta. 
There was some controversy in 2005 over the rising cost of post-secondary education for students (as opposed to taxpayers). In 2005, Premier Ralph Klein made a promise that he would freeze tuition and look into ways of reducing schooling costs.
Health care.
Alberta provides a publicly funded, fully integrated health system, through Alberta Health Services (AHS)—a quasi-independent agency that delivers health care on behalf of the Government of Alberta's Ministry of Health. The Alberta government provides health services for all its residents as set out by the provisions of the "Canada Health Act" of 1984. Alberta became Canada's second province (after Saskatchewan) to adopt a Tommy Douglas-style program in 1950, a precursor to the modern medicare system.
Alberta's health care budget was $22.5 billion during the 2018–2019 fiscal year (approximately 45% of all government spending), making it the best-funded health-care system per-capita in Canada. Every hour the province spends more than $2.5 million, (or $60 million per day), to maintain and improve health care in the province.
Notable health, education, research, and resources facilities in Alberta, all of which are located within Calgary or Edmonton. Health centres in Calgary include:
Health centres in Edmonton include:
The Edmonton Clinic complex, completed in 2012, provides a similar research, education, and care environment as the Mayo Clinic in the United States.
All public health care services funded by the Government of Alberta are delivered operationally by Alberta Health Services. AHS is the province's single health authority, established on July 1, 2008, which replaced nine regional health authorities. AHS also funds all ground ambulance services in the province, as well as the province-wide Shock Trauma Air Rescue Service (STARS) air ambulance service.
Transportation.
Air.
Alberta is well-connected by air, with international airports in both Calgary and Edmonton. Calgary International Airport and Edmonton International Airport are the fourth- and fifth-busiest in Canada, respectively. Calgary's airport is a hub for WestJet Airlines and a regional hub for Air Canada, primarily serving the prairie provinces (Alberta, Saskatchewan and Manitoba) for connecting flights to British Columbia, eastern Canada, fifteen major United States centres, nine European airports, one Asian airport and four destinations in Mexico and the Caribbean. Edmonton's airport acts as a hub for the Canadian north and has connections to all major Canadian airports as well as airports in the United States, Europe, Mexico, and the Caribbean .
Public transit.
Calgary, Edmonton, Red Deer, Medicine Hat, and Lethbridge have substantial public transit systems. In addition to buses, Calgary and Edmonton operate light rail transit (LRT) systems. Edmonton LRT, which is underground in the downtown core and on the surface outside the downtown core was the first of the modern generation of light rail systems to be built in North America, while the Calgary CTrain has one of the highest numbers of daily riders of any LRT system in North America.
Rail.
There are more than of operating mainline railway in Alberta. The vast majority of this trackage is owned by the Canadian Pacific Kansas City (CPKC) and Canadian National Railway (CN) companies, which operate freight transport across the province. Additional railfreight service in the province is provided by two shortline railways: the Battle River Railway and Forty Mile Rail.
Passenger trains include Via Rail's "Canadian" (Toronto–Vancouver) and Jasper–Prince Rupert trains, which use the CN mainline and pass through Jasper National Park and parallel the Yellowhead Highway during at least part of their routes. The Rocky Mountaineer operates two sections: one from Vancouver to Banff over CP tracks, and a section that travels over CN tracks to Jasper.
Alberta's premier, Danielle Smith has also confirmed a 15-year master plan to expand passenger rail into Alberta. This plan is set to provide rail services to Lethbridge, Medicine Hat, Banff, Grand Prairie, Fort McMurray, and most importantly an intercity rail service between Edmonton and Calgary, as well as a commuter rail systems in the respective cities. Groundbreaking is set to start in 2027, according to Transportation Minister Devin Dreeshen.
Road.
Alberta has over of highways and roads in its road network. The main north–south corridor is Highway 2, which begins south of Cardston at the Carway border crossing and is part of the CANAMEX Corridor. Beginning at the Coutts border crossing and ending at Lethbridge, Highway 4, effectively extends Interstate 15 into Alberta and is the busiest United States gateway to the province. Highway 3 joins Lethbridge to Fort Macleod and links Highway 2 to Highway 4. Highway 2 travels north through Fort Macleod, Calgary, Red Deer, and Edmonton.
North of Edmonton, the highway continues to Athabasca, then northwesterly along the south shore of Lesser Slave Lake into High Prairie, north to Peace River, west to Fairview and finally south to Grande Prairie, where it ends at an interchange with Highway 43. The section of Highway 2 between Calgary and Edmonton has been named the Queen Elizabeth II Highway to commemorate the visit of the monarch in 2005. Highway 2 is supplemented by two more highways that run parallel to it: Highway 22, west of Highway 2, known as "Cowboy Trail", and Highway 21, east of Highway 2. Highway 43 travels northwest into Grande Prairie and the Peace River Country. Travelling northeast from Edmonton, the Highway 63 connects to Fort McMurrayand the Athabasca oil sands.
Alberta has two main east–west corridors. The southern corridor, part of the Trans-Canada Highway system, enters the province near Medicine Hat, runs westward through Calgary, and leaves Alberta through Banff National Park. The northern corridor, also part of the Trans-Canada network and known as the Yellowhead Highway (Highway 16), runs west from Lloydminster in eastern Alberta, through Edmonton and Jasper National Park into British Columbia. One of the most scenic drives is along the Icefields Parkway, which runs for between Jasper and Lake Louise, with mountain ranges and glaciers on either side of its entire length. A third corridor stretches across southern Alberta; Highway 3 runs between Crowsnest Pass and Medicine Hat through Lethbridge and forms the eastern portion of the Crowsnest Highway. Another major corridor through central Alberta is Highway 11 (also known as the David Thompson Highway), which runs east from the Saskatchewan River Crossing in Banff National Park through Rocky Mountain House and Red Deer, connecting with Highway 12, west of Stettler. The highway connects many of the smaller towns in central Alberta with Calgary and Edmonton, as it crosses Highway 2 just west of Red Deer.
Urban stretches of Alberta's major highways and freeways are often called "trails". For example, Highway 2, the main north–south highway in the province, is called Deerfoot Trail as it passes through Calgary but becomes Calgary Trail (southbound) and Gateway Boulevard (northbound) as it enters Edmonton and then turns into St. Albert Trail as it leaves Edmonton for the City of St. Albert. Calgary, in particular, has a tradition of calling its largest urban expressways "trails" and naming many of them after prominent First Nations individuals and tribes, such as Crowchild Trail, Deerfoot Trail, and Stoney Trail.
Friendship partners.
Alberta has relationships with many provinces, states, and other entities worldwide.

</doc>
<doc id="727" url="?curid=727" title="Astronomy/History">
Astronomy/History


</doc>
<doc id="728" url="?curid=728" title="List of anthropologists">
List of anthropologists

Anthropologist

</doc>
<doc id="731" url="?curid=731" title="Astronomy and Astrophysics/History">
Astronomy and Astrophysics/History


</doc>
<doc id="734" url="?curid=734" title="Actinopterygii">
Actinopterygii

Actinopterygii (; ), members of which are known as ray-finned fish or actinopterygians, is a class of bony fish that comprise over 50% of living vertebrate species. They are so called because of their lightly built fins made of webbings of skin supported by radially extended thin bony spines called "lepidotrichia", as opposed to the bulkier, fleshy lobed fins of the sister class Sarcopterygii (lobe-finned fish). Resembling folding fans, the actinopterygian fins can easily change shape and wetted area, providing superior thrust-to-weight ratios per movement compared to sarcopterygian and chondrichthyian fins. The fin rays attach directly to the proximal or basal skeletal elements, the radials, which represent the articulation between these fins and the internal skeleton (e.g., pelvic and pectoral girdles).
The vast majority of actinopterygians are teleosts. By species count, they dominate the subphylum Vertebrata, and constitute nearly 99% of the over 30,000 extant species of fish. They are the most abundant nektonic aquatic animals and are ubiquitous throughout freshwater and marine environments from the deep sea to subterranean waters to the highest mountain streams. Extant species can range in size from "Paedocypris", at ; to the massive ocean sunfish, at ; and to the giant oarfish, at . The largest ever known ray-finned fish, the extinct "Leedsichthys" from the Jurassic, has been estimated to have grown to .
Characteristics.
Ray-finned fishes occur in many variant forms. The main features of typical ray-finned fish are shown in the adjacent diagram.
The swim bladder is a more derived structure and used for buoyancy. Except from the bichirs, which just like the lungs of lobe-finned fish have retained the ancestral condition of ventral budding from the foregut, the swim bladder in ray-finned fishes derives from a dorsal bud above the foregut. In early forms the swim bladder could still be used for breathing, a trait still present in Holostei (bowfins and gars). In some fish like the arapaima, the swim bladder has been modified for breathing air again, and in other lineages it have been completely lost.
Ray-finned fishes have many different types of scales; but all teleosts have leptoid scales. The outer part of these scales fan out with bony ridges, while the inner part is crossed with fibrous connective tissue. Leptoid scales are thinner and more transparent than other types of scales, and lack the hardened enamel- or dentine-like layers found in the scales of many other fish. Unlike ganoid scales, which are found in non-teleost actinopterygians, new scales are added in concentric layers as the fish grows.
Teleosts and chondrosteans (sturgeons and paddlefish) also differ from the bichirs and holosteans (bowfin and gars) in having gone through a whole-genome duplication (paleopolyploidy). The WGD is estimated to have happened about 320 million years ago in the teleosts, which on average has retained about 17% of the gene duplicates, and around 180 (124–225) million years ago in the chondrosteans . It has since happened again in some teleost lineages, like Salmonidae (80–100 million years ago) and several times independently within the Cyprinidae (in goldfish and common carp as recently as 14 million years ago). 
Body shapes and fin arrangements.
Ray-finned fish vary in size and shape, in their feeding specializations, and in the number and arrangement of their ray-fins.
Reproduction.
In nearly all ray-finned fish, the sexes are separate, and in most species the females spawn eggs that are fertilized externally, typically with the male inseminating the eggs after they are laid. Development then proceeds with a free-swimming larval stage. However other patterns of ontogeny exist, with one of the commonest being sequential hermaphroditism. In most cases this involves protogyny, fish starting life as females and converting to males at some stage, triggered by some internal or external factor. Protandry, where a fish converts from male to female, is much less common than protogyny.
Most families use external rather than internal fertilization. Of the oviparous teleosts, most (79%) do not provide parental care. Viviparity, ovoviviparity, or some form of parental care for eggs, whether by the male, the female, or both parents is seen in a significant fraction (21%) of the 422 teleost families; no care is likely the ancestral condition. The oldest case of viviparity in ray-finned fish is found in Middle Triassic species of "Saurichthys". Viviparity is relatively rare and is found in about 6% of living teleost species; male care is far more common than female care. Male territoriality "preadapts" a species for evolving male parental care.
There are a few examples of fish that self-fertilise. The mangrove rivulus is an amphibious, simultaneous hermaphrodite, producing both eggs and spawn and having internal fertilisation. This mode of reproduction may be related to the fish's habit of spending long periods out of water in the mangrove forests it inhabits. Males are occasionally produced at temperatures below and can fertilise eggs that are then spawned by the female. This maintains genetic variability in a species that is otherwise highly inbred.
Classification and fossil record.
Actinopterygii is divided into the classes Cladistia and Actinopteri. The latter comprises the subclasses Chondrostei and Neopterygii. The Neopterygii, in turn, is divided into the infraclasses Holostei and Teleostei. During the Mesozoic (Triassic, Jurassic, Cretaceous) and Cenozoic the teleosts in particular diversified widely. As a result, 96% of living fish species are teleosts (40% of all fish species belong to the teleost subgroup Acanthomorpha), while all other groups of actinopterygians represent depauperate lineages.
The classification of ray-finned fishes can be summarized as follows:
The cladogram below shows the main clades of living actinopterygians and their evolutionary relationships to other extant groups of fishes and the four-limbed vertebrates (tetrapods). The latter include mostly terrestrial species but also groups that became secondarily aquatic (e.g. whales and dolphins). Tetrapods evolved from a group of bony fish during the Devonian period. Approximate divergence dates for the different actinopterygian clades (in millions of years, mya) are from Near et al., 2012.
The polypterids (bichirs and reedfish) are the sister lineage of all other actinopterygians, the Acipenseriformes (sturgeons and paddlefishes) are the sister lineage of Neopterygii, and Holostei (bowfin and gars) are the sister lineage of teleosts. The Elopomorpha (eels and tarpons) appear to be the most basal teleosts.
The earliest known fossil actinopterygian is "Andreolepis hedei", dating back 420 million years (Late Silurian), remains of which have been found in Russia, Sweden, and Estonia. Crown group actinopterygians most likely originated near the Devonian-Carboniferous boundary. The earliest fossil relatives of modern teleosts are from the Triassic period ("Prohalecites", "Pholidophorus"), although it is suspected that teleosts originated already during the Paleozoic Era.
Taxonomy.
The listing below is a summary of all extinct (indicated by a dagger, †) and living groups of Actinopterygii with their respective taxonomic rank. The taxonomy follows Phylogenetic Classification of Bony Fishes with notes when this differs from Nelson, ITIS and FishBase and extinct groups from Van der Laan 2016 and Xu 2021.

</doc>
<doc id="735" url="?curid=735" title="Al Gore/Criticisms">
Al Gore/Criticisms


</doc>
<doc id="736" url="?curid=736" title="Albert Einstein">
Albert Einstein

Albert Einstein ( ; ; 14 March 1879 – 18 April 1955) was a German-born theoretical physicist who is widely held as one of the most influential scientists. Best known for developing the theory of relativity, Einstein also made important contributions to quantum mechanics. His mass–energy equivalence formula , which arises from relativity theory, has been called "the world's most famous equation". He received the 1921 Nobel Prize in Physics "for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect", a pivotal step in the development of quantum theory. His intellectual achievements and originality have made the word "Einstein" broadly synonymous with "genius". 
Born in the German Empire, Einstein moved to Switzerland in 1895, forsaking his German citizenship (as a subject of the Kingdom of Württemberg) the following year. In 1897, at the age of seventeen, he enrolled in the mathematics and physics teaching diploma program at the Swiss federal polytechnic school in Zürich, graduating in 1900. In 1901, he acquired Swiss citizenship, which he kept for the rest of his life. In 1903, he secured a permanent position at the Swiss Patent Office in Bern. In 1905, he submitted a successful PhD dissertation to the University of Zurich. In 1914, he moved to Berlin in order to join the Prussian Academy of Sciences and the Humboldt University of Berlin. In 1917, he became director of the Kaiser Wilhelm Institute for Physics; he also became a German citizen again, this time as a subject of the Kingdom of Prussia. In 1933, while he was visiting the United States, Adolf Hitler came to power in Germany. Horrified by the Nazi war of extermination against his fellow Jews, Einstein decided to remain in the US, and was granted American citizenship in 1940. On the eve of World War II, he endorsed a letter to President Franklin D. Roosevelt alerting him to the potential German nuclear weapons program and recommended that the US begin similar research. Einstein supported the Allies but generally viewed the idea of nuclear weapons with great dismay.
Einstein's work is also known for its influence on the philosophy of science. In 1905, he published four groundbreaking papers, sometimes described as his "annus mirabilis" (miracle year). These papers outlined a theory of the photoelectric effect, explained Brownian motion, introduced his special theory of relativity—a theory which addressed the inability of classical mechanics to account satisfactorily for the behavior of the electromagnetic field—and demonstrated that if the special theory is correct, mass and energy are equivalent to each other. In 1915, he proposed a general theory of relativity that extended his system of mechanics to incorporate gravitation. A cosmological paper that he published the following year laid out the implications of general relativity for the modeling of the structure and evolution of the universe as a whole.
In the middle part of his career, Einstein made important contributions to statistical mechanics and quantum theory. Especially notable was his work on the quantum physics of radiation, in which light consists of particles, subsequently called photons. With the Indian physicist Satyendra Nath Bose, he laid the groundwork for Bose-Einstein statistics. For much of the last phase of his academic life, Einstein worked on two endeavors that proved ultimately unsuccessful. First, he advocated against quantum theory's introduction of fundamental randomness into science's picture of the world, objecting that "God does not play dice". Second, he attempted to devise a unified field theory by generalizing his geometric theory of gravitation to include electromagnetism too. As a result, he became increasingly isolated from the mainstream modern physics. In a 1999 poll of 130 leading physicists worldwide by the British journal "Physics World", Einstein was ranked the greatest physicist of all time.
Life and career.
Childhood, youth and education.
Albert Einstein was born in Ulm, in the Kingdom of Württemberg in the German Empire, on 14 March 1879. His parents, secular Ashkenazi Jews, were Hermann Einstein, a salesman and engineer, and Pauline Koch. In 1880, the family moved to Munich's borough of Ludwigsvorstadt-Isarvorstadt, where Einstein's father and his uncle Jakob founded Elektrotechnische Fabrik J. Einstein &amp; Cie, a company that manufactured electrical equipment based on direct current.
Albert attended St. Peter‘s Catholic elementary school in Munich from the age of five. When he was eight, he was transferred to the Luitpold Gymnasium, where he received advanced primary and then secondary school education.
In 1894, Hermann and Jakob's company tendered for a contract to install electric lighting in Munich, but without success—they lacked the capital that would have been required to update their technology from direct current to the more efficient, alternating current alternative. The failure of their bid forced them to sell their Munich factory and search for new opportunities elsewhere. The Einstein family moved to Italy, first to Milan and a few months later to Pavia, where they settled in Palazzo Cornazzani. Einstein, then fifteen, stayed behind in Munich in order to finish his schooling. His father wanted him to study electrical engineering, but he was a fractious pupil who found the Gymnasium's regimen and teaching methods far from congenial. He later wrote that the school's policy of strict rote learning was harmful to creativity. At the end of December 1894, a letter from a doctor persuaded the Luitpold's authorities to release him from its care, and he joined his family in Pavia. While in Italy as a teenager, he wrote an essay entitled "On the Investigation of the State of the Ether in a Magnetic Field".
Einstein excelled at physics and mathematics from an early age, and soon acquired the mathematical expertise normally only found in a child several years his senior. He began teaching himself algebra, calculus and Euclidean geometry when he was twelve; he made such rapid progress that he discovered an original proof of the Pythagorean theorem before his thirteenth birthday. A family tutor, Max Talmud, said that only a short time after he had given the twelve year old Einstein a geometry textbook, the boy "had worked through the whole book. He thereupon devoted himself to higher mathematics ... Soon the flight of his mathematical genius was so high I could not follow." Einstein recorded that he had "mastered integral and differential calculus" while still just fourteen. His love of algebra and geometry was so great that at twelve, he was already confident that nature could be understood as a "mathematical structure".
At thirteen, when his range of enthusiasms had broadened to include music and philosophy, Talmud introduced Einstein to Kant's "Critique of Pure Reason". Kant became his favorite philosopher; according to Talmud, "At the time he was still a child, only thirteen years old, yet Kant's works, incomprehensible to ordinary mortals, seemed to be clear to him."
In 1895, at the age of sixteen, Einstein sat the entrance examination for the federal polytechnic school (later the Eidgenössische Technische Hochschule, ETH) in Zürich, Switzerland. He failed to reach the required standard in the general part of the test, but performed with distinction in physics and mathematics. On the advice of the polytechnic's principal, he completed his secondary education at the Argovian cantonal school (a "gymnasium") in Aarau, Switzerland, graduating in 1896. While lodging in Aarau with the family of Jost Winteler, he fell in love with Winteler's daughter, Marie. (His sister, Maja, later married Winteler's son Paul.)
In January 1896, with his father's approval, Einstein renounced his citizenship of the German Kingdom of Württemberg in order to avoid conscription into military service. The "Matura" (graduation for the successful completion of higher secondary schooling), awarded to him in September 1896, acknowledged him to have performed well across most of the curriculum, allotting him a top grade of 6 for history, physics, algebra, geometry, and descriptive geometry. At seventeen, he enrolled in the four-year mathematics and physics teaching diploma program at the federal polytechnic school. Marie Winteler, a year older than him, took up a teaching post in Olsberg, Switzerland.
The five other polytechnic school freshmen following the same course as Einstein included just one woman, a twenty year old Serbian, Mileva Marić. Over the next few years, the pair spent many hours discussing their shared interests and learning about topics in physics that the polytechnic school's lectures did not cover. In his letters to Marić, Einstein confessed that exploring science with her by his side was much more enjoyable than reading a textbook in solitude. Eventually the two students became not only friends but also lovers.
Historians of physics are divided on the question of the extent to which Marić contributed to the insights of Einstein's "annus mirabilis" publications. There is at least some evidence that he was influenced by her scientific ideas, but there are scholars who doubt whether her impact on his thought was of any great significance at all.
Marriages, relationships and children.
Correspondence between Einstein and Marić, discovered and published in 1987, revealed that in early 1902, while Marić was visiting her parents in Novi Sad, she gave birth to a daughter, Lieserl. When Marić returned to Switzerland it was without the child, whose fate is uncertain. A letter of Einstein's that he wrote in September 1903 suggests that the girl was either given up for adoption or died of scarlet fever in infancy.
Einstein and Marić married in January 1903. In May 1904, their son Hans Albert was born in Bern, Switzerland. Their son Eduard was born in Zürich in July 1910. In letters that Einstein wrote to Marie Winteler in the months before Eduard's arrival, he described his love for his wife as "misguided" and mourned the "missed life" that he imagined he would have enjoyed if he had married Winteler instead: "I think of you in heartfelt love every spare minute and am so unhappy as only a man can be."
In 1912, Einstein entered into a relationship with Elsa Löwenthal, who was both his first cousin on his mother's side and his second cousin on his father's. When Marić learned of his infidelity soon after moving to Berlin with him in April 1914, she returned to Zürich, taking Hans Albert and Eduard with her. Einstein and Marić were granted a divorce on 14 February 1919 on the grounds of having lived apart for five years. As part of the divorce settlement, Einstein agreed that if he were to win a Nobel Prize, he would give the money that he received to Marić; he won the prize two years later.
Einstein married Löwenthal in 1919. In 1923, he began a relationship with a secretary named Betty Neumann, the niece of his close friend Hans Mühsam. Löwenthal nevertheless remained loyal to him, accompanying him when he emigrated to the United States in 1933. In 1935, she was diagnosed with heart and kidney problems. She died in December 1936.
A volume of Einstein's letters released by Hebrew University of Jerusalem in 2006 added further names to the catalog of women with whom he was romantically involved. They included Margarete Lebach (a married Austrian), Estella Katzenellenbogen (the rich owner of a florist business), Toni Mendel (a wealthy Jewish widow) and Ethel Michanowski (a Berlin socialite), with whom he spent time and from whom he accepted gifts while married to Löwenthal. After being widowed, Einstein was briefly in a relationship with Margarita Konenkova, thought by some to be a Russian spy; her husband, the Russian sculptor Sergei Konenkov, created the bronze bust of Einstein at the Institute for Advanced Study at Princeton.
Following an episode of acute mental illness at about the age of twenty, Einstein's son Eduard was diagnosed with schizophrenia. He spent the remainder of his life either in the care of his mother or in temporary confinement in an asylum. After her death, he was committed permanently to Burghölzli, the Psychiatric University Hospital in Zürich.
1902–1909: Assistant at the Swiss Patent Office.
Einstein graduated from the federal polytechnic school in 1900, duly certified as competent to teach mathematics and physics. His successful acquisition of Swiss citizenship in February 1901 was not followed by the usual sequel of conscription; the Swiss authorities deemed him medically unfit for military service. He found that Swiss schools too appeared to have no use for him, failing to offer him a teaching position despite the almost two years that he spent applying for one. Eventually it was with the help of Marcel Grossmann's father that he secured a post in Bern at the Swiss Patent Office, as an assistant examiner – level III.
Patent applications that landed on Einstein's desk for his evaluation included ideas for a gravel sorter and an electric typewriter. His employers were pleased enough with his work to make his position permanent in 1903, although they did not think that he should be promoted until he had "fully mastered machine technology". It is conceivable that his labors at the patent office had a bearing on his development of his special theory of relativity. He arrived at his revolutionary ideas about space, time and light through thought experiments about the transmission of signals and the synchronization of clocks, matters which also figured in some of the inventions submitted to him for assessment.
In 1902, Einstein and some friends whom he had met in Bern formed a group that held regular meetings to discuss science and philosophy. Their choice of a name for their club, the Olympia Academy, was an ironic comment upon its far from Olympian status. Sometimes they were joined by Marić, who limited her participation in their proceedings to careful listening. The thinkers whose works they reflected upon included Henri Poincaré, Ernst Mach and David Hume, all of whom significantly influenced Einstein's own subsequent ideas and beliefs.
1900–1905: First scientific papers.
Einstein's first paper, "Folgerungen aus den Capillaritätserscheinungen" ("Conclusions drawn from the phenomena of capillarity"), in which he proposed a model of intermolecular attraction that he afterwards disavowed as worthless, was published in the journal "Annalen der Physik" in 1901. His 24-page doctoral dissertation also addressed a topic in molecular physics. Titled "Eine neue Bestimmung der Moleküldimensionen" ("A New Determination of Molecular Dimensions") and dedicated to his friend Marcel Grossman, it was completed on 30 April 1905 and approved by Professor Alfred Kleiner of the University of Zurich three months later. (Einstein was formally awarded his PhD on 15 January 1906.) Four other pieces of work that Einstein completed in 1905—his famous papers on the photoelectric effect, Brownian motion, his special theory of relativity and the equivalence of mass and energy—have led to the year being celebrated as an "annus mirabilis" for physics akin to 1666 (the year in which Isaac Newton experienced his greatest epiphanies). The publications deeply impressed Einstein's contemporaries.
1908–1933: Early academic career.
Einstein's sabbatical as a civil servant approached its end in 1908, when he secured a junior teaching position at the University of Bern. In 1909, a lecture on relativistic electrodynamics that he gave at the University of Zurich, much admired by Alfred Kleiner, led to Zürich's luring him away from Bern with a newly created associate professorship. Promotion to a full professorship followed in April 1911, when he accepted a chair at the German Charles-Ferdinand University in Prague, a move which required him to become an Austrian citizen of the Austro-Hungarian Empire. His time in Prague saw him producing eleven research papers.
In July 1912, he returned to his "alma mater", the ETH Zurich, to take up a chair in theoretical physics. His teaching activities there centred on thermodynamics and analytical mechanics, and his research interests included the molecular theory of heat, continuum mechanics and the development of a relativistic theory of gravitation. In his work on the latter topic, he was assisted by his friend, Marcel Grossmann, whose knowledge of the kind of mathematics required was greater than his own.
In the spring of 1913, two German visitors, Max Planck and Walther Nernst, called upon Einstein in Zürich in the hope of persuading him to relocate to Berlin. They offered him membership of the Prussian Academy of Sciences, the directorship of the planned Kaiser Wilhelm Institute for Physics and a chair at the Humboldt University of Berlin that would allow him to pursue his research supported by a professorial salary but with no teaching duties to burden him. Their invitation was all the more appealing to him because Berlin happened to be the home of his latest girlfriend, Elsa Löwenthal. He duly joined the Academy on 24 July 1913, and moved into an apartment in the Berlin district of Dahlem on 1 April 1914. He was installed in his Humboldt University position shortly thereafter.
The outbreak of the First World War in July 1914 marked the beginning of Einstein's gradual estrangement from the nation of his birth. When the "Manifesto of the Ninety-Three" was published in October 1914—a document signed by a host of prominent German thinkers that justified Germany's belligerence—Einstein was one of the few German intellectuals to distance himself from it and sign the alternative, eirenic "Manifesto to the Europeans" instead. However, this expression of his doubts about German policy did not prevent him from being elected to a two-year term as president of the German Physical Society in 1916. When the Kaiser Wilhelm Institute for Physics opened its doors the following year—its foundation delayed because of the war—Einstein was appointed its first director, just as Planck and Nernst had promised.
Einstein was elected a Foreign Member of the Royal Netherlands Academy of Arts and Sciences in 1920, and a Foreign Member of the Royal Society in 1921. In 1922, he was awarded the 1921 Nobel Prize in Physics "for his services to Theoretical Physics, and especially for his discovery of the law of the photoelectric effect". At this point some physicists still regarded the general theory of relativity sceptically, and the Nobel citation displayed a degree of doubt even about the work on photoelectricity that it acknowledged: it did not assent to Einstein's notion of the particulate nature of light, which only won over the entire scientific community when S. N. Bose derived the Planck spectrum in 1924. That same year, Einstein was elected an International Honorary Member of the American Academy of Arts and Sciences. Britain's closest equivalent of the Nobel award, the Royal Society's Copley Medal, was not hung around Einstein's neck until 1925. He was elected an International Member of the American Philosophical Society in 1930.
Einstein resigned from the Prussian Academy in March 1933. His accomplishments in Berlin had included the completion of the general theory of relativity, proving the Einstein–de Haas effect, contributing to the quantum theory of radiation, and the development of Bose–Einstein statistics.
1919: Putting general relativity to the test.
In 1907, Einstein reached a milestone on his long journey from his special theory of relativity to a new idea of gravitation with the formulation of his equivalence principle, which asserts that an observer in an infinitesimally small box falling freely in a gravitational field would be unable to find any evidence that the field exists. In 1911, he used the principle to estimate the amount by which a ray of light from a distant star would be bent by the gravitational pull of the Sun as it passed close to the Sun's photosphere (that is, the Sun's apparent surface). He reworked his calculation in 1913, having now found a way to model gravitation with the Riemann curvature tensor of a non-Euclidean four-dimensional spacetime. By the fall of 1915, his reimagining of the mathematics of gravitation in terms of Riemannian geometry was complete, and he applied his new theory not just to the behavior of the Sun as a gravitational lens but also to another astronomical phenomenon, the precession of the perihelion of Mercury (a slow drift in the point in Mercury's elliptical orbit at which it approaches the Sun most closely). A total eclipse of the Sun that took place on 29 May 1919 provided an opportunity to put his theory of gravitational lensing to the test, and observations performed by Sir Arthur Eddington yielded results that were consistent with his calculations. Eddington's work was reported at length in newspapers around the world. On 7 November 1919, for example, the leading British newspaper, "The Times", printed a banner headline that read: "Revolution in Science – New Theory of the Universe – Newtonian Ideas Overthrown".
1921–1923: Coming to terms with fame.
With Eddington's eclipse observations widely reported not just in academic journals but by the popular press as well, Einstein became "perhaps the world's first celebrity scientist", a genius who had shattered a paradigm that had been basic to physicists' understanding of the universe since the seventeenth century.
Einstein began his new life as an intellectual icon in America, where he arrived on 2 April 1921. He was welcomed to New York City by Mayor John Francis Hylan, and then spent three weeks giving lectures and attending receptions. He spoke several times at Columbia University and Princeton, and in Washington, he visited the White House with representatives of the National Academy of Sciences. He returned to Europe via London, where he was the guest of the philosopher and statesman Viscount Haldane. He used his time in the British capital to meet several people prominent in British scientific, political or intellectual life, and to deliver a lecture at King's College. In July 1921, he published an essay, "My First Impression of the U.S.A.", in which he sought to sketch the American character, much as had Alexis de Tocqueville in "Democracy in America" (1835). He wrote of his transatlantic hosts in highly approving terms: "What strikes a visitor is the joyous, positive attitude to life ... The American is friendly, self-confident, optimistic, and without envy."
In 1922, Einstein's travels were to the old world rather than the new. He devoted six months to a tour of Asia that saw him speaking in Japan, Singapore and Sri Lanka (then known as Ceylon). After his first public lecture in Tokyo, he met Emperor Yoshihito and his wife at the Imperial Palace, with thousands of spectators thronging the streets in the hope of catching a glimpse of him. (In a letter to his sons, he wrote that Japanese people seemed to him to be generally modest, intelligent and considerate, and to have a true appreciation of art. But his picture of them in his diary was less flattering: "[the] intellectual needs of this nation seem to be weaker than their artistic ones – natural disposition?" His journal also contains views of China and India which were uncomplimentary. Of Chinese people, he wrote that "even the children are spiritless and look obtuse... It would be a pity if these Chinese supplant all other races. For the likes of us the mere thought is unspeakably dreary".) He was greeted with even greater enthusiasm on the last leg of his tour, in which he spent twelve days in Mandatory Palestine, newly entrusted to British rule by the League of Nations in the aftermath of the First World War. Sir Herbert Samuel, the British High Commissioner, welcomed him with a degree of ceremony normally only accorded to a visiting head of state, including a cannon salute. One reception held in his honor was stormed by people determined to hear him speak: he told them that he was happy that Jews were beginning to be recognized as a force in the world.
Einstein's decision to tour the eastern hemisphere in 1922 meant that he was unable to go to Stockholm in the December of that year to participate in the Nobel prize ceremony. His place at the traditional Nobel banquet was taken by a German diplomat, who gave a speech praising him not only as a physicist but also as a campaigner for peace. A two-week visit to Spain that he undertook in 1923 saw him collecting another award, a membership of the Spanish Academy of Sciences signified by a diploma handed to him by King Alfonso XIII. (His Spanish trip also gave him a chance to meet a fellow Nobel laureate, the neuroanatomist Santiago Ramón y Cajal.)
1922–1932: Serving the League of Nations.
From 1922 until 1932, with the exception of a few months in 1923 and 1924, Einstein was a member of the Geneva-based International Committee on Intellectual Cooperation of the League of Nations, a group set up by the League to encourage scientists, artists, scholars, teachers and other people engaged in the life of the mind to work more closely with their counterparts in other countries. He was appointed as a German delegate rather than as a representative of Switzerland because of the machinations of two Catholic activists, Oskar Halecki and Giuseppe Motta. By persuading Secretary General Eric Drummond to deny Einstein the place on the committee reserved for a Swiss thinker, they created an opening for Gonzague de Reynold, who used his League of Nations position as a platform from which to promote traditional Catholic doctrine. Einstein's former physics professor Hendrik Lorentz and the Polish chemist Marie Curie were also members of the committee.
1925: Touring South America.
In March and April 1925, Einstein and his wife visited South America, where they spent about a week in Brazil, a week in Uruguay and a month in Argentina. Their tour was suggested by Jorge Duclout (1856–1927) and Mauricio Nirenstein (1877–1935) with the support of several Argentine scholars, including Julio Rey Pastor, Jakob Laub, and Leopoldo Lugones. and was financed primarily by the Council of the University of Buenos Aires and the "Asociación Hebraica Argentina" (Argentine Hebraic Association) with a smaller contribution from the Argentine-Germanic Cultural Institution.
1930–1931: Touring the US.
In December 1930, Einstein began another significant sojourn in the United States, drawn back to the US by the offer of a two month research fellowship at the California Institute of Technology. Caltech supported him in his wish that he should not be exposed to quite as much attention from the media as he had experienced when visiting the US in 1921, and he therefore declined all the invitations to receive prizes or make speeches that his admirers poured down upon him. But he remained willing to allow his fans at least some of the time with him that they requested.
After arriving in New York City, Einstein was taken to various places and events, including Chinatown, a lunch with the editors of "The New York Times", and a performance of "Carmen" at the Metropolitan Opera, where he was cheered by the audience on his arrival. During the days following, he was given the keys to the city by Mayor Jimmy Walker and met Nicholas Murray Butler, the president of Columbia University, who described Einstein as "the ruling monarch of the mind". Harry Emerson Fosdick, pastor at New York's Riverside Church, gave Einstein a tour of the church and showed him a full-size statue that the church made of Einstein, standing at the entrance. Also during his stay in New York, he joined a crowd of 15,000 people at Madison Square Garden during a Hanukkah celebration.
Einstein next traveled to California, where he met Caltech president and Nobel laureate Robert A. Millikan. His friendship with Millikan was "awkward", as Millikan "had a penchant for patriotic militarism", where Einstein was a pronounced pacifist. During an address to Caltech's students, Einstein noted that science was often inclined to do more harm than good.
This aversion to war also led Einstein to befriend author Upton Sinclair and film star Charlie Chaplin, both noted for their pacifism. Carl Laemmle, head of Universal Studios, gave Einstein a tour of his studio and introduced him to Chaplin. They had an instant rapport, with Chaplin inviting Einstein and his wife, Elsa, to his home for dinner. Chaplin said Einstein's outward persona, calm and gentle, seemed to conceal a "highly emotional temperament", from which came his "extraordinary intellectual energy".
Chaplin's film "City Lights" was to premiere a few days later in Hollywood, and Chaplin invited Einstein and Elsa to join him as his special guests. Walter Isaacson, Einstein's biographer, described this as "one of the most memorable scenes in the new era of celebrity". Chaplin visited Einstein at his home on a later trip to Berlin and recalled his "modest little flat" and the piano at which he had begun writing his theory. Chaplin speculated that it was "possibly used as kindling wood by the Nazis".
1933: Emigration to the US.
In February 1933, while on a visit to the United States, Einstein knew he could not return to Germany with the rise to power of the Nazis under Germany's new chancellor, Adolf Hitler.
While at American universities in early 1933, he undertook his third two-month visiting professorship at the California Institute of Technology in Pasadena. In February and March 1933, the Gestapo repeatedly raided his family's apartment in Berlin. He and his wife Elsa returned to Europe in March, and during the trip, they learned that the German Reichstag had passed the Enabling Act on 23 March, transforming Hitler's government into a "de facto" legal dictatorship, and that they would not be able to proceed to Berlin. Later on, they heard that their cottage had been raided by the Nazis and Einstein's personal sailboat confiscated. Upon landing in Antwerp, Belgium on 28 March, Einstein immediately went to the German consulate and surrendered his passport, formally renouncing his German citizenship. The Nazis later sold his boat and converted his cottage into a Hitler Youth camp.
Refugee status.
In April 1933, Einstein discovered that the new German government had passed laws barring Jews from holding any official positions, including teaching at universities. Historian Gerald Holton describes how, with "virtually no audible protest being raised by their colleagues", thousands of Jewish scientists were suddenly forced to give up their university positions and their names were removed from the rolls of institutions where they were employed.
A month later, Einstein's works were among those targeted by the German Student Union in the Nazi book burnings, with Nazi propaganda minister Joseph Goebbels proclaiming, "Jewish intellectualism is dead." One German magazine included him in a list of enemies of the German regime with the phrase, "not yet hanged", offering a $5,000 bounty on his head. In a subsequent letter to physicist and friend Max Born, who had already emigrated from Germany to England, Einstein wrote, "... I must confess that the degree of their brutality and cowardice came as something of a surprise." After moving to the US, he described the book burnings as a "spontaneous emotional outburst" by those who "shun popular enlightenment", and "more than anything else in the world, fear the influence of men of intellectual independence".
Einstein was now without a permanent home, unsure where he would live and work, and equally worried about the fate of countless other scientists still in Germany. Aided by the Academic Assistance Council, founded in April 1933 by British Liberal politician William Beveridge to help academics escape Nazi persecution, Einstein was able to leave Germany. He rented a house in De Haan, Belgium, where he lived for a few months. In late July 1933, he visited England for about six weeks at the invitation of the British Member of Parliament Commander Oliver Locker-Lampson, who had become friends with him in the preceding years. Locker-Lampson invited him to stay near his home in a secluded wooden cabin on Roughton Heath in the Parish of . To protect Einstein, Locker-Lampson had two bodyguards watch over him; a photo of them carrying shotguns and guarding Einstein was published in the "Daily Herald" on 24 July 1933.
Locker-Lampson took Einstein to meet Winston Churchill at his home, and later, Austen Chamberlain and former Prime Minister Lloyd George. Einstein asked them to help bring Jewish scientists out of Germany. British historian Martin Gilbert notes that Churchill responded immediately, and sent his friend, physicist Frederick Lindemann, to Germany to seek out Jewish scientists and place them in British universities. Churchill later observed that as a result of Germany having driven the Jews out, they had lowered their "technical standards" and put the Allies' technology ahead of theirs.
Einstein later contacted leaders of other nations, including Turkey's Prime Minister, İsmet İnönü, to whom he wrote in September 1933 requesting placement of unemployed German-Jewish scientists. As a result of Einstein's letter, Jewish invitees to Turkey eventually totaled over "1,000 saved individuals".
Locker-Lampson also submitted a bill to parliament to extend British citizenship to Einstein, during which period Einstein made a number of public appearances describing the crisis brewing in Europe. In one of his speeches he denounced Germany's treatment of Jews, while at the same time he introduced a bill promoting Jewish citizenship in Palestine, as they were being denied citizenship elsewhere. In his speech he described Einstein as a "citizen of the world" who should be offered a temporary shelter in the UK. Both bills failed, however, and Einstein then accepted an earlier offer from the Institute for Advanced Study, in Princeton, New Jersey, US, to become a resident scholar.
Resident scholar at the Institute for Advanced Study.
On 3 October 1933, Einstein delivered a speech on the importance of academic freedom before a packed audience at the Royal Albert Hall in London, with "The Times" reporting he was wildly cheered throughout. Four days later he returned to the US and took up a position at the Institute for Advanced Study, noted for having become a refuge for scientists fleeing Nazi Germany. At the time, most American universities, including Harvard, Princeton and Yale, had minimal or no Jewish faculty or students, as a result of their Jewish quotas, which lasted until the late 1940s.
Einstein was still undecided on his future. He had offers from several European universities, including Christ Church, Oxford, where he stayed for three short periods between May 1931 and June 1933 and was offered a five-year research fellowship (called a "studentship" at Christ Church), but in 1935, he arrived at the decision to remain permanently in the United States and apply for citizenship.
Einstein's affiliation with the Institute for Advanced Study would last until his death in 1955. He was one of the four first selected (along with John von Neumann, Kurt Gödel, and Hermann Weyl) at the new Institute. He soon developed a close friendship with Gödel; the two would take long walks together discussing their work. Bruria Kaufman, his assistant, later became a physicist. During this period, Einstein tried to develop a unified field theory and to refute the accepted interpretation of quantum physics, both unsuccessfully. He lived in Princeton at his home from 1935 onwards. The Albert Einstein House was made a National Historic Landmark in 1976.
World War II and the Manhattan Project.
In 1939, a group of Hungarian scientists that included émigré physicist Leó Szilárd attempted to alert Washington to ongoing Nazi atomic bomb research. The group's warnings were discounted. Einstein and Szilárd, along with other refugees such as Edward Teller and Eugene Wigner, "regarded it as their responsibility to alert Americans to the possibility that German scientists might win the race to build an atomic bomb, and to warn that Hitler would be more than willing to resort to such a weapon." To make certain the US was aware of the danger, in July 1939, a few months before the beginning of World War II in Europe, Szilárd and Wigner visited Einstein to explain the possibility of atomic bombs, which Einstein, a pacifist, said he had never considered. He was asked to lend his support by writing a letter, with Szilárd, to President Roosevelt, recommending the US pay attention and engage in its own nuclear weapons research.
The letter is believed to be "arguably the key stimulus for the U.S. adoption of serious investigations into nuclear weapons on the eve of the U.S. entry into World War II". In addition to the letter, Einstein used his connections with the Belgian royal family and the Belgian queen mother to get access with a personal envoy to the White House's Oval Office. Some say that as a result of Einstein's letter and his meetings with Roosevelt, the US entered the "race" to develop the bomb, drawing on its "immense material, financial, and scientific resources" to initiate the Manhattan Project.
For Einstein, "war was a disease ... [and] he called for resistance to war." By signing the letter to Roosevelt, some argue he went against his pacifist principles. In 1954, a year before his death, Einstein said to his old friend, Linus Pauling, "I made one great mistake in my life—when I signed the letter to President Roosevelt recommending that atom bombs be made; but there was some justification—the danger that the Germans would make them ..." In 1955, Einstein and ten other intellectuals and scientists, including British philosopher Bertrand Russell, signed a manifesto highlighting the danger of nuclear weapons. In 1960 Einstein was included posthumously as a charter member of the World Academy of Art and Science (WAAS), an organization founded by distinguished scientists and intellectuals who committed themselves to the responsible and ethical advances of science, particularly in light of the development of nuclear weapons.
US citizenship.
Einstein became an American citizen in 1940. Not long after settling into his career at the Institute for Advanced Study in Princeton, New Jersey, he expressed his appreciation of the meritocracy in American culture compared to Europe. He recognized the "right of individuals to say and think what they pleased" without social barriers. As a result, individuals were encouraged, he said, to be more creative, a trait he valued from his early education.
Einstein joined the National Association for the Advancement of Colored People (NAACP) in Princeton, where he campaigned for the civil rights of African Americans. He considered racism America's "worst disease", seeing it as "handed down from one generation to the next". As part of his involvement, he corresponded with civil rights activist W. E. B. Du Bois and was prepared to testify on his behalf during his trial as an alleged foreign agent in 1951. When Einstein offered to be a character witness for Du Bois, the judge decided to drop the case.
In 1946, Einstein visited Lincoln University in Pennsylvania, a historically black college, where he was awarded an honorary degree. Lincoln was the first university in the United States to grant college degrees to African Americans; alumni include Langston Hughes and Thurgood Marshall. Einstein gave a speech about racism in America, adding, "I do not intend to be quiet about it." A resident of Princeton recalls that Einstein had once paid the college tuition for a black student. Einstein has said, "Being a Jew myself, perhaps I can understand and empathize with how black people feel as victims of discrimination".
Personal views.
Political views.
In 1918, Einstein was one of the signatories of the founding proclamation of the German Democratic Party, a liberal party. Later in his life, Einstein's political view was in favor of socialism and critical of capitalism, which he detailed in his essays such as "Why Socialism?". His opinions on the Bolsheviks also changed with time. In 1925, he criticized them for not having a "well-regulated system of government" and called their rule a "regime of terror and a tragedy in human history". He later adopted a more moderated view, criticizing their methods but praising them, which is shown by his 1929 remark on Vladimir Lenin:
Einstein offered and was called on to give judgments and opinions on matters often unrelated to theoretical physics or mathematics. He strongly advocated the idea of a democratic global government that would check the power of nation-states in the framework of a world federation. He wrote "I advocate world government because I am convinced that there is no other possible way of eliminating the most terrible danger in which man has ever found himself." The FBI created a secret dossier on Einstein in 1932; by the time of his death, it was 1,427 pages long.
Einstein was deeply impressed by Mahatma Gandhi, with whom he corresponded. He described Gandhi as "a role model for the generations to come". The initial connection was established on 27 September 1931, when Wilfrid Israel took his Indian guest V. A. Sundaram to meet his friend Einstein at his summer home in the town of Caputh. Sundaram was Gandhi's disciple and special envoy, whom Wilfrid Israel met while visiting India and visiting the Indian leader's home in 1925. During the visit, Einstein wrote a short letter to Gandhi that was delivered to him through his envoy, and Gandhi responded quickly with his own letter. Although in the end Einstein and Gandhi were unable to meet as they had hoped, the direct connection between them was established through Wilfrid Israel.
Relationship with Zionism.
Einstein was a figurehead leader in the establishment of the Hebrew University of Jerusalem, which opened in 1925. Earlier, in 1921, he was asked by the biochemist and president of the World Zionist Organization, Chaim Weizmann, to help raise funds for the planned university. He made suggestions for the creation of an Institute of Agriculture, a Chemical Institute and an Institute of Microbiology in order to fight the various ongoing epidemics such as malaria, which he called an "evil" that was undermining a third of the country's development. He also promoted the establishment of an Oriental Studies Institute, to include language courses given in both Hebrew and Arabic.
Einstein was not a nationalist and opposed the creation of an independent Jewish state. He felt that the waves of arriving Jews of the Aliyah could live alongside existing Arabs in Palestine. The state of Israel was established without his help in 1948; Einstein was limited to a marginal role in the Zionist movement. Upon the death of Israeli president Weizmann in November 1952, Prime Minister David Ben-Gurion offered Einstein the largely ceremonial position of President of Israel at the urging of Ezriel Carlebach. The offer was presented by Israel's ambassador in Washington, Abba Eban, who explained that the offer "embodies the deepest respect which the Jewish people can repose in any of its sons". Einstein wrote that he was "deeply moved", but "at once saddened and ashamed" that he could not accept it.
Religious and philosophical views.
Einstein expounded his spiritual outlook in a wide array of writings and interviews. He said he had sympathy for the impersonal pantheistic God of Baruch Spinoza's philosophy. He did not believe in a personal god who concerns himself with fates and actions of human beings, a view which he described as naïve. He clarified, however, that "I am not an atheist", preferring to call himself an agnostic, or a "deeply religious nonbeliever". When asked if he believed in an afterlife, Einstein replied, "No. And one life is enough for me."
Einstein was primarily affiliated with non-religious humanist and Ethical Culture groups in both the UK and US. He served on the advisory board of the First Humanist Society of New York, and was an honorary associate of the Rationalist Association, which publishes "New Humanist" in Britain. For the 75th anniversary of the New York Society for Ethical Culture, he stated that the idea of Ethical Culture embodied his personal conception of what is most valuable and enduring in religious idealism. He observed, "Without 'ethical culture' there is no salvation for humanity."
In a German-language letter to philosopher Eric Gutkind, dated 3 January 1954, Einstein wrote:The word God is for me nothing more than the expression and product of human weaknesses, the Bible a collection of honorable, but still primitive legends which are nevertheless pretty childish. No interpretation no matter how subtle can (for me) change this. ... For me the Jewish religion like all other religions is an incarnation of the most childish superstitions. And the Jewish people to whom I gladly belong and with whose mentality I have a deep affinity have no different quality for me than all other people. ... I cannot see anything 'chosen' about them.
Einstein had been sympathetic toward vegetarianism for a long time. In a letter in 1930 to Hermann Huth, vice-president of the German Vegetarian Federation (Deutsche Vegetarier-Bund), he wrote:Although I have been prevented by outward circumstances from observing a strictly vegetarian diet, I have long been an adherent to the cause in principle. Besides agreeing with the aims of vegetarianism for aesthetic and moral reasons, it is my view that a vegetarian manner of living by its purely physical effect on the human temperament would most beneficially influence the lot of mankind.
He became a vegetarian himself only during the last part of his life. In March 1954 he wrote in a letter: "So I am living without fats, without meat, without fish, but am feeling quite well this way. It almost seems to me that man was not born to be a carnivore."
Love of music.
Einstein developed an appreciation for music at an early age. In his late journals he wrote:
His mother played the piano reasonably well and wanted her son to learn the violin, not only to instill in him a love of music but also to help him assimilate into German culture. According to conductor Leon Botstein, Einstein began playing when he was 5. However, he did not enjoy it at that age.
When he turned 13, he discovered the violin sonatas of Mozart, whereupon he became enamored of Mozart's compositions and studied music more willingly. Einstein taught himself to play without "ever practicing systematically". He said that "love is a better teacher than a sense of duty". At the age of 17, he was heard by a school examiner in Aarau while playing Beethoven's violin sonatas. The examiner stated afterward that his playing was "remarkable and revealing of 'great insight. What struck the examiner, writes Botstein, was that Einstein "displayed a deep love of the music, a quality that was and remains in short supply. Music possessed an unusual meaning for this student."
Music took on a pivotal and permanent role in Einstein's life from that period on. Although the idea of becoming a professional musician himself was not on his mind at any time, among those with whom Einstein played chamber music were a few professionals, including Kurt Appelbaum, and he performed for private audiences and friends. Chamber music had also become a regular part of his social life while living in Bern, Zürich, and Berlin, where he played with Max Planck and his son, among others. He is sometimes erroneously credited as the editor of the 1937 edition of the Köchel catalog of Mozart's work; that edition was prepared by Alfred Einstein, who may have been a distant relation.
In 1931, while engaged in research at the California Institute of Technology, he visited the Zoellner family conservatory in Los Angeles, where he played some of Beethoven and Mozart's works with members of the Zoellner Quartet. Near the end of his life, when the young Juilliard Quartet visited him in Princeton, he played his violin with them, and the quartet was "impressed by Einstein's level of coordination and intonation".
Death.
On 17 April 1955, Einstein experienced internal bleeding caused by the rupture of an abdominal aortic aneurysm, which had previously been reinforced surgically by Rudolph Nissen in 1948. He took the draft of a speech he was preparing for a television appearance commemorating the state of Israel's seventh anniversary with him to the hospital, but he did not live to complete it.
Einstein refused surgery, saying, "I want to go when I want. It is tasteless to prolong life artificially. I have done my share; it is time to go. I will do it elegantly." He died in the Princeton Hospital early the next morning at the age of 76, having continued to work until near the end.
During the autopsy, the pathologist Thomas Stoltz Harvey removed Einstein's brain for preservation without the permission of his family, in the hope that the neuroscience of the future would be able to discover what made Einstein so intelligent. Einstein's remains were cremated in Trenton, New Jersey, and his ashes were scattered at an undisclosed location.
In a memorial lecture delivered on 13 December 1965 at UNESCO headquarters, nuclear physicist J. Robert Oppenheimer summarized his impression of Einstein as a person: "He was almost wholly without sophistication and wholly without worldliness ... There was always with him a wonderful purity at once childlike and profoundly stubborn."
Einstein bequeathed his personal archives, library, and intellectual assets to the Hebrew University of Jerusalem in Israel.
Scientific career.
Throughout his life, Einstein published hundreds of books and articles. He published more than 300 scientific papers and 150 non-scientific ones. On 5 December 2014, universities and archives announced the release of Einstein's papers, comprising more than 30,000 unique documents. Einstein's intellectual achievements and originality have made the word "Einstein" synonymous with "genius". In addition to the work he did by himself he also collaborated with other scientists on additional projects including the Bose–Einstein statistics, the Einstein refrigerator and others.
1905 – "Annus Mirabilis" papers.
The "Annus Mirabilis" papers are four articles pertaining to the photoelectric effect (which gave rise to quantum theory), Brownian motion, the special theory of relativity, and "E" = "mc"2 that Einstein published in the "Annalen der Physik" scientific journal in 1905. These four works contributed substantially to the foundation of modern physics and changed views on space, time, and matter. The four papers are:
Statistical mechanics.
Thermodynamic fluctuations and statistical physics.
Einstein's first paper submitted in 1900 to "Annalen der Physik" was on capillary attraction. It was published in 1901 with the title "Folgerungen aus den Capillaritätserscheinungen", which translates as "Conclusions from the capillarity phenomena". Two papers he published in 1902–1903 (thermodynamics) attempted to interpret atomic phenomena from a statistical point of view. These papers were the foundation for the 1905 paper on Brownian motion, which showed that Brownian movement can be construed as firm evidence that molecules exist. His research in 1903 and 1904 was mainly concerned with the effect of finite atomic size on diffusion phenomena.
Theory of critical opalescence.
Einstein returned to the problem of thermodynamic fluctuations, giving a treatment of the density variations in a fluid at its critical point. Ordinarily the density fluctuations are controlled by the second derivative of the free energy with respect to the density. At the critical point, this derivative is zero, leading to large fluctuations. The effect of density fluctuations is that light of all wavelengths is scattered, making the fluid look milky white. Einstein relates this to Rayleigh scattering, which is what happens when the fluctuation size is much smaller than the wavelength, and which explains why the sky is blue. Einstein quantitatively derived critical opalescence from a treatment of density fluctuations, and demonstrated how both the effect and Rayleigh scattering originate from the atomistic constitution of matter.
Special relativity.
Einstein's "" ("On the Electrodynamics of Moving Bodies") was received on 30 June 1905 and published 26 September of that same year. It reconciled conflicts between Maxwell's equations (the laws of electricity and magnetism) and the laws of Newtonian mechanics by introducing changes to the laws of mechanics. Observationally, the effects of these changes are most apparent at high speeds (where objects are moving at speeds close to the speed of light). The theory developed in this paper later became known as Einstein's special theory of relativity.
This paper predicted that, when measured in the frame of a relatively moving observer, a clock carried by a moving body would appear to slow down, and the body itself would contract in its direction of motion. This paper also argued that the idea of a luminiferous aether—one of the leading theoretical entities in physics at the time—was superfluous.
In his paper on mass–energy equivalence, Einstein produced "E" = "mc"2 as a consequence of his special relativity equations. Einstein's 1905 work on relativity remained controversial for many years, but was accepted by leading physicists, starting with Max Planck.
Einstein originally framed special relativity in terms of kinematics (the study of moving bodies). In 1908, Hermann Minkowski reinterpreted special relativity in geometric terms as a theory of spacetime. Einstein adopted Minkowski's formalism in his 1915 general theory of relativity.
General relativity.
General relativity and the equivalence principle.
General relativity (GR) is a theory of gravitation that was developed by Einstein between 1907 and 1915. According to it, the observed gravitational attraction between masses results from the warping of spacetime by those masses. General relativity has developed into an essential tool in modern astrophysics; it provides the foundation for the current understanding of black holes, regions of space where gravitational attraction is so strong that not even light can escape.
As Einstein later said, the reason for the development of general relativity was that the preference of inertial motions within special relativity was unsatisfactory, while a theory which from the outset prefers no state of motion (even accelerated ones) should appear more satisfactory. Consequently, in 1907 he published an article on acceleration under special relativity. In that article titled "On the Relativity Principle and the Conclusions Drawn from It", he argued that free fall is really inertial motion, and that for a free-falling observer the rules of special relativity must apply. This argument is called the equivalence principle. In the same article, Einstein also predicted the phenomena of gravitational time dilation, gravitational redshift and gravitational lensing.
In 1911, Einstein published another article "On the Influence of Gravitation on the Propagation of Light" expanding on the 1907 article, in which he estimated the amount of deflection of light by massive bodies. Thus, the theoretical prediction of general relativity could for the first time be tested experimentally.
Gravitational waves.
In 1916, Einstein predicted gravitational waves, ripples in the curvature of spacetime which propagate as waves, traveling outward from the source, transporting energy as gravitational radiation. The existence of gravitational waves is possible under general relativity due to its Lorentz invariance which brings the concept of a finite speed of propagation of the physical interactions of gravity with it. By contrast, gravitational waves cannot exist in the Newtonian theory of gravitation, which postulates that the physical interactions of gravity propagate at infinite speed.
The first, indirect, detection of gravitational waves came in the 1970s through observation of a pair of closely orbiting neutron stars, PSR B1913+16. The explanation for the decay in their orbital period was that they were emitting gravitational waves. Einstein's prediction was confirmed on 11 February 2016, when researchers at LIGO published the first observation of gravitational waves, detected on Earth on 14 September 2015, nearly one hundred years after the prediction.
Hole argument and Entwurf theory.
While developing general relativity, Einstein became confused about the gauge invariance in the theory. He formulated an argument that led him to conclude that a general relativistic field theory is impossible. He gave up looking for fully generally covariant tensor equations and searched for equations that would be invariant under general linear transformations only.
In June 1913, the Entwurf ('draft') theory was the result of these investigations. As its name suggests, it was a sketch of a theory, less elegant and more difficult than general relativity, with the equations of motion supplemented by additional gauge fixing conditions. After more than two years of intensive work, Einstein realized that the hole argument was mistaken and abandoned the theory in November 1915.
Physical cosmology.
In 1917, Einstein applied the general theory of relativity to the structure of the universe as a whole. He discovered that the general field equations predicted a universe that was dynamic, either contracting or expanding. As observational evidence for a dynamic universe was lacking at the time, Einstein introduced a new term, the cosmological constant, into the field equations, in order to allow the theory to predict a static universe. The modified field equations predicted a static universe of closed curvature, in accordance with Einstein's understanding of Mach's principle in these years. This model became known as the Einstein World or Einstein's static universe.
Following the discovery of the recession of the galaxies by Edwin Hubble in 1929, Einstein abandoned his static model of the universe, and proposed two dynamic models of the cosmos, the Friedmann–Einstein universe of 1931 and the Einstein–de Sitter universe of 1932. In each of these models, Einstein discarded the cosmological constant, claiming that it was "in any case theoretically unsatisfactory".
In many Einstein biographies, it is claimed that Einstein referred to the cosmological constant in later years as his "biggest blunder", based on a letter George Gamow claimed to have received from him. The astrophysicist Mario Livio has cast doubt on this claim.
In late 2013, a team led by the Irish physicist Cormac O'Raifeartaigh discovered evidence that, shortly after learning of Hubble's observations of the recession of the galaxies, Einstein considered a steady-state model of the universe. In a hitherto overlooked manuscript, apparently written in early 1931, Einstein explored a model of the expanding universe in which the density of matter remains constant due to a continuous creation of matter, a process that he associated with the cosmological constant. As he stated in the paper, "In what follows, I would like to draw attention to a solution to equation (1) that can account for Hubbel's ["sic"] facts, and in which the density is constant over time" ... "If one considers a physically bounded volume, particles of matter will be continually leaving it. For the density to remain constant, new particles of matter must be continually formed in the volume from space."
It thus appears that Einstein considered a steady-state model of the expanding universe many years before Hoyle, Bondi and Gold. However, Einstein's steady-state model contained a fundamental flaw and he quickly abandoned the idea.
Energy momentum pseudotensor.
General relativity includes a dynamical spacetime, so it is difficult to see how to identify the conserved energy and momentum. Noether's theorem allows these quantities to be determined from a Lagrangian with translation invariance, but general covariance makes translation invariance into something of a gauge symmetry. The energy and momentum derived within general relativity by Noether's prescriptions do not make a real tensor for this reason.
Einstein argued that this is true for a fundamental reason: the gravitational field could be made to vanish by a choice of coordinates. He maintained that the non-covariant energy momentum pseudotensor was, in fact, the best description of the energy momentum distribution in a gravitational field. While the use of non-covariant objects like pseudotensors was criticized by Erwin Schrödinger and others, Einstein's approach has been echoed by physicists including Lev Landau and Evgeny Lifshitz.
Wormholes.
In 1935, Einstein collaborated with Nathan Rosen to produce a model of a wormhole, often called Einstein–Rosen bridges. His motivation was to model elementary particles with charge as a solution of gravitational field equations, in line with the program outlined in the paper "Do Gravitational Fields play an Important Role in the Constitution of the Elementary Particles?". These solutions cut and pasted Schwarzschild black holes to make a bridge between two patches. Because these solutions included spacetime curvature without the presence of a physical body, Einstein and Rosen suggested that they could provide the beginnings of a theory that avoided the notion of point particles. However, it was later found that Einstein–Rosen bridges are not stable.
Einstein–Cartan theory.
In order to incorporate spinning point particles into general relativity, the affine connection needed to be generalized to include an antisymmetric part, called the torsion. This modification was made by Einstein and Cartan in the 1920s.
Equations of motion.
In general relativity, gravitational force is reimagined as curvature of spacetime. A curved path like an orbit is not the result of a force deflecting a body from an ideal straight-line path, but rather the body's attempt to fall freely through a background that is itself curved by the presence of other masses. A remark by John Archibald Wheeler that has become proverbial among physicists summarizes the theory: "Spacetime tells matter how to move; matter tells spacetime how to curve." The Einstein field equations cover the latter aspect of the theory, relating the curvature of spacetime to the distribution of matter and energy. The geodesic equation covers the former aspect, stating that freely falling bodies follow lines that are as straight as possible in a curved spacetime. Einstein regarded this as an "independent fundamental assumption" that had to be postulated in addition to the field equations in order to complete the theory. Believing this to be a shortcoming in how general relativity was originally presented, he wished to derive it from the field equations themselves. Since the equations of general relativity are non-linear, a lump of energy made out of pure gravitational fields, like a black hole, would move on a trajectory which is determined by the Einstein field equations themselves, not by a new law. Accordingly, Einstein proposed that the field equations would determine the path of a singular solution, like a black hole, to be a geodesic. Both physicists and philosophers have often repeated the assertion that the geodesic equation can be obtained from applying the field equations to the motion of a gravitational singularity, but this claim remains disputed.
Old quantum theory.
Photons and energy quanta.
In a 1905 paper, Einstein postulated that light itself consists of localized particles ("quanta"). Einstein's light quanta were nearly universally rejected by all physicists, including Max Planck and Niels Bohr. This idea only became universally accepted in 1919, with Robert Millikan's detailed experiments on the photoelectric effect, and with the measurement of Compton scattering.
Einstein concluded that each wave of frequency "f" is associated with a collection of photons with energy "hf" each, where "h" is the Planck constant. He did not say much more, because he was not sure how the particles were related to the wave. But he did suggest that this idea would explain certain experimental results, notably the photoelectric effect.
Quantized atomic vibrations.
In 1907, Einstein proposed a model of matter where each atom in a lattice structure is an independent harmonic oscillator. In the Einstein model, each atom oscillates independently—a series of equally spaced quantized states for each oscillator. Einstein was aware that getting the frequency of the actual oscillations would be difficult, but he nevertheless proposed this theory because it was a particularly clear demonstration that quantum mechanics could solve the specific heat problem in classical mechanics. Peter Debye refined this model.
Bose–Einstein statistics.
In 1924, Einstein received a description of a statistical model from Indian physicist Satyendra Nath Bose, based on a counting method that assumed that light could be understood as a gas of indistinguishable particles. Einstein noted that Bose's statistics applied to some atoms as well as to the proposed light particles, and submitted his translation of Bose's paper to the "Zeitschrift für Physik". Einstein also published his own articles describing the model and its implications, among them the Bose–Einstein condensate phenomenon that some particulates should appear at very low temperatures. It was not until 1995 that the first such condensate was produced experimentally by Eric Allin Cornell and Carl Wieman using ultra-cooling equipment built at the NIST–JILA laboratory at the University of Colorado at Boulder. Bose–Einstein statistics are now used to describe the behaviors of any assembly of bosons. Einstein's sketches for this project may be seen in the Einstein Archive in the library of the Leiden University.
Wave–particle duality.
Although the patent office promoted Einstein to Technical Examiner Second Class in 1906, he had not given up on academia. In 1908, he became a "Privatdozent" at the University of Bern. In "Über die Entwicklung unserer Anschauungen über das Wesen und die Konstitution der Strahlung" (""), on the quantization of light, and in an earlier 1909 paper, Einstein showed that Max Planck's energy quanta must have well-defined momenta and act in some respects as independent, point-like particles. This paper introduced the "photon" concept (although the name "photon" was introduced later by Gilbert N. Lewis in 1926) and inspired the notion of wave–particle duality in quantum mechanics. Einstein saw this wave–particle duality in radiation as concrete evidence for his conviction that physics needed a new, unified foundation.
Zero-point energy.
In a series of works completed from 1911 to 1913, Planck reformulated his 1900 quantum theory and introduced the idea of zero-point energy in his "second quantum theory". Soon, this idea attracted the attention of Einstein and his assistant Otto Stern. Assuming the energy of rotating diatomic molecules contains zero-point energy, they then compared the theoretical specific heat of hydrogen gas with the experimental data. The numbers matched nicely. However, after publishing the findings, they promptly withdrew their support, because they no longer had confidence in the correctness of the idea of zero-point energy.
Stimulated emission.
In 1917, at the height of his work on relativity, Einstein published an article in "Physikalische Zeitschrift" that proposed the possibility of stimulated emission, the physical process that makes possible the maser and the laser.
This article showed that the statistics of absorption and emission of light would only be consistent with Planck's distribution law if the emission of light into a mode with n photons would be enhanced statistically compared to the emission of light into an empty mode. This paper was enormously influential in the later development of quantum mechanics, because it was the first paper to show that the statistics of atomic transitions had simple laws.
Matter waves.
Einstein discovered Louis de Broglie's work and supported his ideas, which were received skeptically at first. In another major paper from this era, Einstein observed that de Broglie waves could explain the quantization rules of Bohr and Sommerfeld. This paper would inspire Schrödinger's work of 1926.
Quantum mechanics.
Einstein's objections to quantum mechanics.
Einstein played a major role in developing quantum theory, beginning with his 1905 paper on the photoelectric effect. However, he became displeased with modern quantum mechanics as it had evolved after 1925, despite its acceptance by other physicists. He was skeptical that the randomness of quantum mechanics was fundamental rather than the result of determinism, stating that God "is not playing at dice". Until the end of his life, he continued to maintain that quantum mechanics was incomplete.
Bohr versus Einstein.
 The Bohr–Einstein debates were a series of public disputes about quantum mechanics between Einstein and Niels Bohr, who were two of its founders. Their debates are remembered because of their importance to the philosophy of science. Their debates would influence later interpretations of quantum mechanics.
Einstein–Podolsky–Rosen paradox.
Einstein never fully accepted quantum mechanics. While he recognized that it made correct predictions, he believed a more fundamental description of nature must be possible. Over the years he presented multiple arguments to this effect, but the one he preferred most dated to a debate with Bohr in 1930. Einstein suggested a thought experiment in which two objects are allowed to interact and then moved apart a great distance from each other. The quantum-mechanical description of the two objects is a mathematical entity known as a wavefunction. If the wavefunction that describes the two objects before their interaction is given, then the Schrödinger equation provides the wavefunction that describes them after their interaction. But because of what would later be called quantum entanglement, measuring one object would lead to an instantaneous change of the wavefunction describing the other object, no matter how far away it is. Moreover, the choice of which measurement to perform upon the first object would affect what wavefunction could result for the second object. Einstein reasoned that no influence could propagate from the first object to the second instantaneously fast. Indeed, he argued, physics depends on being able to tell one thing apart from another, and such instantaneous influences would call that into question. Because the true "physical condition" of the second object could not be immediately altered by an action done to the first, Einstein concluded, the wavefunction could not be that true physical condition, only an incomplete description of it.
A more famous version of this argument came in 1935, when Einstein published a paper with Boris Podolsky and Nathan Rosen that laid out what would become known as the EPR paradox. In this thought experiment, two particles interact in such a way that the wavefunction describing them is entangled. Then, no matter how far the two particles were separated, a precise position measurement on one particle would imply the ability to predict, perfectly, the result of measuring the position of the other particle. Likewise, a precise momentum measurement of one particle would result in an equally precise prediction for of the momentum of the other particle, without needing to disturb the other particle in any way. They argued that no action taken on the first particle could instantaneously affect the other, since this would involve information being transmitted faster than light, which is forbidden by the theory of relativity. They invoked a principle, later known as the "EPR criterion of reality", positing that: "If, without in any way disturbing a system, we can predict with certainty (i.e., with probability equal to unity) the value of a physical quantity, then there exists an element of reality corresponding to that quantity." From this, they inferred that the second particle must have a definite value of both position and of momentum prior to either quantity being measured. But quantum mechanics considers these two observables incompatible and thus does not associate simultaneous values for both to any system. Einstein, Podolsky, and Rosen therefore concluded that quantum theory does not provide a complete description of reality.
In 1964, John Stewart Bell carried the analysis of quantum entanglement much further. He deduced that if measurements are performed independently on the two separated particles of an entangled pair, then the assumption that the outcomes depend upon hidden variables within each half implies a mathematical constraint on how the outcomes on the two measurements are correlated. This constraint would later be called a Bell inequality. Bell then showed that quantum physics predicts correlations that violate this inequality. Consequently, the only way that hidden variables could explain the predictions of quantum physics is if they are "nonlocal", which is to say that somehow the two particles are able to interact instantaneously no matter how widely they ever become separated. Bell argued that because an explanation of quantum phenomena in terms of hidden variables would require nonlocality, the EPR paradox "is resolved in the way which Einstein would have liked least".
Despite this, and although Einstein personally found the argument in the EPR paper overly complicated, that paper became among the most influential papers published in "Physical Review". It is considered a centerpiece of the development of quantum information theory.
Unified field theory.
Encouraged by his success with general relativity, Einstein sought an even more ambitious geometrical theory that would treat gravitation and electromagnetism as aspects of a single entity. In 1950, he described his unified field theory in a "Scientific American" article titled "On the Generalized Theory of Gravitation". His attempt to find the most fundamental laws of nature won him praise but not success: a particularly conspicuous blemish of his model was that it did not accommodate the strong and weak nuclear forces, neither of which was well understood until many years after his death. Although most researchers now believe that Einstein's approach to unifying physics was mistaken, his goal of a theory of everything is one to which his successors still aspire.
Other investigations.
Einstein conducted other investigations that were unsuccessful and abandoned. These pertain to force, superconductivity, and other research.
Collaboration with other scientists.
In addition to longtime collaborators Leopold Infeld, Nathan Rosen, Peter Bergmann and others, Einstein also had some one-shot collaborations with various scientists.
Einstein–de Haas experiment.
In 1908, Owen Willans Richardson predicted that a change in the magnetic moment of a free body will cause this body to rotate. This effect is a consequence of the conservation of angular momentum and is strong enough to be observable in ferromagnetic materials. Einstein and Wander Johannes de Haas published two papers in 1915 claiming the first experimental observation of the effect. Measurements of this kind demonstrate that the phenomenon of magnetization is caused by the alignment (polarization) of the angular momenta of the electrons in the material along the axis of magnetization. These measurements also allow the separation of the two contributions to the magnetization: that which is associated with the spin and with the orbital motion of the electrons. The Einstein-de Haas experiment is the only experiment concived, realized and published by Albert Einstein himself.
A complete original version of the Einstein-de Haas experimental equipment was donated by Geertruida de Haas-Lorentz, wife of de Haas and daughter of Lorentz, to the Ampère Museum in Lyon France in 1961 where it is currently on display. It was lost among the museum's holdings and was rediscovered in 2023.
Einstein as an inventor.
In 1926, Einstein and his former student Leó Szilárd co-invented (and in 1930, patented) the Einstein refrigerator. This absorption refrigerator was then revolutionary for having no moving parts and using only heat as an input. On 11 November 1930, was awarded to Einstein and Leó Szilárd for the refrigerator. Their invention was not immediately put into commercial production, but the most promising of their patents were acquired by the Swedish company Electrolux.
Einstein also invented an electromagnetic pump, sound reproduction device, and several other household devices.
Non-scientific legacy.
While traveling, Einstein wrote daily to his wife Elsa and adopted stepdaughters Margot and Ilse. The letters were included in the papers bequeathed to the Hebrew University of Jerusalem. Margot Einstein permitted the personal letters to be made available to the public, but requested that it not be done until twenty years after her death (she died in 1986). Barbara Wolff, of the Hebrew University's Albert Einstein Archives, told the BBC that there are about 3,500 pages of private correspondence written between 1912 and 1955.
Einstein's right of publicity was litigated in 2015 in a federal district court in California. Although the court initially held that the right had expired, that ruling was immediately appealed, and the decision was later vacated in its entirety. The underlying claims between the parties in that lawsuit were ultimately settled. The right is enforceable, and the Hebrew University of Jerusalem is the exclusive representative of that right. Corbis, successor to The Roger Richman Agency, licenses the use of his name and associated imagery, as agent for the university.
Mount Einstein in the Chugach Mountains of Alaska was named in 1955.
Mount Einstein in New Zealand's Paparoa Range was named after him in 1970 by the Department of Scientific and Industrial Research.
In popular culture.
Einstein became one of the most famous scientific celebrities after the confirmation of his general theory of relativity in 1919. Although most of the public had little understanding of his work, he was widely recognized and admired. In the period before World War II, "The New Yorker" published a vignette in their "The Talk of the Town" feature saying that Einstein was so well known in America that he would be stopped on the street by people wanting him to explain "that theory". Eventually he came to cope with unwanted enquirers by pretending to be someone else: "Pardon me, sorry! Always I am mistaken for Professor Einstein."
Einstein has been the subject of or inspiration for many novels, films, plays, and works of music. He is a favorite model for depictions of absent-minded professors; his expressive face and distinctive hairstyle have been widely copied and exaggerated. "Time" magazine's Frederic Golden wrote that Einstein was "a cartoonist's dream come true".
Many popular quotations are often misattributed to him.
Awards and honors.
Einstein received numerous awards and honors, and in 1922, he was awarded the 1921 Nobel Prize in Physics "for his services to Theoretical Physics, and especially for his discovery of the law of the photoelectric effect". None of the nominations in 1921 met the criteria set by Alfred Nobel, so the 1921 prize was carried forward and awarded to Einstein in 1922.
Einsteinium, a synthetic chemical element, was named in his honor in 1955, a few months after his death.

</doc>